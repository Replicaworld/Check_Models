{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ef9d4afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'home/Gourav/Models'\n",
      "/home/Gourav/Models\n"
     ]
    }
   ],
   "source": [
    "cd home/Gourav/Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "af50d50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting of MLP training\n",
      "/home/Gourav/Models\n"
     ]
    }
   ],
   "source": [
    "print('Starting of MLP training')\n",
    "import logging\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "print(os.getcwd())\n",
    "if not os.path.exists('/home/Gourav/Models/approach_logs/MLP_classifier/'):\n",
    "    os.makedirs('/home/Gourav/Models/approach_logs/MLP_classifier/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84f7c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52243f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, udf, when\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import DataFrameStatFunctions as stat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, udf, when\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "from pymongo import MongoClient\n",
    "from tqdm import tqdm\n",
    "from pyspark.sql import DataFrameStatFunctions as stat\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "NIS_DATA_BASE_PATH = \"gs://nis-segment-datasource-v3/processed/\"\n",
    "NIS_OLD_DATA_BASE_PATH = \"gs://nis-localytics-datasource/processed/\"\n",
    "\n",
    "conf = SparkConf().setAll([('spark.driver.memory', '100g'), ('spark.broadcast.blockSize', '50m'), (\"spark.executor.instances\", '30')])\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751edd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(dates, prefix, padding=None):\n",
    "    dates_ = dates + []\n",
    "    if padding:\n",
    "        st_date, ed_date = sorted(dates)[0], sorted(dates)[-1]\n",
    "        for i in range(1, 4):\n",
    "            d = (datetime.datetime.strptime(ed_date, date_fmt) + datetime.timedelta(days=i)).strftime(date_fmt)\n",
    "            if d < datetime.datetime.today().strftime(date_fmt):\n",
    "                dates_.append(d)\n",
    "    dates_ = list(set(dates_) - set([\"2019/02/17\", \"2019/02/18\", \"2019/05/28\", \"2019/06/03\", \"2019/07/02\", \"2019/07/03\", \"2019/07/04\", \"2019/11/13\", \"2019/11/14\", \"2020/02/22\", \"2020/03/31\", \"2020/04/16\", \"2020/04/18\", \"2020/05/11\", \"2021/05/13\"]))\n",
    "    paths = []\n",
    "    for date in dates_:\n",
    "        base_path = NIS_DATA_BASE_PATH\n",
    "        if date < \"2018/06/26\":\n",
    "            base_path = NIS_OLD_DATA_BASE_PATH\n",
    "        paths.append(base_path + date + \"/\" + prefix + \"/*.parquet\")\n",
    "    return paths\n",
    "\n",
    "date_fmt = \"%Y/%m/%d\"\n",
    "month_fmt = \"%Y/%m\"\n",
    "\n",
    "def millis2date(x):\n",
    "    try:\n",
    "        if x < 15000000000:\n",
    "            return datetime.datetime.fromtimestamp(x).strftime(date_fmt)\n",
    "        else:\n",
    "            return datetime.datetime.fromtimestamp(x / 1000.).strftime(date_fmt)\n",
    "    except:\n",
    "        return \"1970/01/01\"\n",
    "\n",
    "def millis2month(x):\n",
    "    try:\n",
    "        if x < 15000000000:\n",
    "            return datetime.datetime.fromtimestamp(x).strftime(month_fmt)\n",
    "        else:\n",
    "            return datetime.datetime.fromtimestamp(x / 1000.).strftime(month_fmt)\n",
    "    except:\n",
    "        return \"1970/01\"\n",
    "\n",
    "millis2date_udf = F.udf(millis2date, StringType())\n",
    "millis2month_udf = F.udf(millis2month, StringType())\n",
    "\n",
    "def divide_maps(d1, d2):\n",
    "    keys = set(d1.keys()).intersection(set(d2.keys()))\n",
    "    res = {}\n",
    "    for k in keys:\n",
    "        res[k] = d1[k] * 1. / (d2[k] + 1e-10)\n",
    "    return res\n",
    "\n",
    "def timediff(y, x, date_fmt=\"%Y/%m/%d\"): \n",
    "    end = datetime.datetime.strptime(y, date_fmt)\n",
    "    start = datetime.datetime.strptime(x, date_fmt)\n",
    "    delta = (end - start).days\n",
    "    return delta\n",
    "\n",
    "def monthdiff(y, x, month_fmt=\"%Y/%m\"): \n",
    "    millis = y - x\n",
    "    delta = millis / (1000 * 3600 * 24 * 30)\n",
    "    return delta\n",
    "\n",
    "timediff_udf = udf(timediff, IntegerType())\n",
    "monthdiff_udf = udf(monthdiff, IntegerType())\n",
    "\n",
    "def filter_platform(data, platform=None):\n",
    "    if platform == \"ANDROID\":\n",
    "        data = data.filter(data.platform == \"ANDROID\")\n",
    "    elif platform == \"IOS\":\n",
    "        data = data.filter(data.platform != \"ANDROID\")\n",
    "    return data\n",
    "\n",
    "def filter_category(data, categories=None):\n",
    "    if categories:\n",
    "        data = data.filter(data.categoryWhenEventHappened.isin(categories))\n",
    "    return data\n",
    "\n",
    "def filter_tenant(data, tenant=None):\n",
    "    if tenant in ['hi', 'HINDI']:\n",
    "        data = data.filter(data.tenant.isin(['hi', 'HINDI', 'Hindi', 'hindi']))\n",
    "    elif tenant in ['en', 'ENGLISH']:\n",
    "        data = data.filter(~data.tenant.isin(['hi', 'HINDI', 'Hindi', 'hindi']))\n",
    "    return data\n",
    "\n",
    "\n",
    "def filter_app(data, app_name=None):\n",
    "    if 'appName' in data.columns:\n",
    "        if app_name:\n",
    "            data = data.filter(data.appName == app_name)\n",
    "        else:\n",
    "            data = data.filter((data.appName != \"mini\") & (data.appName != \"crux\"))\n",
    "    return data\n",
    "\n",
    "\n",
    "# In[7]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ffad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_path(date, hours=None):\n",
    "    paths = []\n",
    "    base_path = NIS_RAW_DATA_BASE_PATH + date\n",
    "    if not hours:\n",
    "        return base_path + \"/*/*.gz\"\n",
    "    for hour in hours:\n",
    "        paths.append(base_path + \"/\" + str(hour).zfill(2) + \"/*.gz\")\n",
    "    return \",\".join(paths)\n",
    "\n",
    "def process_raw_data(paths):\n",
    "    def view_data_filters(x):\n",
    "        x = x['properties']\n",
    "        deviceid_filter = ('deviceId' in x) and (x['deviceId'] != '')\n",
    "        time_filter = ('timeSpent' in x) and (int(x['timeSpent']) <= 100) and (int(x['timeSpent']) >= 0)\n",
    "        return deviceid_filter and time_filter\n",
    "\n",
    "    try:\n",
    "        rdd = sc.textFile(paths) \\\n",
    "            .map(json.loads) \\\n",
    "            .filter(lambda x: \"batch\" in x).flatMap(lambda x: x[\"batch\"]) \\\n",
    "            .filter(lambda x: (\"event\" in x) and (x[\"event\"].lower() == \"timespent-front\")) \\\n",
    "            .filter(view_data_filters) \\\n",
    "            .map(lambda x: x['properties'])\n",
    "        view_data = rdd.map(lambda x: (x['deviceId'], x['hashId'][:-2], x['timeSpent'])) \\\n",
    "            .toDF(['deviceId', 'hashId', 'timeSpent'])\n",
    "        \n",
    "        view_data = view_data.filter(view_data.timeSpent.isNotNull())\n",
    "        #view_data = view_data.filter((getHashBucketUDF(view_data.deviceId) >= 16) & (getHashBucketUDF(view_data.deviceId) <= 25))\n",
    "        view_data = view_data.groupby(view_data.deviceId, view_data.hashId) \\\n",
    "                             .agg(F.max(view_data.timeSpent).alias('overallTimeSpent'))\n",
    "        \n",
    "        return view_data\n",
    "    except Exception as e:\n",
    "        logger.warning(\"Error processing data: \" + str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6020f580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import *\n",
    "def getNewsData(d1, d2):\n",
    "    newsMap = getNewsInDates(d1, d2)\n",
    "    hashIdList = list(newsMap.keys())\n",
    "\n",
    "    hashIdsWithFilter = []\n",
    "    for h in hashIdList:\n",
    "        if 'newsLanguage' in newsMap[h] and newsMap[h]['newsLanguage'] == 'english' and newsMap[h]['publishGroupList'][0]['countryCode'] == 'IN':\n",
    "            hashIdsWithFilter.append(h.split('-')[0])\n",
    "    \n",
    "    return hashIdsWithFilter, newsMap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c840742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(str(datetime.datetime.today().date()))\n",
    "hdlr = logging.FileHandler(\n",
    "    '/home/Gourav/Models/approach_logs/MLP_classifier/' + str(datetime.datetime.today().date()) + '.log')\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22977824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Log(s, flag=True):\n",
    "    if flag:\n",
    "        logger.info(s)\n",
    "        print(s)\n",
    "\n",
    "Log('----', flag=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d43a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dates = [datetime.datetime(2023,2,1,0,0,0), datetime.datetime(2023,2,6,0,0,0),datetime.datetime(2023,2,9,0,0,0),datetime.datetime(2023,2,14,0,0,0),\n",
    "                datetime.datetime(2023,2,18,0,0,0),datetime.datetime(2023,2,20,0,0,0), datetime.datetime(2023,2,25,0,0,0)]\n",
    "sampled_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e316b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_days = 10\n",
    "test_days = 7\n",
    "embed_days = 30\n",
    "n_days = train_days + test_days + embed_days\n",
    "k = 0\n",
    "st_date = (sampled_dates[k]+ datetime.timedelta(days=test_days)) - datetime.timedelta(days=n_days)\n",
    "\n",
    "\n",
    "date_fmt = \"%Y/%m/%d\"\n",
    "\n",
    "dates = [(st_date + datetime.timedelta(days=i)) for i in range(n_days+1)]\n",
    "dates.sort()\n",
    "\n",
    "dates_str = [date.strftime(date_fmt) for date in dates]\n",
    "dates_str_embed = dates_str[0:embed_days]\n",
    "dates_str_train = dates_str[embed_days:embed_days+train_days+1]\n",
    "dates_str_test = dates_str[-test_days:]\n",
    "print(\"embed data dates : \", dates_str_embed, len(dates_str_embed))\n",
    "print(\"train data dates : \",dates_str_train, len(dates_str_train) )\n",
    "print(\"test data dates : \",dates_str_test, len(dates_str_test))\n",
    "# millisMin = dates[0].timestamp() * 1000\n",
    "# millisMax = (dates[-1] + datetime.timedelta(days=1)).timestamp() * 1000\n",
    "print(dates[0], dates[-1])\n",
    "hashIdsWithFilter, newsMap = getNewsData(dates[0], dates[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c58479-8c41-4c1f-866c-316c517e09ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d525b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need this for live data else skip if backtesting\n",
    "NIS_RAW_DATA_BASE_PATH = \"gs://inshorts-segment-raw/data/segment-raw-v5a/\"\n",
    "\n",
    "path = get_raw_path(datetime.datetime.now().strftime(\"%Y/%m/%d\"))\n",
    "today_data = process_raw_data(path)\n",
    "today_data = today_data.filter(today_data.hashId.isin(hashIdsWithFilter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3586e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate datasets\n",
    "def generate_data(datestr):\n",
    "#     datestr = [d.strftime(date_fmt) for d in dates]\n",
    "    paths = get_path(datestr, 'otherEvents')\n",
    "#     print(sorted(paths), len(paths))\n",
    "    data = sqlContext.read.parquet(*paths)\n",
    "\n",
    "    data = filter_app(data, app_name=None)\n",
    "    data = filter_tenant(data, tenant='en')\n",
    "\n",
    "    # data = data.filter((data.eventTimestamp > millisMin) & (data.eventTimestamp < millisMax))\n",
    "\n",
    "    data = data.select(data.deviceId, (F.split(data.hashId, '-')[0]).alias('hashId'), data.overallTimeSpent, data.notificationOpened, data.notificationShown, data.notificationType,data.eventName)\n",
    "    data = data.filter( data.eventName.isin(['Notification Shown','Notification Opened']) )\n",
    "\n",
    "    data = data.filter(data.hashId.isin(hashIdsWithFilter))\n",
    "    \n",
    "    data = data.select('deviceId', 'hashId' ,'overallTimeSpent', 'notificationOpened', 'notificationShown')\\\n",
    "            .groupby('deviceId', 'hashId')\\\n",
    "            .agg(F.max('overallTimeSpent').alias('timeSpent'), \n",
    "                 F.sum('notificationShown').alias('notiShown'),F.sum('notificationOpened').alias('notiOpened'))\n",
    "    data = data.withColumn(\"notiShown\",F.when(data.notiShown < data.notiOpened, \n",
    "                                                     data.notiOpened).otherwise(data.notiShown))\n",
    "    return data\n",
    "\n",
    "data_embed = generate_data(dates_str_embed)\n",
    "data_train = generate_data(dates_str_train)\n",
    "data_test = generate_data(dates_str_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28e848a-b575-4cfd-bfc8-03bf726c7f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372f0193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to be used when live data is needed\n",
    "data = data.union(today_data)\n",
    "data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a570c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity(data, newsMeanMap):\n",
    "    Log(\"Computing news mean\")\n",
    "    newsMeandf = data.groupby(data.hashId).agg({'overallTimeSpent' : 'sum', 'hashId' : 'count'}).toPandas()\n",
    "\n",
    "    for v in newsMeandf.values:\n",
    "        newsMeanMap[v[0]] = newsMeanMap[v[0]][0] + v[1],  newsMeanMap[v[0]][1] + v[2]\n",
    "\n",
    "newsMeanMap_train = defaultdict(lambda: (7, 1))\n",
    "popularity(data, newsMe1banMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b8e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "9b2986af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_comb = data_embed.union(data_train)\n",
    "#data_comb.groupBy('eventName').count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "bee8e979-1559-4dfd-b58a-479d1a2b14db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[deviceId: string, hashId: string, timeSpent: double, notiShown: bigint, notiOpened: bigint]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342c0a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_comb.filter(data_comb.notificationOpened > 0).groupBy('eventName').count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaea4bb3-0e38-424a-9420-14bf8337ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whats happening in next few rowS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0943dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.clustering import LDA\n",
    "import time\n",
    "\n",
    "training_start = time.time()\n",
    "\n",
    "\n",
    "# data_filtered = data.filter(F.udf(lambda hashId, noti: overallTimeSpent > (2.5 * newsMeanMap[hashId][0]/newsMeanMap[hashId][1]), BooleanType())('hashId', 'overallTimeSpent'))\n",
    "# abcset=sqlContext.read.parquet(\"gs://pvtrough_asia_south1/tf_idf/devices_fulfilling_criteria\")\n",
    "# df_temp=abcset.filter(abcset.set_typ == 'roberta').select(['deviceid']).distinct()\n",
    "# df_temp=df_temp.withColumnRenamed(\"deviceid\",\"deviceId\")\n",
    "\n",
    "# data_filtered=data_filtered.join(df_temp,[\"deviceId\"],\"inner\")\n",
    "\n",
    "data_embed = data_embed.filter(data_embed.notiOpened>0)    \n",
    "\n",
    "\n",
    "# Log(\"Training took %s minutes\"%(int((time.time() - training_start) / 60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c1fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = data.select('deviceId').distinct().collect()\n",
    "d2 = data_filtered.select('deviceId').distinct().collect()\n",
    "d3 = df_temp.select('deviceId').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1284dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "df_left = pd.DataFrame(columns = ['deviceId', 'cluster_left_out_date'])\n",
    "left_out = set(d3) - set(d2)\n",
    "pbar = tqdm(total=len(left_out))\n",
    "cluster_left_out = (datetime.datetime.now() + datetime.timedelta(days=0)) .strftime(\"%Y_%m_%d\") \n",
    "i = 0\n",
    "for d in left_out:\n",
    "    df_left.loc[i,'deviceId'] = d['deviceId']\n",
    "    df_left.loc[i,'cluster_left_out_date'] = cluster_left_out\n",
    "    i= i+1\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49936e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = df_left.copy()\n",
    "print(dfl.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde4dbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_leftout = 'gs://pvtrough_asia_south1/clustering/leftout_devices_roberta'\n",
    "df_left = sqlContext.createDataFrame(dfl[['deviceId', 'cluster_left_out_date']])\n",
    "df_left.coalesce(1).write.partitionBy('cluster_left_out_date').csv(path_leftout , sep=',', header=True,mode = 'append')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a00a51d-13c8-4b82-93aa-8e7162264e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#till here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dec3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DataParallel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, RobertaTokenizer , RobertaForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "\n",
    "model_name = \"roberta-base\"\n",
    "num_classes = 17\n",
    "# model_path = \"output/checkpoint-12000\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_classes,output_hidden_states=True)\n",
    "\n",
    "# Move the model to the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Check the number of available GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    # Specify which GPUs to use\n",
    "    device_ids = [0, 1]  # Adjust the GPU IDs based on your system configuration\n",
    "    model = DataParallel(model, device_ids=device_ids)\n",
    "    print(isinstance(model, DataParallel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083cb92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model, tokenizer, text):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        input_ids = tokenizer.encode(text, add_special_tokens=True, return_tensors='pt').to(device)\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        hidden_states = outputs.hidden_states\n",
    "        last_layer_hidden_states = hidden_states[-1]\n",
    "        embeddings = torch.mean(last_layer_hidden_states, dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "4b7c4541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def Dim_reduction(sentences, tokenizer, model):\n",
    "    '''\n",
    "        This method will accept array of sentences, roberta tokenizer & model\n",
    "        next it will call methods for dimention reduction\n",
    "    '''\n",
    "\n",
    "    vecs = []\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for sentence in sentences:\n",
    "            inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True,  max_length=64)\n",
    "            inputs['input_ids'] = inputs['input_ids'].to(DEVICE)\n",
    "            inputs['attention_mask'] = inputs['attention_mask'].to(DEVICE)\n",
    "\n",
    "            hidden_states = model(**inputs, return_dict=True, output_hidden_states=True).hidden_states\n",
    "\n",
    "            #Averaging the first & last hidden states\n",
    "            output_hidden_state = (hidden_states[-1] + hidden_states[1]).mean(dim=1)\n",
    "\n",
    "            vec = output_hidden_state.cpu().numpy()[0]\n",
    "\n",
    "            vecs.append(vec)\n",
    "    \n",
    "#     print(\"35:\",vecs)\n",
    "    #Finding Kernal\n",
    "    kernel, bias = compute_kernel_bias([vecs])\n",
    "    kernel = kernel[:, :200]\n",
    "    #If you want to reduce it to 128 dim\n",
    "    #kernel = kernel[:, :128]\n",
    "    embeddings = []\n",
    "    embeddings = np.vstack(vecs)\n",
    "#     print(\"43:\", kernel, bias)\n",
    "#     print(\"44:\",embeddings)\n",
    "    #Sentence embeddings can be converted into an identity matrix\n",
    "    #by utilizing the transformation matrix\n",
    "    embeddings = transform_and_normalize(embeddings, \n",
    "                kernel=kernel,\n",
    "                bias=bias\n",
    "            )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "3af77bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def transform_and_normalize(vecs, kernel, bias):\n",
    "    \"\"\"\n",
    "        Applying transformation then standardize\n",
    "    \"\"\"\n",
    "    if not (kernel is None or bias is None):\n",
    "        vecs = (vecs + bias).dot(kernel)\n",
    "    return normalize(vecs)\n",
    "    \n",
    "def normalize(vecs):\n",
    "    \"\"\"\n",
    "        Standardization\n",
    "    \"\"\"\n",
    "    return vecs / (vecs**2).sum(axis=1, keepdims=True)**0.5\n",
    "    \n",
    "def compute_kernel_bias(vecs):\n",
    "    \"\"\"\n",
    "    Calculate Kernal & Bias for the final transformation - y = (x + bias).dot(kernel)\n",
    "    \"\"\"\n",
    "    vecs = np.concatenate(vecs, axis=0)\n",
    "    mu = vecs.mean(axis=0, keepdims=True)\n",
    "    cov = np.cov(vecs.T)\n",
    "    u, s, vh = np.linalg.svd(cov)\n",
    "    W = np.dot(u, np.diag(s**0.5))\n",
    "    W = np.linalg.inv(W.T)\n",
    "    return W, -mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "f80758e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score of two sentences 0.7305076834448454\n",
      "New Dimension is (2, 100)\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('C:/Models/roberta-base-nli-stsb-mean-tokens')\n",
    "# model = RobertaModel.from_pretrained('C:/Models/roberta-base-nli-stsb-mean-tokens')\n",
    "\n",
    "sentences = ['someone is slicing a onion', 'someone is carrying a fish']\n",
    "\n",
    "embeddings = Dim_reduction(sentences, tokenizer, model)\n",
    "# print(distance.cosine(embeddings[0],embeddings[1]))\n",
    "print(f'Similarity score of two sentences {cosine_similarity([embeddings[0],embeddings[1]])[0][1]}')\n",
    "print(f\"New Dimension is {np.shape(embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379c6bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4137ba6-c9fc-4647-915a-663ccac6ea33",
   "metadata": {},
   "source": [
    "# Checking for News embeddings just using title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f726d28-f27f-44ab-a9df-48a474042224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing the size of embeddings using dimensionally reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97176885",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "from pyspark.sql.functions import array, col\n",
    "path_embedding = \"gs://pvtrough_asia_south1/clustering/News_tilte_embedding\"+sampled_dates[k].strftime(\"%Y_%m_%d\")+\"/\"\n",
    "df_embedding = sqlContext.read.csv(path_embedding, sep=',',\n",
    "                         inferSchema=True, header=True)\n",
    "# df_embedding.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50c4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24176/24176 [08:26<00:00, 47.70it/s]>         (153 + 35) / 188]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding took 8 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#generate news embeddings based on notification text\n",
    "embedding_start = time.time()\n",
    "document_embeddings_title={}\n",
    "\n",
    "newsMapProcessed = {}\n",
    "document_embeddings = {}\n",
    "#try:\n",
    "#    already_embedded = df_embedding.select('hid').distinct().collect()\n",
    "#    already_embedded = [x['hid'] for x in already_embedded]\n",
    "#except:\n",
    "#    print(\"News Embeddings not found, generating new one\")\n",
    "already_embedded = []\n",
    "for hId in tqdm(newsMap):\n",
    "    \n",
    "    h = hId.split('-')[0]\n",
    "#     print(h)\n",
    "    if h in already_embedded or (h not in hashIdsWithFilter) :\n",
    "        continue\n",
    "    else:    \n",
    "        newsMapProcessed[h] = {}\n",
    "        newsMapProcessed[h]['title'] = newsMap[hId]['title']\n",
    "        newsMapProcessed[h]['content'] = newsMap[hId]['content']\n",
    "        newsMapProcessed[h]['features'] = newsMapProcessed[h]['title'] #+ \".\" + newsMapProcessed[h]['content']\n",
    "        document_embeddings[h] = get_embedding(model, tokenizer, newsMapProcessed[h]['features'])\n",
    "        \n",
    "\n",
    "        document_embeddings_title[h] = document_embeddings[h]\n",
    "        \n",
    "\n",
    "Log(\"Embedding took %s minutes\"%(int((time.time() - embedding_start) / 60)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ff49d7-4e84-4465-828f-715bd118459e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15280"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_embeddings_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566bde2a-445b-42e7-ac55-98feba861c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_embedding = \"gs://pvtrough_asia_south1/clustering/News_tilte_embedding\"+sampled_dates[k].strftime(\"%Y_%m_%d\")+\"/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055a8565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/23 19:41:36 WARN org.apache.spark.scheduler.TaskSetManager: Stage 3459 contains a task of very large size (2158 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 3455:==============================================>    (173 + 15) / 188]\r"
     ]
    }
   ],
   "source": [
    "if len(document_embeddings_title) > 0:\n",
    "    df_em_pandas = pd.DataFrame(document_embeddings_title).T\n",
    "    df_em_pandas['hid'] = df_em_pandas.index\n",
    "    df_em_pandas['embed_date'] = sampled_dates[0]\n",
    "    df_em_spark = (sqlContext.createDataFrame(df_em_pandas))\n",
    "    df_em_spark.write.partitionBy('embed_date').csv(path_embedding , sep=',', header=True,mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bc6ed7-a7f0-42f0-81c9-be9ae6b90239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ff372d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(hid='gkefkzeg', embedding=[0.018744973465800285, 0.07465686649084091, 0.00890152808278799, -0.11559038609266281, -0.26225796341896057, 0.0443819984793663, 0.015611527487635612, 0.030492758378386497, 0.19157782196998596, -0.025312107056379318, -0.11552142351865768, -0.016290172934532166, 0.01366395503282547, -0.1699371635913849, 0.0643385648727417, -0.17743484675884247, 0.06688039004802704, -0.013621450401842594, 0.05057955160737038, 0.36965441703796387, -0.028559869155287743, 0.06138797476887703, -0.15975259244441986, -0.029601287096738815, 0.040057308971881866, 0.035204965621232986, 0.001116596395149827, 0.030557503923773766, 0.08012364059686661, -0.12357477843761444, -0.05885322764515877, -0.04981595650315285, -0.05566999688744545, -0.040192216634750366, 0.05430286005139351, -0.05986151844263077, 0.08223418891429901, 0.07280068099498749, 0.11737637966871262, 0.0020225071348249912, 0.06029454246163368, -0.08854582160711288, 0.029932724311947823, 0.004650622606277466, 0.0880812406539917, 0.08063347637653351, -0.01958540268242359, 0.1408916562795639, 0.10857795923948288, -0.03470798209309578, -0.06870229542255402, 0.10351473838090897, 0.11602730304002762, -0.05679521709680557, 0.1240803524851799, -0.15366502106189728, 0.029534151777625084, 0.09168054163455963, 0.12088245153427124, 0.1266418993473053, -0.0023763547651469707, 0.13780620694160461, -0.06048223748803139, 0.053707849234342575, -0.03910117596387863, 0.03661927953362465, 0.03643858805298805, 0.16438081860542297, 0.016076451167464256, -0.021946828812360764, 0.08039601892232895, -0.05761314183473587, 0.05178648605942726, -0.1718236654996872, -0.1498291939496994, -0.008926172740757465, 0.06128501892089844, -5.703286647796631, 0.14519628882408142, -0.027409890666604042, -0.06698785722255707, -0.05163932964205742, 1.321637511253357, 0.045489273965358734, 0.11232741177082062, -0.12541033327579498, 0.09302927553653717, 0.2099856585264206, -0.03699721023440361, 0.12692058086395264, 0.3049249053001404, -0.003263742895796895, 0.06516600400209427, 0.0838218405842781, 0.035143572837114334, -0.09029201418161392, -0.05097139626741409, 0.801727294921875, -0.013369898311793804, -0.07359544187784195, -0.012051539495587349, -0.008893017657101154, 0.275858610868454, 0.11226534843444824, -0.01766391657292843, -0.0008801519870758057, -0.07011265307664871, -0.2805693447589874, -0.05904959514737129, 0.08293867111206055, 0.19051764905452728, -0.050889696925878525, 0.015001173131167889, 0.026296110823750496, 0.11066615581512451, -0.02061980590224266, 0.06580702215433121, 0.1529437154531479, 0.043607354164123535, -0.07231389731168747, 0.030801599845290184, 0.10462137311697006, 0.10181394964456558, 0.05057259649038315, 0.09771580249071121, 0.2459271401166916, 0.06888674199581146, 0.0579947829246521, 0.01695469208061695, -0.05710932984948158, -0.10265399515628815, -0.14579446613788605, -0.03534652665257454, 0.051259253174066544, 0.04503288492560387, 0.03875048831105232, 0.04064375162124634, 0.06601978838443756, 0.05189531669020653, -0.08557504415512085, 0.0020936597138643265, -0.023985326290130615, 0.061517659574747086, 0.04606608301401138, 0.3795916736125946, -0.0035730174276977777, -0.005488633178174496, -0.06845260411500931, 0.06959716975688934, 0.024758297950029373, 0.007456051651388407, 0.02330511435866356, 0.04924995079636574, 0.11087958514690399, 0.1941433846950531, -0.029864463955163956, 0.10959087312221527, 0.14938558638095856, 0.05035029724240303, 0.09177857637405396, 0.0950501412153244, 0.10241836309432983, 0.05163142830133438, 0.022673940286040306, -0.14740657806396484, 0.08402297645807266, 0.0017286123475059867, 0.04323698952794075, 0.033341627568006516, -0.06358630955219269, 0.12147986143827438, 0.11485137045383453, 0.02921343222260475, -0.03291895613074303, 0.08737298101186752, -0.027355654165148735, 0.14232267439365387, -0.06030852347612381, 0.010543422773480415, 0.010312855243682861, 0.058849163353443146, 0.1440284699201584, -0.05968979373574257, 0.020024064928293228, 0.0838073343038559, 0.021229075267910957, 0.0631992295384407, 0.0015225366223603487, 0.1135878935456276, 0.06245552748441696, -0.012454023584723473, 0.01477936189621687, -0.01779984124004841, 0.2181854248046875, 0.18168506026268005, 0.10102363675832748, -0.037532515823841095, -0.13337180018424988, -0.0022982791997492313, -0.006113776005804539, -0.08458207547664642, -0.19286690652370453, 0.14487552642822266, 0.04115418717265129, 0.007488373201340437, 0.026679040864109993, -0.0019121821969747543, 0.0895848274230957, 0.05216819792985916, 0.018435491248965263, -0.06177651137113571, -0.028375430032610893, -0.0036418309900909662, 0.21037186682224274, -0.10266126692295074, -0.0055440738797187805, -0.07955111563205719, -0.8355193734169006, 0.07035254687070847, 0.2350270003080368, -0.03740519657731056, -0.019150855019688606, 0.07064329832792282, -0.0010371910175308585, 0.12689271569252014, 0.10131315141916275, 0.04924812540411949, 0.0272599495947361, 0.04006761312484741, 0.08131974935531616, -0.0049653430469334126, 0.024442637339234352, -0.07616598159074783, -0.1417519897222519, -0.03270290046930313, -0.055582672357559204, -0.1431601345539093, 0.014914254657924175, -0.01609380543231964, -0.04923204332590103, -0.2549200654029846, 0.08462471514940262, 0.1369764506816864, 0.037350352853536606, 0.04373113065958023, 0.19009140133857727, -0.07200850546360016, -0.05038933828473091, 0.10735535621643066, -0.05493713542819023, 0.0627913698554039, 0.033841270953416824, 0.12138932198286057, -0.08674442768096924, 0.08046245574951172, -0.2155223786830902, -0.16466684639453888, -0.06605536490678787, -0.1673446148633957, -0.4931202530860901, 0.023266762495040894, -0.10866595059633255, 0.09905035048723221, -0.11036437749862671, 0.004468982573598623, 0.08205269277095795, -0.04126334935426712, 0.07713628560304642, 0.044601161032915115, -0.0875425785779953, -0.006879511289298534, 0.08509712666273117, 0.048089124262332916, 0.16998450458049774, 0.006240408401936293, 0.0881069228053093, -0.02389853075146675, 0.020911559462547302, 0.016347641125321388, -0.009233089163899422, -0.001276108087040484, -0.009721803478896618, -0.1408597230911255, -0.09506231546401978, 0.04415841028094292, 0.18706752359867096, 0.22098346054553986, -0.21196845173835754, -0.04385078698396683, 0.11225008219480515, 0.06034053489565849, 0.04472661763429642, 0.03111911192536354, 0.05129265412688255, -0.05590050667524338, -0.1564807891845703, 0.09282456338405609, -0.0407322533428669, -0.0004873609577771276, 0.28580227494239807, 0.28245094418525696, 0.04820365831255913, 0.0980248898267746, 0.06211774796247482, 0.007710915524512529, -0.023506101220846176, 0.06848248094320297, -0.057813387364149094, -0.020526545122265816, 0.15055647492408752, -0.03315459564328194, -0.18692104518413544, -0.03301098942756653, 0.020231788977980614, 0.051128096878528595, -0.046589143574237823, 0.06238449364900589, -0.010020988993346691, 0.10699962079524994, -0.058848787099123, 0.05905335769057274, -0.0802183672785759, 0.008873244747519493, 0.03352877125144005, -0.11570043116807938, -0.031233837828040123, -0.12011518329381943, -0.04099233075976372, 0.06345327198505402, 0.08190687745809555, 0.8493670225143433, 0.4075874388217926, 0.08830545097589493, 0.21632669866085052, 0.031808625906705856, 0.11057362705469131, -0.1512264907360077, 0.03064170852303505, -0.035705968737602234, 0.11254838854074478, -0.05455332249403, -0.037228237837553024, -0.18831290304660797, 0.02501729503273964, 0.022285539656877518, -0.007005475927144289, 0.009541383013129234, -0.10088550299406052, -0.16594506800174713, -0.09770563989877701, 0.053689926862716675, 0.05945689603686333, 0.0373622365295887, -0.02732846699655056, -0.051987823098897934, 0.062433965504169464, -0.013494075275957584, -0.1238328292965889, 0.07558994740247726, -0.10651443153619766, 0.3141270577907562, 0.1464744359254837, 0.061183176934719086, 0.06947235763072968, 0.019050154834985733, 0.04216444492340088, 0.022434892132878304, 0.08792056888341904, 0.009582792408764362, 0.01504929456859827, 0.059916723519563675, -0.055671412497758865, 0.04187846556305885, 0.017700549215078354, -0.16976790130138397, 0.02150174044072628, 0.09140657633543015, 0.08421315997838974, 0.21734455227851868, 0.19437095522880554, 0.0006106566288508475, -0.06113743036985397, -0.05096794292330742, 0.0033839622046798468, 0.10292215645313263, 0.0393541045486927, 0.014405867084860802, -0.19294238090515137, -0.3805198669433594, -0.041820138692855835, -0.008883228525519371, 0.06488368660211563, -0.018271474167704582, -0.052413154393434525, 0.07029394060373306, -0.020919067785143852, 0.015985455363988876, 0.22316618263721466, 0.1417064517736435, -0.0023958045057952404, 0.011540727689862251, -0.06265237182378769, -0.05394228547811508, 0.10588660091161728, 0.01808171160519123, -0.025017883628606796, 0.05870265141129494, -0.034118764102458954, 0.00897299312055111, 0.035035859793424606, 0.15795792639255524, -0.16156761348247528, 0.025771457701921463, 0.12371760606765747, -0.08540304005146027, -0.047867707908153534, 0.06022069603204727, -0.09013355523347855, 0.008827101439237595, -0.009588132612407207, 0.07512431591749191, -0.004961781203746796, 0.054840534925460815, -0.08469737321138382, 0.09190352261066437, -0.015434854663908482, 0.27810877561569214, -0.21804025769233704, 0.04841027781367302, -0.08698773384094238, -0.04536675661802292, 0.06550296396017075, 0.06528200954198837, 0.057024817913770676, 0.19026029109954834, 0.015393420122563839, -0.040125198662281036, -0.17108085751533508, 0.0014066735748201609, 0.02647620067000389, 0.0516962967813015, 0.062176432460546494, -0.052458398044109344, -0.18539205193519592, -0.044601839035749435, 0.02496274746954441, 0.19847746193408966, 0.04571491852402687, -0.006507051642984152, -0.0875348448753357, 0.01444895751774311, -0.015253422781825066, 0.05042199417948723, -1.354299783706665, 0.12426149845123291, 0.01160296332091093, 0.024925902485847473, -0.07461347430944443, 0.1946467012166977, 0.11255642026662827, 0.022740477696061134, -0.08530572801828384, -0.05518288165330887, -0.008718165569007397, 0.030544934794306755, -0.06479357928037643, -0.04779190570116043, 0.006788085214793682, -0.1942427009344101, -0.018184855580329895, -0.025326645001769066, -0.00791656132787466, 0.043685849756002426, -0.14596962928771973, -0.02085043676197529, -0.08131678402423859, 0.16727088391780853, 0.177639439702034, -0.12605372071266174, 0.00630723312497139, 0.045617397874593735, 0.006883388850837946, -0.06043247506022453, -0.01289906445890665, 0.009806348010897636, -0.00284783192910254, 0.024801889434456825, 0.010646939277648926, 0.061457209289073944, -0.07110464572906494, 0.07357790321111679, 0.026784656569361687, 0.10101877897977829, 0.013135930523276329, 0.17746153473854065, 0.04376263543963432, -0.3669966161251068, -0.05830811709165573, 0.0686878114938736, 0.03029133751988411, -0.021619442850351334, -0.046850018203258514, -0.021459612995386124, 0.12639740109443665, 0.03781741484999657, 0.25515398383140564, 0.04183759540319443, -0.029965324327349663, 0.09066052734851837, -0.018873821943998337, -0.0605064332485199, -0.08138073235750198, 0.07403918355703354, -0.047247011214494705, 0.008198007009923458, 0.02102121338248253, -0.06325401365756989, 0.03641000762581825, 0.10615650564432144, -0.08478336781263351, 0.04860781878232956, 0.1288061887025833, -0.03407091274857521, -0.012007289566099644, 0.02283474989235401, 0.030712271109223366, 0.03641055151820183, -0.0012813979992642999, 0.10854851454496384, 0.02667340077459812, 0.03706711530685425, -0.08629819005727768, 0.09967266023159027, -0.10436604171991348, 0.0648670345544815, 0.04337722435593605, 0.08250598609447479, 0.1594150811433792, -0.108658067882061, -0.01899559609591961, -0.04584381729364395, 0.08733617514371872, 0.11747445911169052, 0.03762483969330788, 0.1323719322681427, -0.042575836181640625, 0.13131913542747498, 0.0773710086941719, -0.08820341527462006, -0.05526762455701828, 0.025618523359298706, -0.2392127513885498, 0.008883667178452015, 0.025885509327054024, 0.1890820562839508, -0.12498050183057785, 0.03154468908905983, 0.09800036996603012, -0.04942769184708595, -0.059157975018024445, 0.012644250877201557, -0.10458485782146454, 0.041503872722387314, -0.08239541202783585, -0.0836556926369667, -0.04423412308096886, -0.04615354537963867, 0.05982605740427971, 0.07887484133243561, 0.09739124029874802, -0.25410616397857666, -0.0794253721833229, -0.04499361291527748, -0.04171019420027733, 0.06592454016208649, 0.003225501161068678, -0.056299157440662384, 0.15762051939964294, -0.08023406565189362, 0.042011164128780365, -0.24064794182777405, 0.05299937352538109, 0.010272929444909096, 0.13810458779335022, 0.08084338158369064, 0.16612860560417175, -0.0804457813501358, -0.029290447011590004, 9.927062034606934, -0.08422353118658066, -0.0565105676651001, 0.18754079937934875, 0.01773112826049328, -0.03133469820022583, -0.010018269531428814, -0.06381012499332428, -0.13831683993339539, 0.08220058679580688, -0.14471891522407532, 0.019647983834147453, 0.007391448598355055, 0.06451486796140671, -0.0055514974519610405, 0.028811829164624214, -0.1740170419216156, -0.11502818018198013, -0.011669682338833809, -0.0399438738822937, 0.013755222782492638, 0.1556755155324936, -0.0045498027466237545, 0.645097553730011, 0.05899369344115257, 0.12058663368225098, 0.1469486504793167, 0.06035801023244858, -0.04841684550046921, -0.1747170388698578, 0.19078440964221954, -0.0391988530755043, -0.017128828912973404, 0.08729206770658493, -0.013199812732636929, -0.05094074457883835, 0.18506750464439392, -0.04580715298652649, 0.04736838489770889, 0.02508932165801525, 0.057939134538173676, 0.08742355555295944, -0.06263186782598495, 0.11504453420639038, 0.012126480229198933, 0.009360318072140217, -0.03236962482333183, 0.14069703221321106, -0.024661792442202568, 0.03343387693166733, 0.027446512132883072, 0.0034815925173461437, 0.07232671231031418, -0.05157560482621193, -0.1049606055021286, 0.09233645349740982, -0.2811112701892853, -0.05280572548508644, 0.1322956532239914, -0.07495349645614624, -0.01641089841723442, 0.09774193912744522, -0.03035668469965458, 0.2169845551252365, -0.00027141135069541633, 0.12050680071115494, -0.055392950773239136, -0.02542068064212799, 0.06286637485027313, -0.022421974688768387, -0.00015129681560210884, -0.018346842378377914, -0.017716486006975174, -0.0806313082575798, 0.057422906160354614, 0.040808431804180145, 0.1393595188856125, 0.03399338200688362, -0.09089731425046921, 0.1864718198776245, 0.05454796180129051, 0.045409370213747025, 0.05668836086988449, 0.1931711733341217, 0.23024173080921173, -0.005019585136324167, 0.2797386944293976, 0.1007613018155098, 0.04082685336470604, 0.08229280263185501, -0.0944051519036293, 0.03193358704447746, -0.05271449312567711, -0.10484935343265533, -0.05878356471657753, -0.1305091381072998, -0.18685117363929749, -0.0682159960269928, 0.04314184933900833, 0.20649354159832, 0.002114988164976239, 0.19574759900569916, -0.008904128335416317, -0.012028975412249565, 0.04043352231383324, -0.21448104083538055, -0.20484252274036407, -0.22789470851421356, -0.048198338598012924, 0.03200163319706917, 0.013784543611109257, 0.016241181641817093, -0.008085155859589577, 0.12936072051525116, 0.09791562706232071, -0.08237684518098831, -0.024671444669365883, 0.034014564007520676, 0.03636496886610985, 0.10752467811107635, -0.05986267328262329, 0.075772225856781, 0.006390328984707594, 0.07986655086278915, -0.001035150489769876, -0.05711138993501663, 0.016621185466647148, -0.08737623691558838, 0.10525677353143692, -0.049564655870199203, -0.023353343829512596, 0.027887102216482162, 0.13447780907154083, 0.025099636986851692, -0.0004234373918734491, 0.011097515001893044, 0.008888044394552708, -0.018251659348607063, -0.09941304475069046, -0.009833401069045067, -0.10273933410644531, -0.05845791846513748, -0.009224860928952694, -0.3794397711753845, 0.09366142749786377, -0.10968121141195297, 0.09301317483186722, -0.002301241736859083, -0.08424444496631622, 0.04243313521146774, 0.17234869301319122, 0.10359276831150055, 0.13417010009288788, -0.05079454928636551, -0.25076255202293396, -0.014812727458775043, 0.06400657445192337, 0.046052273362874985, 0.06761001795530319, -0.02474655769765377, 0.03964642807841301, -0.9096497893333435, 0.1971762627363205, 0.05565195530653, 0.24784168601036072, 0.0015706000849604607, -0.02224111184477806, 0.046558890491724014, -0.11344991624355316, 0.08009344339370728, 0.15545152127742767, -0.008942446671426296, -0.03980488330125809, -0.021470649167895317, -0.03850773349404335, 0.06738656014204025, 0.13497434556484222, -0.12014885246753693, 0.031403910368680954, 0.03883589059114456])]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import array, col\n",
    "\n",
    "\n",
    "df_news_embedding = sqlContext.read.csv(path_embedding, sep=',',\n",
    "                         inferSchema=True, header=True)\n",
    "df_news_embedding = df_news_embedding.distinct()\n",
    "feat_cols = [str(x) for x in range(0,len(df_news_embedding.columns) - 2)]\n",
    "df_embedding_exploded = df_news_embedding.distinct()\n",
    "df_news_embedding = df_news_embedding.select('hid', array([col(x) for x in feat_cols ]).alias('embedding'))\n",
    "df_news_embedding.take(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b187ace9-8657-47ee-868a-3a07d3d0d689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15280"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news_embedding.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf29d30e-3ab4-4450-b491-b2d4eada4c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15280"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news_embedding.select(['hid']).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd08d9a-1d21-4315-b10f-c0722ace42eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71368da6-f547-45af-b287-3bc048dd0eb4",
   "metadata": {},
   "source": [
    "# Checking for deviceids embeddings using content and title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e04a69-2e3f-43b9-ba42-3548967ee8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_embedding = \"gs://pvtrough_asia_south1/clustering/Content_news_embedding.csv\"+sampled_dates[k].strftime(\"%Y_%m_%d\")+\"/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78603a12-fa6d-415c-b1e0-c91d103ec408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, col\n",
    "path_embedding = 'gs://pvtrough_asia_south1/clustering/Content_news_embedding.csv'\n",
    "df_embedding = sqlContext.read.csv(path_embedding, sep=',',\n",
    "                         inferSchema=True, header=True)\n",
    "\n",
    "\n",
    "# df_embedding.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e5d04b-b333-46ff-b803-9d018232052d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 10611/24176 [06:32<10:06, 22.35it/s]"
     ]
    }
   ],
   "source": [
    "#generate news embeddings based on notification text\n",
    "newsMapProcessed = {}\n",
    "document_embeddings_content = {}\n",
    "document_embeddings={}\n",
    "embedding_start = time.time()\n",
    "\n",
    "#try:\n",
    "#    already_embedded = df_embedding.select('hid').distinct().collect()\n",
    "#    already_embedded = [x['hid'] for x in already_embedded]\n",
    "#except:\n",
    "#    print(\"News Embeddings not found, generating new one\")\n",
    "already_embedded = []\n",
    "for hId in tqdm(newsMap):\n",
    "    \n",
    "    h = hId.split('-')[0]\n",
    "#     print(h)\n",
    "    if h in already_embedded or (h not in hashIdsWithFilter) :\n",
    "        continue\n",
    "    else:    \n",
    "        newsMapProcessed[h] = {}\n",
    "        newsMapProcessed[h]['title'] = newsMap[hId]['title']\n",
    "        newsMapProcessed[h]['content'] = newsMap[hId]['content']\n",
    "        newsMapProcessed[h]['features'] = newsMapProcessed[h]['title'] + newsMapProcessed[h]['content']\n",
    "        document_embeddings[h] = get_embedding(model, tokenizer, newsMapProcessed[h]['features'])\n",
    "\n",
    "        document_embeddings_content[h] = document_embeddings[h]\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "Log(\"Embedding took %s minutes\"%(int((time.time() - embedding_start) / 60)))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "fe8a9c20-21d5-46fb-8f4f-05b7e08028fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n",
      "\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\n",
      "\tat java.net.ServerSocket.accept(ServerSocket.java:528)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n"
     ]
    }
   ],
   "source": [
    "len(document_embeddings_content['l_ebpzzatk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "d68c4501-f46c-4f6f-9575-f27bc9118e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15280"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_embeddings_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2810d90-4d97-42a7-8e10-6de14ce9eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate news embeddings based on notification text\n",
    "newsMapProcessed = {}\n",
    "document_embeddings = {}\n",
    "try:\n",
    "    already_embedded = df_embedding.select('hid').distinct().collect()\n",
    "    already_embedded = [x['hid'] for x in already_embedded]\n",
    "except:\n",
    "    print(\"News Embeddings not found, generating new one\")\n",
    "    already_embedded = []\n",
    "for hId in tqdm(newsMap):\n",
    "    \n",
    "    h = hId.split('-')[0]\n",
    "#     print(h)\n",
    "    if h in already_embedded or (h not in hashIdsWithFilter) :\n",
    "        continue\n",
    "    else:    \n",
    "        newsMapProcessed[h] = {}\n",
    "        newsMapProcessed[h]['title'] = newsMap[hId]['title']\n",
    "        newsMapProcessed[h]['content'] = newsMap[hId]['content']\n",
    "        newsMapProcessed[h]['features'] = newsMapProcessed[h]['title'] + \".\" + newsMapProcessed[h]['content']\n",
    "        document_embeddings[h] = get_embedding(model, tokenizer, newsMapProcessed[h]['features'])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ed04ff-fbb5-4aeb-a801-f26417394d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "896aa3b6-8dbb-40b3-8a54-e1080bb07639",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_embedding = \"gs://pvtrough_asia_south1/clustering/Content_news_embedding.csv\"+sampled_dates[k].strftime(\"%Y_%m_%d\")+\"/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "2b14692f-29d9-41a6-a247-6df833819641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/23 18:46:55 WARN org.apache.spark.scheduler.TaskSetManager: Stage 3398 contains a task of very large size (48558 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if len(document_embeddings_content) > 0:\n",
    "    df_em_pandas = pd.DataFrame(document_embeddings_content).T\n",
    "    df_em_pandas['hid'] = df_em_pandas.index\n",
    "    df_em_pandas['embed_date'] = sampled_dates[0]\n",
    "    df_em_spark = (sqlContext.createDataFrame(df_em_pandas))\n",
    "    \n",
    "    df_em_spark.write.partitionBy('embed_date').csv(path_embedding , sep=',', header=True,mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b47e947-9fde-47d2-979f-c63d0a3e3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embedding = sqlContext.read.csv(path_embedding, sep=',',\n",
    "                         inferSchema=True, header=True)\n",
    "\n",
    "from pyspark.sql.functions import array, col\n",
    "feat_cols = [str(x) for x in range(0,len(df_embedding.columns) - 2)]\n",
    "df_embedding = df_embedding.select('hid', array([col(x) for x in feat_cols ]).alias('embedding'))\n",
    "df_embedding.take(1)\n",
    "\n",
    "#df_embedding=df_embedding.withColumnRenamed(\"hid\",\"hashId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "b9dd51e2-7f24-4226-b9e4-e6a9d43af83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15280"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_embedding.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999256b3-7924-451d-8174-bf693e5ccc2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884469bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/23 19:16:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "k = 0\n",
    "data_filtered = data_embed\n",
    "# # calculate idf_weight\n",
    "N = data_filtered.select('hashId').distinct().count()\n",
    "users = data_filtered.groupBy('hashId').agg(F.countDistinct('deviceId').alias('users'))\n",
    "idf_weights = users.withColumn('idf_weight', F.log(N / F.col('users')))\n",
    "\n",
    "# Multiply embeddings with IDF weights\n",
    "df_result = df_embedding.join(idf_weights, df_embedding.hid == idf_weights.hashId)\n",
    "df_result = df_result.withColumn('weighted_embedding', F.expr('transform(embedding, (x, i) -> x * idf_weight)'))\n",
    "\n",
    "\n",
    "df_result = df_result.select('hid','weighted_embedding')\n",
    "df_joined = data_filtered.join(df_result, data_filtered.hashId == df_result.hid , 'left')\n",
    "\n",
    "\n",
    "# assuming your dataframe is called df\n",
    "grouped_df = df_joined.groupBy(\"deviceId\").agg(*[\n",
    "    expr(f\"avg(weighted_embedding[{i}]) as avg_{i}\") for i in range(768)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# assuming your dataframe is called grouped_df\n",
    "avg_cols = [col(f\"avg_{i}\") for i in range(768)]\n",
    "grouped_df = grouped_df.withColumn(\"topic_avg_embedding\", array(*avg_cols))\n",
    "\n",
    "path_vec = \"gs://pvtrough_asia_south1/clustering/MLP_user_embeddings\"+sampled_dates[k].strftime(\"%Y_%m_%d\")+\"/\"\n",
    "path_vec_exploded = \"gs://pvtrough_asia_south1/clustering/MLP_device_vectors_exploded_\"+sampled_dates[k].strftime(\"%Y_%m_%d\")+\"/\"\n",
    "print(path_vec, path_vec_exploded)\n",
    "\n",
    "grouped_df.select('deviceId', 'topic_avg_embedding').write.parquet(path=path_vec, mode='overwrite')\n",
    "#grouped_df.select('deviceId', *avg_cols).write.parquet(path=path_vec_exploded, mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c5ea07-a1dd-4757-9eb6-1a270ba1415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0613149b-2a5e-439b-b2df-b777f7fdc573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "8246a8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://pvtrough_asia_south1/clustering/MLP_user_embeddings2023_02_01/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains, size, col\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "k = 0\n",
    "#num_clusters = 1000\n",
    "path_vec = \"gs://pvtrough_asia_south1/clustering/MLP_user_embeddings\"+sampled_dates[k].strftime(\"%Y_%m_%d\")+\"/\"\n",
    "# path_vec = \"gs://pvtrough_asia_south1/clustering/device_vectors_roberta_live/\"\n",
    "print(path_vec)\n",
    "# print(path_vec)\n",
    "\n",
    "temp = sqlContext.read.parquet(path_vec)\n",
    "df_devices_embeddings = temp.filter(~array_contains(temp.topic_avg_embedding, float('nan')))\n",
    "\n",
    "\n",
    "df_devices_embeddings = df_devices_embeddings.dropna(how='all')\n",
    "df_devices_embeddings = df_devices_embeddings.where(size(col(\"topic_avg_embedding\")) == 768)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759d5dc9-a37b-4a12-962e-d4d6ba951237",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_devices_embeddings.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca5edc7-e544-4928-bf95-22afdec91d51",
   "metadata": {},
   "source": [
    "# Reducing the size of vectors using LDA for device and news embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "5c028888-5796-486f-abea-0520534995a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news_embedding = df_news_embedding.where(size(col(\"embedding\")) == 768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bf5ae1-6392-45bf-bd69-d96ed46e9688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a020cae-72b2-4ba3-9025-c15dae595b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef94fad-3ac6-4ffe-875c-fe1e87c965b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b887635-241e-42bd-a405-775196a58b43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "11910734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- deviceId: string (nullable = true)\n",
      " |-- hashId: string (nullable = true)\n",
      " |-- topic_avg_embedding: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- embedding: array (nullable = false)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- notiOpened: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = data_train.join(df_devices_embeddings,[\"deviceId\"],\"inner\")\n",
    "train_data = train_data.join(df_news_embedding,train_data.hashId == df_news_embedding.hid,\"inner\")\n",
    "train_data = train_data.select('deviceId', 'hashId', 'topic_avg_embedding', 'embedding','notiOpened')\n",
    "train_data = train_data.withColumn(\"notiOpened\",F.when(train_data.notiOpened > 1, \n",
    "                                                     1).otherwise(train_data.notiOpened))\n",
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca15578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "train_data.groupby('notiOpened').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aac61cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_length = 256\n",
    "train_data = train_data.dropna(how='all')\n",
    "train_data = train_data.where((size(col(\"topic_avg_embedding\")) == embedding_length) & (size(col(\"embedding\")) == embedding_length))\n",
    "\n",
    "# train_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "83f8fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat\n",
    "\n",
    "train_data_features = train_data.withColumn('embedding_features', \n",
    "                                            concat(col('topic_avg_embedding'), col('embedding')))\\\n",
    "                                .select('embedding_features', 'notiOpened')\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7dda1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47b2631-f552-4b6c-8b81-d3e3ca086669",
   "metadata": {},
   "outputs": [],
   "source": [
    "jhuh=[0.0023, 0.0477, -0.0396, 0.0009, 0.0234, -0.0007, -0.0263, 0.0094, -0.003, -0.0079, 0.0083, 0.0157, 0.0157, 0.0268, -0.0272, 0.016, -0.0234, -0.0126, -0.0291, -0.0232, -0.0609, -0.0379, -0.0736, 0.0412, 0.0053, -0.0663, 0.0372, 0.0556, 0.0942, 0.0222, -0.0089, -0.0569, -0.085, 0.073, -0.0409, 0.0562, 0.0087, 0.0767, 0.0242, 0.1207, 0.3078, -0.0326, -0.0746, 0.0526, -0.0916, -0.1128, 0.0008, -0.1823, -0.0065, 0.0489, 0.001, -0.0053, -0.0069, -0.0497, 0.0191, -0.063, 0.0224, -0.0335, -0.0016, 0.0158, -0.0006, 0.0125, -0.007, 0.0115, -0.0181, -0.0065, -0.0081, -0.0752, 0.0297, 0.0259, 0.0103, 0.1363, 0.0398, 0.0333, -0.0134, 0.0258, -0.0841, 0.1014, 0.0526, -0.1862, -0.0682, 0.16, -0.0917, -0.0708, -0.4389, 0.1636, -0.0263, -0.2365, -0.2017, -0.1257, -0.2023, -0.4874, 0.1562, 0.4067, -0.0379, -0.0119, 0.1549, -0.1315, 0.1141, 0.076]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685271dd-d8c2-4e4f-b23d-e36c71c33e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(jhuh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "01046746-c78f-480b-b572-94d29eb1fa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import array_to_vector\n",
    "\n",
    "train_data_features=train_data_features.select(array_to_vector('embedding_features').alias('vec_embedding'),'notiOpened')#.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3fbc60-2876-4a11-9515-fbbe42680023",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc71db-3ea8-4bb5-8f49-eb7c88744714",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c38d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = data_train.select('deviceId').distinct().collect()\n",
    "d2 = train_data.select('deviceId').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5080f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_out = set(d1) - set(d2)\n",
    "len(left_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6491782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = train_data.sample(fraction = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358665b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = ['embedding_features'], outputCol = 'vector_features')\n",
    "train_data_vector = assembler.transform(train_data_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10d6b68-16cc-4fd9-8e22-6a12a87e86ac",
   "metadata": {},
   "source": [
    "# using underampling for imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "df144252-9a3b-4a78-9b99-00124933fefb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[vec_embedding: vector, notiOpened: bigint]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddeba92-7010-42c6-ac51-4529e4225e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "major_df = train_data_features.filter(col(\"notiOpened\") == 0)\n",
    "minor_df = train_data_features.filter(col(\"notiOpened\") == 1)\n",
    "ratio = int(major_df.count()/minor_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "fbe6adcb-50e7-4abe-b891-1ebe41a7f077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio: 14\n"
     ]
    }
   ],
   "source": [
    "print(\"ratio: {}\".format(ratio))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "374c8f07-81ff-4e97-b0b8-82969f868f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_majority_df = major_df.sample(False, 1/ratio)\n",
    "combined_df_2 = sampled_majority_df.unionAll(minor_df)\n",
    "#combined_df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "9a796976-31f1-4ceb-90d4-3b7ed98dc122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/23 19:50:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1022.2 KiB\n",
      "23/06/23 19:50:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1053.7 KiB\n",
      "23/06/23 19:51:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1339.2 KiB\n",
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n",
      "\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\n",
      "\tat java.net.ServerSocket.accept(ServerSocket.java:528)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "83031423"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df_2.count()# 83031423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3529dae-dee3-4fb9-89a1-249ed50178cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "bb9c4ae9-52f1-4f65-b1fd-910efe895593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[vec_embedding: vector, notiOpened: bigint]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aa2504-0101-47e5-879a-e10d0cbfe161",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "a0442981-e5c4-4ed8-aa13-c398f36cb690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/23 19:53:10 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1566.0 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path_vec=path_vec = \"gs://pvtrough_asia_south1/clustering/MLP_undersampled_train\"+sampled_dates[k].strftime(\"%Y_%m_%d\")+\"/\"\n",
    "\n",
    "combined_df_2.select('vec_embedding', 'notiOpened').write.parquet(path=path_vec, mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a34c5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4312:=========>(1829 + 1) / 1830][Stage 4320:===========>(198 + 2) / 200]1) / 200]]\r"
     ]
    }
   ],
   "source": [
    "\n",
    "training_start = time.time()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import expr, udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "data = combined_df_2.select(\"vec_embedding\", \"notiOpened\").sample(fraction = 0.01)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(training_data, testing_data) = data.randomSplit([0.7, 0.3])#.sample(fraction)\n",
    "\n",
    "# Define the deep learning model architecture\n",
    "layers = [1536,1400,1000,600,450,300,150,50,10,2]  # Adjust the layer sizes as needed\n",
    "classifier = MultilayerPerceptronClassifier(layers=layers, labelCol=\"notiOpened\", featuresCol=\"vec_embedding\",solver='gd')\n",
    "# add loss function to check training ho rha hai ya nhi\n",
    "# Train the model\n",
    "model_class = classifier.fit(training_data)\n",
    "result = model_class.transform(testing_data)\n",
    "# # Make predictions on the testing data\n",
    "# predictions = model.transform(testing_data)\n",
    "\n",
    "# # Evaluate the model\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol=\"notiOpened\", rawPredictionCol=\"prediction\")\n",
    "# areaUnderROC = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "# print(\"Area under ROC curve:\", areaUnderROC)\n",
    "Log(\"Model fitting took %s minutes\"%(int((time.time() - training_start) / 60)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33504f30-85ad-4ee6-9f5e-3bfc9df6e10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add loss function to check training ho rha hai ya nhi, \n",
    "# reduce layers size to half\n",
    "# dimension reduction using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b13d8-158f-4431-a39a-91680869e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = result.select(['notiOpened']).collect()\n",
    "y_pred = result.select(['prediction']).collect()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "Log(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb3762-93c7-4a5c-a397-2f52d2e2e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_and_labels = result.select(['prediction','notiOpened']).withColumn('label', F.col('notiOpened').cast(DoubleType())).orderBy('prediction')\n",
    "preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "cm=metrics.confusionMatrix().toArray()\n",
    "Log(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "2cf706b3-03e4-4802-b805-f8227fbccaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5005877822368154 nan 0.0 nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24395/3738225083.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  precision=(cm[1][1])/(cm[0][1]+cm[1][1])\n"
     ]
    }
   ],
   "source": [
    "accuracy=(cm[0][0]+cm[1][1])/cm.sum()\n",
    "precision=(cm[1][1])/(cm[0][1]+cm[1][1])\n",
    "recall=(cm[1][1])/(cm[1][0]+cm[1][1])\n",
    "f1_score=(2*precision*recall)/(recall+precision)\n",
    "print(accuracy, precision, recall,f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68d09d3-7a3d-4b4a-be5d-da5a8b519411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb83ef6-0dd0-4c14-af8f-92e9e5d53662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea4c84-d4bb-46a1-9ad5-87e89afbd97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8204fd-c0aa-4198-8a9a-af60eebc3333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create classifier on very high precision on a) for opening b) for not opening c) with imbalance data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcca389-5a65-431d-abf1-f6e3597782e2",
   "metadata": {},
   "source": [
    "# Trying logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cec0e1f-8cba-4f4b-bf18-ca52474501c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next two blocks for class weight\n",
    "y_collect = train_data_features.select(\"notiOpened\").groupBy(\"notiOpened\").count().collect()\n",
    "unique_y = [x[\"notiOpened\"] for x in y_collect]\n",
    "total_y = sum([x[\"count\"] for x in y_collect])\n",
    "unique_y_count = len(y_collect)\n",
    "bin_count = [x[\"count\"] for x in y_collect]\n",
    "\n",
    "class_weights_spark = {i: ii for i, ii in zip(unique_y, total_y / (unique_y_count * np.array(bin_count)))}\n",
    "print(class_weights_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1734c0-1d88-454d-80ef-6bbef3b760c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_expr = F.create_map([F.lit(x) for x in chain(*class_weights_spark.items())])\n",
    "\n",
    "train_data_features = train_data_features.withColumn(\"weight\", mapping_expr.getItem(F.col(\"y\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4330f523-c31f-4f43-bfac-7af3d90dcc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_start = time.time()\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "data = combined_df_2.select(\"vec_embedding\", \"notiOpened\")#.sample(fraction = 0.8)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(training_data, testing_data) = data.randomSplit([0.7, 0.3])\n",
    "lr = LogisticRegression(featuresCol=\"vec_embedding\", labelCol=\"notiOpened\")\n",
    "lrModel = lr.fit(training_data)\n",
    "result = lrModel.transform(testing_data)\n",
    "Log(\"Model training took %s minutes\"%(int((time.time() - training_start) / 60)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e912fd37-5ae8-43db-a8c9-e59401c4ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = result.select(['notiOpened']).collect()\n",
    "y_pred = result.select(['prediction']).collect()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "Log(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "bd047119-cb57-4d77-bdd3-a32f4e5191ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5426:============================>                           (1 + 1) / 2] / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[74910. 51053.]\n",
      " [50202. 72411.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# sample of 0.01\n",
    "preds_and_labels = result.select(['prediction','notiOpened']).withColumn('label', F.col('notiOpened').cast(DoubleType())).orderBy('prediction')\n",
    "preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "cm=metrics.confusionMatrix().toArray()\n",
    "Log(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "e21b81b8-52ce-4221-b32d-3b64ea2a98c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5926597901647785 0.5864948487008359 0.5905654375963397 0.5885231045567038\n"
     ]
    }
   ],
   "source": [
    "#sample of 0.01\n",
    "accuracy=(cm[0][0]+cm[1][1])/cm.sum()\n",
    "precision=(cm[1][1])/(cm[0][1]+cm[1][1])\n",
    "recall=(cm[1][1])/(cm[1][0]+cm[1][1])\n",
    "f1_score=(2*precision*recall)/(recall+precision)\n",
    "print(accuracy, precision, recall,f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "345f49d3-a28f-46b3-950a-1043b2c15d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6259:============================>                           (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7604563. 5016202.]\n",
      " [5065028. 7222488.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#complete data\n",
    "preds_and_labels = result.select(['prediction','notiOpened']).withColumn('label', F.col('notiOpened').cast(DoubleType())).orderBy('prediction')\n",
    "preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "cm=metrics.confusionMatrix().toArray()\n",
    "Log(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "11b5162b-bde3-4b71-92a1-28544f67f94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5952659278253686 0.590135708968852 0.5877907300385204 0.5889608853485125\n"
     ]
    }
   ],
   "source": [
    "#complete data\n",
    "accuracy=(cm[0][0]+cm[1][1])/cm.sum()\n",
    "precision=(cm[1][1])/(cm[0][1]+cm[1][1])\n",
    "recall=(cm[1][1])/(cm[1][0]+cm[1][1])\n",
    "f1_score=(2*precision*recall)/(recall+precision)\n",
    "print(accuracy, precision, recall,f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21361531-273b-4f3f-b8a4-dd66d8c49927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2a130ba-4347-48db-9f02-fd6cf3aa6f09",
   "metadata": {},
   "source": [
    "# on training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d9fe7-2fe5-46a7-a96a-730f6caeb2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.60      0.60  29454134\n",
      "           1       0.59      0.59      0.59  28669008\n",
      "\n",
      "    accuracy                           0.60  58123142\n",
      "   macro avg       0.60      0.60      0.60  58123142\n",
      "weighted avg       0.60      0.60      0.60  58123142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_training = lrModel.transform(training_data)\n",
    "y_true = result_training.select(['notiOpened']).collect()\n",
    "y_pred = result_training.select(['prediction']).collect()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "Log(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69c5454-f7cc-4673-93ba-cb3dcf08cd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6316:============================>                           (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17752136. 11701998.]\n",
      " [11818570. 16850438.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "preds_and_labels = result_training.select(['prediction','notiOpened']).withColumn('label', F.col('notiOpened').cast(DoubleType())).orderBy('prediction')\n",
    "preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "cm=metrics.confusionMatrix().toArray()\n",
    "Log(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c9dc3f-1fbb-4b93-a50e-006af2313d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5953321312189214 0.5901576313838861 0.5877579719535465 0.5889553573656757\n"
     ]
    }
   ],
   "source": [
    "#sample of 0.01\n",
    "accuracy=(cm[0][0]+cm[1][1])/cm.sum()\n",
    "precision=(cm[1][1])/(cm[0][1]+cm[1][1])\n",
    "recall=(cm[1][1])/(cm[1][0]+cm[1][1])\n",
    "f1_score=(2*precision*recall)/(recall+precision)\n",
    "print(accuracy, precision, recall,f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4890c52-f993-4826-b821-0bd95b00ae28",
   "metadata": {},
   "source": [
    "# Training on 10 days and predicting on 7 days data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "b916a9b8-cf46-49ac-9ab0-dc63b960ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_embedding = \"gs://pvtrough_asia_south1/clustering/Content_news_embedding.csv\"+sampled_dates[k].strftime(\"%Y_%m_%d\")+\"/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "a7ec4e10-70d3-408e-a801-62f8f5eef6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(hid='svovy3mz', embedding=[-0.026875313371419907, 0.1945246458053589, 0.02433736063539982, -0.09676317125558853, 0.16876408457756042, 0.006928965449333191, 0.01006230991333723, 0.04343455657362938, 0.001783062587492168, -0.05376378819346428, -0.03129350394010544, -0.11458952724933624, 0.05109705775976181, -0.10799243301153183, 0.02900136634707451, -0.02166307531297207, 0.1632353961467743, -0.02483336813747883, -0.08497937023639679, 0.25693243741989136, 0.028930554166436195, 0.08322273194789886, -0.14118240773677826, -0.162668839097023, -0.010364331305027008, -0.020930470898747444, 0.06786119937896729, 0.1229231059551239, -0.05521037429571152, 0.016381191089749336, 0.006578350905328989, -0.07388663291931152, 0.09268548339605331, 0.09028810262680054, -0.043422721326351166, 0.05357334017753601, 0.07575681805610657, 0.03570711612701416, 0.3640774190425873, -0.007589297369122505, -0.04624001309275627, 0.0024187632370740175, -0.09080038219690323, 0.002034494187682867, -0.03369442746043205, 0.09223038703203201, -0.08667536079883575, 0.06114588677883148, 0.043825335800647736, 0.08694221079349518, -0.16980025172233582, 0.0954771488904953, 0.044546257704496384, -0.04534754157066345, 0.04052549973130226, 0.02187351882457733, 0.07238206267356873, 0.050710536539554596, 0.1481563001871109, -0.10210961103439331, -0.04814773052930832, 0.25381842255592346, -0.08740101009607315, -0.051861878484487534, 0.04442398250102997, -0.08338622748851776, 0.045190103352069855, 0.10778442770242691, 0.037162840366363525, 0.21535758674144745, -0.04159730672836304, -0.1419164389371872, 0.020884672179818153, -0.18582822382450104, 0.06468304246664047, 0.14179624617099762, 0.0023109482135623693, -6.8723602294921875, -0.1485593616962433, -0.0015786625444889069, -0.0243090707808733, -0.05896935611963272, 1.541007399559021, 0.05852126330137253, 0.10888901352882385, -0.35683342814445496, 0.0702618956565857, 0.023382913321256638, 0.009697210974991322, 0.052695900201797485, 0.1759069710969925, 0.01854867674410343, 0.14026083052158356, 0.14013724029064178, 0.02656138315796852, 0.07758722454309464, 0.10602835565805435, 0.29031887650489807, 0.06204957887530327, 0.05728638544678688, 0.13911126554012299, -0.03622305020689964, -0.036115605384111404, 0.12827011942863464, -0.01550320629030466, -0.16749097406864166, -0.028579222038388252, -0.10803612321615219, 0.06388536840677261, 0.0379033200442791, 0.04939479008316994, 0.1412270963191986, 0.04305171221494675, -0.04877016320824623, 0.003824337152764201, 0.044319771230220795, 0.005176925100386143, 0.025860771536827087, 0.09302780032157898, -0.023422440513968468, 0.0127518055960536, -0.05230606719851494, 0.189875528216362, -0.0004743692697957158, 0.05012701824307442, 0.2574861943721771, -0.14786787331104279, 0.04744134098291397, 0.026153409853577614, -0.02348361350595951, -0.052148107439279556, -0.5693292617797852, -0.01170708891004324, 0.025650670751929283, 0.0303775817155838, 0.023928334936499596, -0.023371312767267227, 0.08816159516572952, -0.0938500314950943, -0.09840911626815796, 0.12706421315670013, 0.12074581533670425, 0.01640474423766136, -0.02810550294816494, -0.13288354873657227, 0.07608244568109512, -0.07666955143213272, -0.12093771994113922, 0.058824408799409866, 0.05053682625293732, 0.059821657836437225, -0.09610898047685623, 0.06295082718133926, -0.036316514015197754, -0.061740193516016006, -0.06370070576667786, 0.07941155880689621, 0.18369217216968536, 0.061280395835638046, 0.3305308222770691, 0.010100970976054668, 0.23543427884578705, -0.07267622649669647, -0.05684437230229378, -0.04728735238313675, 0.04422646388411522, -0.025732768699526787, 0.12456841766834259, -0.17042338848114014, 0.03884283825755119, -0.001163537846878171, 0.014816253446042538, 0.017931349575519562, 0.09578631818294525, -0.09799465537071228, -0.0038762963376939297, 0.07780997455120087, 0.11988411843776703, -0.05317464470863342, -0.06058993190526962, 0.05304022133350372, 0.0486600436270237, 0.017901215702295303, 0.07765865325927734, 0.012376547791063786, -0.007924665696918964, 0.06668946146965027, 0.1426086276769638, 0.042201753705739975, -0.12682493031024933, 0.038796696811914444, 0.01747061498463154, 0.03711871802806854, 0.15807797014713287, -0.0814492404460907, -0.027369191870093346, 0.092432901263237, -0.081514373421669, -0.04806564003229141, 0.02827508933842182, -0.19167977571487427, 0.005717881955206394, 0.07131033390760422, -0.124134860932827, 0.26895904541015625, 0.035508930683135986, -0.03317182511091232, 0.09484314918518066, -0.018521057441830635, -0.14232830703258514, 0.0026402799412608147, 0.053442806005477905, 0.03277117758989334, 0.13682091236114502, -0.017397990450263023, -0.04719213768839836, 0.004003466572612524, -0.9719285368919373, -0.03863724693655968, -0.45486563444137573, 0.043160177767276764, 0.0074066887609660625, 0.05151991546154022, -0.0725102350115776, 0.12818580865859985, -0.057493746280670166, 0.07810606807470322, -0.011704356409609318, -0.02144262194633484, 0.15147723257541656, -0.011446935124695301, -0.02485118806362152, -0.18571151793003082, 0.016429757699370384, 0.07060256600379944, -0.05659184604883194, -0.0797475129365921, 0.02171161398291588, 0.09712805598974228, 0.08817049860954285, 0.07905423641204834, 0.0921645313501358, 0.05704538896679878, 0.11128716170787811, 0.08377324789762497, -0.029820529744029045, -0.08214779198169708, 0.15464231371879578, 0.20060239732265472, 0.06685569882392883, -0.02682969532907009, -0.16371379792690277, -0.05076389014720917, -0.062272585928440094, -0.03072439879179001, -0.10788975656032562, -0.05350576341152191, -0.008578834123909473, 0.05430369824171066, -0.145062655210495, 0.027345385402441025, -0.0012910558143630624, 0.05600496381521225, -0.10835206508636475, -0.15492072701454163, 0.13311521708965302, -0.0728459283709526, 0.061126064509153366, 0.11796320229768753, 0.05303232744336128, 0.08167867362499237, 0.036095187067985535, -0.21676945686340332, 0.11663556098937988, -0.013304275460541248, 0.09813420474529266, -0.005202090367674828, -0.044746097177267075, 0.06912434846162796, 0.05346515774726868, -0.0008890341850928962, 0.08403481543064117, -0.3189665973186493, -0.18921971321105957, -0.054191648960113525, -0.014027130790054798, -0.10599575936794281, 0.05874066427350044, 0.04788783937692642, 0.014723327942192554, -0.03666335344314575, 0.052624888718128204, 0.04223320633172989, 0.08532179892063141, -0.2119707316160202, -0.06810861080884933, -0.0534856878221035, 0.06869006901979446, 0.002284308662638068, 0.10526403039693832, 0.08965189009904861, -0.19561654329299927, 0.05311140418052673, -0.053318463265895844, -0.10345399379730225, -0.11054638773202896, 0.012491296976804733, -0.013895846903324127, -0.08886152505874634, 0.1914050579071045, -0.029212288558483124, -0.026063289493322372, -0.09706927835941315, 0.02255840227007866, 0.006846563424915075, -0.004653365351259708, 0.1328514665365219, 0.010230842977762222, 0.023040669038891792, -0.09239836782217026, 0.08737263828516006, -0.12866446375846863, 0.07030131667852402, -0.009554448537528515, -0.12437676638364792, -0.042904190719127655, -0.14223599433898926, -0.07709619402885437, 0.147563636302948, -0.023328935727477074, 1.2320364713668823, 0.7456817030906677, 0.03519555926322937, 0.17987622320652008, -0.002711711684241891, 0.16615726053714752, -0.07855938374996185, 0.0472584143280983, 0.03556651622056961, 0.14232195913791656, 0.03874513879418373, 0.050349630415439606, -0.08088962733745575, 0.1051677018404007, 0.04724046587944031, 0.024487877264618874, 0.0003093263367190957, -0.10144580155611038, -0.03746140003204346, -0.040465496480464935, -0.0436992272734642, 0.05435359477996826, 0.04078400507569313, 0.015191181562840939, -0.05202799662947655, -0.001356506603769958, -0.02251988649368286, 0.09323485940694809, -0.017627641558647156, -0.23317170143127441, 0.1071913093328476, -0.07578686624765396, -0.0314205177128315, 0.0533951036632061, 0.04548667371273041, -0.05343877524137497, -0.07244746387004852, 0.09402074664831161, 0.07949507981538773, 0.031249435618519783, 0.07180431485176086, -0.0014343832153826952, -0.012275469489395618, -0.07381687313318253, -0.041921962052583694, -0.056304119527339935, -0.002966467523947358, 0.07191938906908035, 0.04022744670510292, 0.06485530734062195, -0.11001750826835632, -0.0960293561220169, 0.012729266658425331, -0.024855198338627815, 0.0378873310983181, -0.22092857956886292, -0.11769154667854309, -0.23397070169448853, -0.036456868052482605, 0.0184054933488369, 0.11566025763750076, 0.03371414542198181, 0.1510666012763977, -0.036823008209466934, -0.007269016467034817, 0.05737852305173874, 0.0004163332632742822, -0.299025297164917, 0.16910715401172638, 0.057787660509347916, 0.008919458836317062, 0.08116757124662399, 0.0036374337505549192, -0.10174249857664108, -0.06416652351617813, 0.011258693411946297, 0.13967756927013397, -0.09331341832876205, 0.06619023531675339, -0.016258738934993744, 0.11263089627027512, -0.032141607254743576, 0.09182047843933105, 0.04012656956911087, -0.08908016979694366, -0.04662598669528961, 0.048881348222494125, -0.006253420375287533, 0.06775863468647003, -0.027720730751752853, 0.035713233053684235, -0.05716056376695633, 0.25803515315055847, 0.04801402613520622, 0.34125420451164246, 0.043037641793489456, 0.10207606852054596, 0.040005456656217575, 0.15021617710590363, -0.05701098218560219, -0.10039062798023224, 0.019543875008821487, -0.2635963559150696, 0.059311967343091965, 0.04556197673082352, 0.03583092242479324, -0.07068183273077011, -0.09928502142429352, -0.00392859848216176, 0.004801923409104347, -0.0009874006500467658, 0.016397828236222267, 0.11504403501749039, 0.07473499327898026, 0.09913099557161331, 0.06942229717969894, 0.11854062229394913, -0.02966459095478058, 0.07646014541387558, 0.19679579138755798, 0.0960056483745575, 0.10080233961343765, -0.1374969184398651, -1.3554680347442627, 0.005693379789590836, 0.07238959521055222, 0.06960189342498779, -0.07234682887792587, 0.03735234588384628, -0.003208975773304701, 0.0069719585590064526, -0.06737072765827179, 0.03374319523572922, 0.07707411050796509, 0.058722950518131256, -0.048285774886608124, -0.08056271821260452, 0.1879502832889557, -0.14687718451023102, -0.004801824223250151, 0.014257808215916157, -0.0011305208317935467, 0.06671936810016632, -0.1821708083152771, 0.051315564662218094, 0.007867780514061451, 0.13109371066093445, 0.15276527404785156, 0.05739662051200867, 0.05155743286013603, 0.09533046931028366, 0.06099584698677063, -0.015522331930696964, -0.015240254811942577, -0.023905130103230476, 0.07136506587266922, 0.01648080162703991, -0.01798589527606964, 0.0012994535500183702, -0.03207122161984444, 0.024893702939152718, 0.009191781282424927, 0.002613438991829753, -0.005221118684858084, 0.15281942486763, 0.010126229375600815, 0.20467565953731537, 0.10111059993505478, -0.01811797358095646, -0.023433297872543335, -0.06775125861167908, 0.009764175862073898, 0.21713171899318695, -0.021453360095620155, 0.00722924480214715, 0.03254087641835213, 0.014130007475614548, -0.09336245805025101, 0.0714762806892395, -0.03423885628581047, 0.0238895732909441, -0.10274968296289444, 0.06705858558416367, 0.008308258838951588, 0.026831094175577164, -0.13797324895858765, -0.10544779151678085, -0.06710071116685867, 0.2776769995689392, 0.1337103247642517, -0.0475563108921051, -0.031415145844221115, -0.02676931396126747, 0.12802854180335999, -0.06000661104917526, 0.003577656578272581, 0.10844424366950989, -0.07934872061014175, 0.0057246810756623745, 0.03643517568707466, -0.015864966437220573, -0.029229847714304924, 0.05332371965050697, 0.10761967301368713, -0.05309126526117325, 0.0713983103632927, 0.04218892753124237, 0.0743192806839943, 0.004302362445741892, 0.15825045108795166, -0.010910356417298317, 0.13510991632938385, 0.14969123899936676, 0.06807462126016617, 0.22382016479969025, -0.05844010040163994, 0.10499653220176697, -0.056218765676021576, 0.00290654762648046, 0.07395603507757187, 0.07511603832244873, 1.0807592868804932, 0.03391594812273979, -0.0770629495382309, 0.19175054132938385, 0.03285382688045502, -0.038249727338552475, 0.2730986773967743, 0.0023287872318178415, 0.042511653155088425, 0.018341336399316788, 0.10757151246070862, -0.04410802945494652, 0.025200774893164635, 0.09638354182243347, 0.019173890352249146, 0.02627195604145527, -0.12656907737255096, -0.04293966293334961, -0.007558520883321762, 0.14094725251197815, 0.10143597424030304, -0.22802148759365082, -0.010575276799499989, 0.004632078111171722, 0.04532312974333763, -0.050942596048116684, 0.05123716592788696, 0.054557137191295624, 0.04666668549180031, 0.017727646976709366, 0.08952829241752625, 0.023125899955630302, -0.0040930211544036865, 0.010427327826619148, 0.09482348710298538, 0.02657974883913994, -0.12059081345796585, 9.186250686645508, 0.016892101615667343, -0.002797539345920086, 0.05035785213112831, 0.08852849155664444, 0.010954228229820728, 0.011370689608156681, 0.0007839392637833953, 0.07162696868181229, 0.11463010311126709, 0.02732192538678646, -0.14373429119586945, 0.05578694865107536, -0.10124685615301132, 0.03747684881091118, -0.07410845905542374, 0.05364847928285599, -0.0227277223020792, 0.029699400067329407, 0.06083807721734047, 0.13197989761829376, 0.11248519271612167, -0.004295621532946825, 0.7837474942207336, 0.04585354030132294, 0.06789051741361618, 0.08766526728868484, 0.023588627576828003, -0.0638863816857338, -0.11106456816196442, 0.15902988612651825, -0.05933689698576927, 0.03361048549413681, 0.10777387022972107, 0.12946593761444092, -0.0005633750697597861, -0.22193770110607147, -0.005432246718555689, 0.01073230430483818, -0.10689839720726013, 0.03236471116542816, 0.030414894223213196, -0.014198101125657558, 0.056279636919498444, 0.04368212819099426, 0.016256950795650482, 0.00674782320857048, 0.04624741151928902, 0.10876799374818802, -0.15301825106143951, 0.08502079546451569, 0.12323605269193649, -0.00014064920833334327, -0.12214212119579315, -0.013410442508757114, 0.22314268350601196, -0.045343998819589615, -0.028817562386393547, 0.09751633554697037, 0.062088027596473694, -0.010568341240286827, 0.14050982892513275, -0.01955467276275158, 0.17626014351844788, -0.044670939445495605, 0.017437627539038658, 0.05514436960220337, 0.10355061292648315, 0.09568317979574203, -0.05969151481986046, -0.023639455437660217, -0.05031437426805496, 0.02029857039451599, 0.0682918056845665, 0.033558666706085205, -0.020497538149356842, 0.40838563442230225, -0.019571833312511444, -0.08508356660604477, -0.03703799098730087, 0.009991617873311043, 0.1427609920501709, -0.057176776230335236, 0.02713296189904213, 0.10564251989126205, 0.08501213788986206, 0.08242114633321762, -0.02306465245783329, 0.05775136500597, -0.006271045189350843, 0.03320765495300293, 0.16196338832378387, 0.10599721223115921, -0.06718358397483826, -0.04908137395977974, -0.12565754354000092, -0.0876779779791832, -0.0008180672884918749, 0.16055947542190552, 0.02537522464990616, -0.0603165365755558, -0.03146211802959442, -0.03313753008842468, 0.020410869270563126, 0.12427305430173874, -0.025124212726950645, -0.06052703782916069, -0.22280089557170868, -0.014182612299919128, 0.013576000928878784, -0.028624724596738815, 0.11334191262722015, 0.1989130973815918, 0.09526524692773819, -0.06043041869997978, 0.03739462420344353, 0.05611808970570564, -0.006062293890863657, 0.09435728192329407, 0.008836029097437859, -0.05401206389069557, -0.0849197581410408, -0.08029846847057343, -0.04492323473095894, 0.04947192221879959, 0.13093271851539612, 0.040341347455978394, -0.030470533296465874, -0.020519323647022247, 0.035674337297677994, 0.007711993996053934, 0.132970929145813, 0.06433214247226715, -0.0027767373248934746, 0.04802876338362694, -0.03162355348467827, 0.019208598881959915, -0.039439618587493896, 0.019644247367978096, 0.03075932152569294, 0.05842285603284836, 0.04658685624599457, 0.12988309562206268, -0.5614086389541626, 0.08871667087078094, -0.0068311006762087345, -0.12379113584756851, -0.11676673591136932, 0.002104750834405422, -0.06425211578607559, -0.13820333778858185, 0.017501207068562508, 0.007290655747056007, -0.09886584430932999, 0.023577015846967697, 0.05216231197118759, 0.03870539739727974, 0.20200850069522858, 0.12627744674682617, -0.09352757781744003, -0.07100357115268707, 0.08352860063314438, 0.07599987834692001, 0.15023598074913025, 0.037654705345630646, 0.06859089434146881, -0.15589579939842224, 0.1473063826560974, -0.04252004623413086, 0.03994534909725189, 0.06339692324399948, -0.052479665726423264, -0.09990664571523666, 0.012803031131625175, 0.1383216232061386, 0.06991474330425262, -0.16606482863426208, 0.06426792591810226, 0.02787962555885315, -0.059284795075654984])]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_embedding = sqlContext.read.csv(path_embedding, sep=',',\n",
    "                         inferSchema=True, header=True)\n",
    "\n",
    "from pyspark.sql.functions import array, col\n",
    "feat_cols = [str(x) for x in range(0,len(df_embedding.columns) - 2)]\n",
    "df_embedding = df_embedding.select('hid', array([col(x) for x in feat_cols ]).alias('embedding'))\n",
    "df_embedding.take(1)\n",
    "\n",
    "#df_embedding=df_embedding.withColumnRenamed(\"hid\",\"hashId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a3e1ff-5bbd-4560-9e72-84aca1a14957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6357:==================>                                (71 + 129) / 200]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "k = 0\n",
    "data_filtered = data_comb\n",
    "# # calculate idf_weight\n",
    "N = data_filtered.select('hashId').distinct().count()\n",
    "users = data_filtered.groupBy('hashId').agg(F.countDistinct('deviceId').alias('users'))\n",
    "idf_weights = users.withColumn('idf_weight', F.log(N / F.col('users')))\n",
    "\n",
    "# Multiply embeddings with IDF weights\n",
    "df_result = df_embedding.join(idf_weights, df_embedding.hid == idf_weights.hashId)\n",
    "df_result = df_result.withColumn('weighted_embedding', F.expr('transform(embedding, (x, i) -> x * idf_weight)'))\n",
    "\n",
    "\n",
    "df_result = df_result.select('hid','weighted_embedding')\n",
    "df_joined = data_filtered.join(df_result, data_filtered.hashId == df_result.hid , 'left')\n",
    "\n",
    "\n",
    "# assuming your dataframe is called df\n",
    "grouped_df = df_joined.groupBy(\"deviceId\").agg(*[\n",
    "    expr(f\"avg(weighted_embedding[{i}]) as avg_{i}\") for i in range(768)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# assuming your dataframe is called grouped_df\n",
    "avg_cols = [col(f\"avg_{i}\") for i in range(768)]\n",
    "grouped_df = grouped_df.withColumn(\"topic_avg_embedding\", array(*avg_cols))\n",
    "\n",
    "path_vec = \"gs://pvtrough_asia_south1/clustering/MLP_user_embeddings40\"+sampled_dates[k].strftime(\"%Y_%m_%d\")+\"/\"\n",
    "path_vec_exploded = \"gs://pvtrough_asia_south1/clustering/MLP_device_vectors_exploded_40\"+sampled_dates[k].strftime(\"%Y_%m_%d\")+\"/\"\n",
    "print(path_vec, path_vec_exploded)\n",
    "\n",
    "grouped_df.select('deviceId', 'topic_avg_embedding').write.parquet(path=path_vec, mode='overwrite')\n",
    "#grouped_df.select('deviceId', *avg_cols).write.parquet(path=path_vec_exploded, mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "e2e41089-daf3-45ae-a682-b1e11182b806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://pvtrough_asia_south1/clustering/MLP_user_embeddings402023_02_01/\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains, size, col\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "k = 0\n",
    "#num_clusters = 1000\n",
    "path_vec = \"gs://pvtrough_asia_south1/clustering/MLP_user_embeddings40\"+sampled_dates[k].strftime(\"%Y_%m_%d\")+\"/\"\n",
    "# path_vec = \"gs://pvtrough_asia_south1/clustering/device_vectors_roberta_live/\"\n",
    "print(path_vec)\n",
    "# print(path_vec)\n",
    "\n",
    "temp = sqlContext.read.parquet(path_vec)\n",
    "df_devices_embeddings = temp.filter(~array_contains(temp.topic_avg_embedding, float('nan')))\n",
    "\n",
    "\n",
    "df_devices_embeddings = df_devices_embeddings.dropna(how='all')\n",
    "df_devices_embeddings = df_devices_embeddings.where(size(col(\"topic_avg_embedding\")) == 768)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "57c327af-cf08-4988-aa3a-d0c2b01a69d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hid: string, embedding: array<double>]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "0875113f-5872-40e3-8a4c-3b9b7584278d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- deviceId: string (nullable = true)\n",
      " |-- hashId: string (nullable = true)\n",
      " |-- topic_avg_embedding: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- embedding: array (nullable = false)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- notiOpened: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = data_test.join(df_devices_embeddings,[\"deviceId\"],\"inner\")\n",
    "test_data = test_data.join(df_news_embedding,test_data.hashId == df_news_embedding.hid,\"inner\")\n",
    "test_data = test_data.select('deviceId', 'hashId', 'topic_avg_embedding', 'embedding','notiOpened')\n",
    "test_data = test_data.withColumn(\"notiOpened\",F.when(test_data.notiOpened > 1, \n",
    "                                                     1).otherwise(test_data.notiOpened))\n",
    "test_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "fdfce183-743b-471c-97bd-305a514ef1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat\n",
    "\n",
    "test_data_features = test_data.withColumn('embedding_features', \n",
    "                                            concat(col('topic_avg_embedding'), col('embedding')))\\\n",
    "                                .select('embedding_features', 'notiOpened')\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "cb0808d8-7505-483d-8be1-e625ac3f9c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import array_to_vector\n",
    "\n",
    "test_data_features=test_data_features.select(array_to_vector('embedding_features').alias('vec_embedding'),'notiOpened')#.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deeb614-42ff-45aa-b780-a9e80f04c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vec = \"gs://pvtrough_asia_south1/clustering/test_data\"+sampled_dates[k].strftime(\"%Y_%m_%d\")+\"/\"\n",
    "\n",
    "test_data_features.select('vec_embedding', 'notiOpened').write.parquet(path=path_vec, mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de9497f-ebc0-4b37-9895-f36ce253dda6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20d68e6-3c32-4c41-9c83-4ae1b5349c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "Log(\"training started\")\n",
    "\n",
    "\n",
    "testing_data=test_data_features\n",
    "\n",
    "#test data\n",
    "result = lrModel.transform(testing_data)\n",
    "\n",
    "y_true = result.select(['notiOpened']).collect()\n",
    "y_pred = result.select(['prediction']).collect()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "Log(\"Classification report test data\")\n",
    "\n",
    "Log(classification_report(y_true, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preds_and_labels = result.select(['prediction','notiOpened']).withColumn('label', F.col('notiOpened').cast(DoubleType())).orderBy('prediction')\n",
    "preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "cm=metrics.confusionMatrix().toArray()\n",
    "Log(\"confusion matrix test data\")\n",
    "\n",
    "Log(cm)\n",
    "\n",
    "#sample of 0.01\n",
    "accuracy=(cm[0][0]+cm[1][1])/cm.sum()\n",
    "precision=(cm[1][1])/(cm[0][1]+cm[1][1])\n",
    "recall=(cm[1][1])/(cm[1][0]+cm[1][1])\n",
    "f1_score=(2*precision*recall)/(recall+precision)\n",
    "print(accuracy, precision, recall,f1_score)\n",
    "Log(\" accuracy, precision, recall, f1_score for test data\")\n",
    "Log(\" accuracy on test data = %g\" % accuracy)\n",
    "Log(\" precision on test data = %g\" % precision)\n",
    "Log(\" recall on test data = %g\" % recall)\n",
    "Log(\" f1_score on test data = %g\" % f1_score)\n",
    "\n",
    "\n",
    "Log(\"Model training took %s minutes\"%(int((time.time() - training_start) / 60)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e3c331-f49f-4c6a-8c1a-65d7e019f726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b68d4a-14e8-441e-b082-f128847d7741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ad72db-d030-45cf-8c31-dc6e13737575",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_start = time.time()\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "training_data = combined_df_2.select(\"vec_embedding\", \"notiOpened\")#.sample(fraction = 0.8)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "#(training_data, testing_data) = data.randomSplit([0.7, 0.3])\n",
    "testing_data=test_data_features\n",
    "lr = LogisticRegression(featuresCol=\"vec_embedding\", labelCol=\"notiOpened\")\n",
    "lrModel = lr.fit(training_data)\n",
    "\n",
    "result_training = lrModel.transform(training_data)\n",
    "y_true = result_training.select(['notiOpened']).collect()\n",
    "y_pred = result_training.select(['prediction']).collect()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "Log(\"Classification report training data\")\n",
    "\n",
    "Log(classification_report(y_true, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preds_and_labels = result_training.select(['prediction','notiOpened']).withColumn('label', F.col('notiOpened').cast(DoubleType())).orderBy('prediction')\n",
    "preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "cm=metrics.confusionMatrix().toArray()\n",
    "Log(\"confusion matrix training data\")\n",
    "\n",
    "Log(cm)\n",
    "\n",
    "#sample of 0.01\n",
    "accuracy=(cm[0][0]+cm[1][1])/cm.sum()\n",
    "precision=(cm[1][1])/(cm[0][1]+cm[1][1])\n",
    "recall=(cm[1][1])/(cm[1][0]+cm[1][1])\n",
    "f1_score=(2*precision*recall)/(recall+precision)\n",
    "print(accuracy, precision, recall,f1_score)\n",
    "Log(\" accuracy, precision, recall, f1_score for traininga data\")\n",
    "Log(\" accuracy on train data = %g\" % accuracy)\n",
    "Log(\" precision on train data = %g\" % precision)\n",
    "Log(\" recall on train data = %g\" % recall)\n",
    "Log(\" f1_score on train data = %g\" % f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353647e2-e580-413b-8fb5-24b9f4af49b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cbe9a5-9134-44f3-ade8-86e20a0ff9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0915aeed-658c-4ba3-b9de-bea50a9ab5e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217af8d9-64ae-4af2-b08d-0694ae5d6134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b326c0-8546-4893-863f-0d564b9142ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e902a229-0fee-4fbd-b7eb-fb58fea6d5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8c1bd7-c9a5-4287-9d40-97e0c70194c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836c86c-7acd-473f-a886-c5ad1541b60a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b1742c1-4db3-4658-b02a-ffca2ef2b307",
   "metadata": {},
   "source": [
    "# adding category feature also in the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491ca9e3-5f32-4260-9155-df28aec039e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dd22c0-23fd-46d4-ae43-ab991ccbb1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c7863d-60cb-40fd-abef-1a458f2cf593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58784f09-756e-4f4d-94f8-e1e41590f3e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c8cf7-fdcc-417f-874b-dd5b5c4a37f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4f22ae-0481-4f84-9663-f84655845d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa05af7a-4ff9-4ffe-b81b-ebcb6d94ddbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6815deba-35fe-49e0-b3a9-61ca0b9aaad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f9eac5-3850-4086-a576-5ab6bbbdc8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b05b8-b337-40e8-98fa-467a77835f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "predictionAndLabels = result.select(['prediction', 'notiOpened']\\\n",
    "                                  ).withColumn('label',F.col('notiOpened').cast(DoubleType())).rdd\n",
    "\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "cm = metrics.confusionMatrix().toArray()\n",
    "\n",
    "accuracy=(cm[0][0]+cm[1][1])/cm.sum()\n",
    "precision=(cm[1][1])/(cm[0][1]+cm[1][1])\n",
    "recall=(cm[1][1])/(cm[1][0]+cm[1][1])\n",
    "\n",
    "print(accuracy, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deac59fc-54f0-4169-94ab-b1c500cedf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b3e82-38a3-4bd2-a90c-ce9f0154b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model_class.transform(training_data)\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c727fa3-78bd-4658-a98c-1dc7bdf0b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b45fc2-0b21-4c3c-88b1-81bdaca95804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "testing_data.filter(F.size(F.col('vec_embedding'))!=50).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf1941f-cd2e-4c4d-9247-f004bb4bc0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537fd879-8e06-4241-b18a-cf1e06ce3ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4395a374-f7f4-4b81-a398-13559ac7f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b9440-0e76-4543-b634-f135d6062d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dad280-78a7-473e-b20c-da62bc71d3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c728e67b-f856-4158-95d1-8b21a0d959c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a0505f8-ce53-4051-aeb1-9c20c96dbbd5",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c6c13-71ec-4f91-a1b9-e2bb0dfc63df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "training_start = time.time()\n",
    "\n",
    "data = combined_df_2.select(\"vec_embedding\", \"notiOpened\")#.sample(fraction = 0.1)\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "\n",
    "model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=3, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=4, maxBins=32)\n",
    "\n",
    "predictions = model.predict(testData.map(lambda x: x.vec_embedding))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.notiOpened).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification forest model:')\n",
    "print(model.toDebugString())\n",
    "\n",
    "# Save and load model\n",
    "#model.save(sc, \"target/tmp/myRandomForestClassificationModel\")\n",
    "#sameModel = RandomForestModel.load(sc, \"target/tmp/myRandomForestClassificationModel\")\n",
    "Log(\"Model fitting took %s minutes\"%(int((time.time() - training_start) / 60)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6550b09-dcf7-48da-a508-c1af9e577497",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = result.select(['notiOpened']).collect()\n",
    "y_pred = result.select(['prediction']).collect()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112d2da1-80a6-48ce-b1bf-b58cd66bd521",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44119247-5e23-4e77-b915-67afc1ebcbf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86afc871-1764-47cf-a576-f4e2cd47e1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce87907-0db2-424b-9cdd-da4d9cf97ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "predictionAndLabels = result.select(\"prediction\", \"notiOpened\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d85533-6a08-48a8-8591-32bf86da48ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "# Overall statistics\n",
    "precision = metrics.precision()\n",
    "recall = metrics.recall()\n",
    "f1Score = metrics.fMeasure()\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e679a843-c08d-4c9a-9400-b5590f6f5151",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_and_labels = result.select(['prediction','notiOpened']).withColumn('label', F.col('notiOpened').cast(DoubleType())).orderBy('prediction')\n",
    "preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "dbe1f9d0-c2e1-47d8-8ed3-a6347087014b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "precision() missing 1 required positional argument: 'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [210]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m precision \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m recall \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mrecall()\n\u001b[1;32m      3\u001b[0m f1Score \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mfMeasure()\n",
      "\u001b[0;31mTypeError\u001b[0m: precision() missing 1 required positional argument: 'label'"
     ]
    }
   ],
   "source": [
    "precision = metrics.precision()\n",
    "recall = metrics.recall()\n",
    "f1Score = metrics.fMeasure()\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "64701e8b-ac95-45b7-85cf-cea2011a1cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.1084e+04 1.8000e+01]\n",
      " [2.0385e+04 2.0000e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(metrics.confusionMatrix().toArray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "5f9414f4-deda-44e7-aad2-a8c032c07e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49157201765173986"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.0385e+04/(2.1084e+04+2.0385e+04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "5bf7f2f8-7b48-4f69-be55-e31b5938f646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9991470002843332"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.0385e+04/(2.0385e+04+2.0000e+00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86938ff1-e027-4eea-abf2-7ada3181a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c010d-22d8-450c-8cb9-9d0073ef454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.select(['notiOpened','prediction']).show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89252826-8fbb-48c7-b015-376651de3d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d4b6325a-c9a9-4e58-aaed-a14c969fa83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      1.00      0.67     21102\n",
      "           1       0.20      0.00      0.00     20387\n",
      "\n",
      "    accuracy                           0.51     41489\n",
      "   macro avg       0.35      0.50      0.34     41489\n",
      "weighted avg       0.36      0.51      0.34     41489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = result.select(['notiOpened']).collect()\n",
    "y_pred = result.select(['prediction']).collect()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07acac3-bca4-4dee-8655-7dd6a3b48e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select only prediction and label columns\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "preds_and_labels = result.select(['prediction','notiOpened'])\n",
    "\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "Log(metrics.confusionMatrix().toArray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e0f31b-7d20-43b7-a014-6dd610318f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "Log(metrics.confusionMatrix().toArray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5342eb9e-45fa-4d44-b2a4-48a78e857c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4845a5d-42f1-4106-b8b4-8d53bb26065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86d1f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import UpdateOne\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from Utils import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "mongoClient = getMongoClient()\n",
    "trainDB = mongoClient['trialDB_roberta']\n",
    "deviceVectorCollection = trainDB['deviceVectors']\n",
    "deviceClusterCollection = trainDB['notiDeviceClusters']\n",
    "\n",
    "newsVectorCollection = trainDB['newsVectors']\n",
    "newsTSpentCollection = trainDB['newsTSpent']\n",
    "newsScoreCollection = trainDB['newsScores']\n",
    "deviceNotiClustersCollection = trainDB['notiDeviceClusters']\n",
    "\n",
    "clusterCentersCollection = trainDB['clusterCenters']\n",
    " \n",
    "def getDeviceVectorsFromMongo(deviceIds):\n",
    "    \n",
    "    deviceVectorMap = {}\n",
    "    cursor = deviceVectorCollection.find({\"_id\" : { \"$in\" : deviceIds }})\n",
    "    \n",
    "    for c in cursor:\n",
    "        deviceVectorMap[c['_id']] = np.array(c['vector'])\n",
    "    \n",
    "    return deviceVectorMap\n",
    "        \n",
    "def getNewsVectorsFromMongo(hashIds):\n",
    "    newsVectorMap = {}\n",
    "    cursor = newsVectorCollection.find({\"_id\" : { \"$in\" : hashIds }})\n",
    "    \n",
    "    for c in cursor:\n",
    "        newsVectorMap[c['_id']] = np.array(c['vector'])\n",
    "    \n",
    "    return newsVectorMap\n",
    "\n",
    "\n",
    "def insertDeviceVectorsInMongo(deviceVectors):\n",
    "    \n",
    "    collection = deviceVectorCollection\n",
    "    ops = []\n",
    "    results = []\n",
    "    for deviceId in tqdm(deviceVectors):\n",
    "        key = {\"_id\" : deviceId}\n",
    "        devData = {\"$set\" : {\"vector\" : deviceVectors[deviceId]}}\n",
    "        ops.append(UpdateOne(key, devData, upsert=True))\n",
    "    \n",
    "        if len(ops) == 100000:\n",
    "            results.append(collection.bulk_write(ops,ordered=False))\n",
    "            ops = []\n",
    "\n",
    "    if len(ops) > 0:\n",
    "        results.append(collection.bulk_write(ops,ordered=False))\n",
    "\n",
    "def insertNewsVectorsInMongo(newsVectors):\n",
    "    for hashId in newsVectors:\n",
    "        key = {\"_id\" : hashId}\n",
    "        newsData = {\"$set\" : {\"vector\" : list(newsVectors[hashId])}}\n",
    "        newsVectorCollection.update_one(key, newsData, upsert=True)\n",
    "        \n",
    "        \n",
    "oldClusters = {}\n",
    "def getDeviceClustersFromMongo(deviceIds):\n",
    "    print(\"here: \", deviceClusterCollection)\n",
    "    deviceClusterMap = {}\n",
    "    if len(deviceIds) == 0:\n",
    "        cursor = deviceClusterCollection.find()\n",
    "    else:\n",
    "        cursor = deviceClusterCollection.find({\"_id\" : { \"$in\" : deviceIds }})\n",
    "    \n",
    "    for c in cursor:\n",
    "        deviceClusterMap[c['_id']] = c['cluster']\n",
    "        oldClusters[c['_id']] = c['cluster']\n",
    "    \n",
    "    return deviceClusterMap\n",
    "\n",
    "\"\"\"deprecated\"\"\"\n",
    "# def insertDeviceClustersInMongo(deviceClusters):\n",
    "#     collection = deviceClusterCollection\n",
    "#     ops = []\n",
    "#     results = []\n",
    "    \n",
    "#     for deviceId in deviceClusters:\n",
    "#         key = {\"_id\" : deviceId}\n",
    "        \n",
    "#         devData = {\"$set\" : {\"cluster\" : int(deviceClusters[deviceId])}}\n",
    "        \n",
    "#         if deviceId not in oldClusters or int(deviceClusters[deviceId]) != oldClusters[deviceId]:\n",
    "#             devData = {\"$set\" : {\"cluster\" : int(deviceClusters[deviceId]), \"updatedAt\" : datetime.datetime.now()}}\n",
    "            \n",
    "#         ops.append(UpdateOne(key, devData, upsert=True))\n",
    "    \n",
    "#         if len(ops) == 1000:\n",
    "#             results.append(collection.bulk_write(ops,ordered=False))\n",
    "#             ops = []\n",
    "\n",
    "#     if len(ops) > 0:\n",
    "#         results.append(collection.bulk_write(ops,ordered=False))\n",
    "        \n",
    "\n",
    "\n",
    "def insertNewsVectorsInMongo(newsVectors):\n",
    "    for hashId in newsVectors:\n",
    "        key = {\"_id\" : hashId}\n",
    "        newsData = {\"$set\" : {\"vector\" : list(newsVectors[hashId])}}\n",
    "        newsVectorCollection.update_one(key, newsData, upsert=True)\n",
    "        \n",
    "def getNewsTSpentFromMongo(hashIds):\n",
    "    newsTSpentMap = {}\n",
    "    cursor = newsTSpentCollection.find({\"_id\" : {\"$in\" : hashIds }})\n",
    "    \n",
    "    for c in cursor:\n",
    "        data = {}\n",
    "        for cluster in c['tSpent']:\n",
    "            data[int(cluster)] = c['tSpent'][cluster]\n",
    "        newsTSpentMap[c['_id']] = data\n",
    "        \n",
    "    return newsTSpentMap\n",
    "\n",
    "def insertNewsTSpentInMongo(newsTSpentMap):\n",
    "    for hashId in newsTSpentMap:\n",
    "        key = {\"_id\" : hashId}\n",
    "        data = {}\n",
    "        for cluster in newsTSpentMap[hashId]:\n",
    "            data[str(cluster)] = newsTSpentMap[hashId][cluster]\n",
    "        tSpentData = {\"$set\" : {\"tSpent\" : data}}\n",
    "        newsTSpentCollection.update_one(key, tSpentData, upsert=True)\n",
    "\n",
    "def getNewsScoreInMongo(hashIds):\n",
    "    newsScoreMap = {}\n",
    "    cursor = newsScoreCollection.find({\"_id\" : {\"$in\" : hashIds }})\n",
    "    \n",
    "    for c in cursor:\n",
    "        data = {}\n",
    "        for cluster in c['tSpent']:\n",
    "            data[int(cluster)] = c['tSpent'][cluster]\n",
    "        newsScoreMap[c['_id']] = data\n",
    "        \n",
    "    return newsScoreMap\n",
    "\n",
    "def insertNewsScoreInMongo(newsTSpentMap):\n",
    "    for hashId in newsTSpentMap:\n",
    "        key = {\"_id\" : hashId}\n",
    "        data = {}\n",
    "        for cluster in newsTSpentMap[hashId]:\n",
    "            if newsTSpentMap[hashId][cluster][1] > 1:\n",
    "                data[str(cluster)] = newsTSpentMap[hashId][cluster][0] / newsTSpentMap[hashId][cluster][1]  \n",
    "        \n",
    "        scoreData = {\"$set\" : {\"tSpent\" : data, \"updatedAt\" : datetime.datetime.now()}}\n",
    "        newsScoreCollection.update_one(key, scoreData, upsert=True)\n",
    "\n",
    "def insertDeviceClustersInMongo(deviceClusters, oldClusters, Log):\n",
    "    ops = []\n",
    "    #results = []\n",
    "    \n",
    "    cntUpdated = 0\n",
    "    for deviceId in deviceClusters:\n",
    "        key = {\"_id\" : deviceId}\n",
    "        \n",
    "        devData = {\"$set\" : {\"cluster\" : int(deviceClusters[deviceId]), \"updatedAt\" : datetime.datetime.now()}}\n",
    "        \n",
    "        if deviceId in oldClusters and deviceClusters[deviceId] == oldClusters[deviceId]:\n",
    "            continue\n",
    "        \n",
    "        cntUpdated += 1\n",
    "        ops.append(UpdateOne(key, devData, upsert=True))\n",
    "    \n",
    "        if len(ops) == 100000:\n",
    "            try:\n",
    "                result = deviceNotiClustersCollection.bulk_write(ops,ordered=False)\n",
    "                ops = []\n",
    "            except Exception as e:\n",
    "                ops = []\n",
    "                Log(\"Failed to insert devices clusters, \" + str(e))\n",
    "        \n",
    "        if cntUpdated % 100000 == 0:\n",
    "            Log(\"Inserted \" + str(cntUpdated) + \" Device Clusters\")\n",
    "                \n",
    "    if len(ops) > 0:\n",
    "        deviceNotiClustersCollection.bulk_write(ops,ordered=False)\n",
    "    \n",
    "    print(cntUpdated,len(deviceClusters))\n",
    "    Log(\"Updated  device clusters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627d52bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from ColabUtils import *\n",
    "device_vect_df = df.toPandas()\n",
    "device_vect_df\n",
    "\n",
    "deviceVectors = {}\n",
    "for v in tqdm(device_vect_df.values):\n",
    "    deviceVectors[v[0]] = v[1]\n",
    "insertDeviceVectorsInMongo(deviceVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbef02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dfdump.select('deviceId','cluster').toPandas()\n",
    "labels = pd.Series(labels.cluster.values,index=labels.deviceId).to_dict()\n",
    "d4 = labels.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93048337",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "newClusterWiseDevices = defaultdict(lambda: set())\n",
    "for deviceId, cluster in tqdm(labels.items()):\n",
    "    newClusterWiseDevices[cluster].add(deviceId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995407ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ColabUtils import *\n",
    "deviceClusterMapExisiting = getDeviceClustersFromMongo([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a37e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterWiseDevices = defaultdict(lambda: set())\n",
    "for d in tqdm(deviceClusterMapExisiting):\n",
    "    clusterWiseDevices[deviceClusterMapExisiting[d]].add(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e08e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "marked = {}\n",
    "clusterMap = {}\n",
    "for x in newClusterWiseDevices:\n",
    "    mxinter = -1\n",
    "    mxidx = -1\n",
    "    for y in clusterWiseDevices:\n",
    "        if y in marked:\n",
    "            continue\n",
    "        inter = (len(clusterWiseDevices[y].intersection(newClusterWiseDevices[x])) / len(clusterWiseDevices[y])*100)\n",
    "        if inter > mxinter:\n",
    "            mxinter = inter\n",
    "            mxidx = y\n",
    "            \n",
    "    if mxidx < 0:\n",
    "        mxidx = x\n",
    "    \n",
    "    marked[mxidx] = 1\n",
    "    clusterMap[x] = mxidx\n",
    "    Log(\"Mapping cluster \" + str(x) + \" to \" + str(mxidx) + \", %age intersection = \" + str(mxinter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3dccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "updatedDeviceClusters = {}\n",
    "for cluster in newClusterWiseDevices:\n",
    "    for deviceId in tqdm(newClusterWiseDevices[cluster]):\n",
    "        updatedDeviceClusters[deviceId] = clusterMap[cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82be67b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_start = time.time()\n",
    "# from ColabUtils import *\n",
    "insertDeviceClustersInMongo(updatedDeviceClusters, deviceClusterMapExisiting, Log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b2c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_time = (time.time() - insert_start)\n",
    "\n",
    "Log(\"Insertion took %s percent of time\" % (100 * insert_time / (time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db661843",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataWithClusters = data_filtered.select('hashId', 'overallTimeSpent', F.udf(lambda deviceId: updatedDeviceClusters[deviceId], IntegerType())('deviceId').alias('cluster'))\n",
    "newsClusterMean = dataWithClusters.groupby('hashId', 'cluster').agg(F.sum('overallTimeSpent'),F.count('hashId'))\n",
    "newsClusterMeanDf = newsClusterMean.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a73453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsTSpentMap = defaultdict(lambda: defaultdict(lambda: (7, 1)))\n",
    "for v in tqdm(newsClusterMeanDf.values):\n",
    "    newsTSpentMap[v[0]][v[1]] = (v[2], v[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e26278",
   "metadata": {},
   "outputs": [],
   "source": [
    "Log(\"Inserting News Score Data\")\n",
    "insert_start = time.time()\n",
    "insertNewsTSpentInMongo(newsTSpentMap)\n",
    "insertNewsScoreInMongo(newsTSpentMap)\n",
    "insert_time = (time.time() - insert_start)\n",
    "Log(\"Inserting News Score took %s percent of time\" % str(100 * insert_time / (time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724ce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_path_notification(date, hours=None):\n",
    "    paths = []\n",
    "    base_path = NIS_RAW_DATA_BASE_PATH + date\n",
    "    if not hours:\n",
    "        return base_path + \"/*/*.gz\"\n",
    "    for hour in hours:\n",
    "        paths.append(base_path + \"/\" + str(hour).zfill(2) + \"/*.gz\")\n",
    "    return \",\".join(paths)\n",
    "\n",
    "def process_raw_data_notification(paths):\n",
    "    def view_data_filters(x):\n",
    "        x = x['properties']\n",
    "        deviceid_filter = ('device_id' in x) and (x['device_id'] != '')\n",
    "        noti_filter = ('notiOpened' in x) and (int(x['timeSpent']) <= 100)\n",
    "        return deviceid_filter and time_filter\n",
    "\n",
    "    try:\n",
    "        rdd = sc.textFile(paths)             .map(json.loads)   .filter(lambda x: (\"event_name\" in x) and ( (x[\"event_name\"].lower() == \"notification opened\") or (x[\"event_name\"].lower() == \"notification shown\") ))            \n",
    "        view_data = rdd.map(lambda x: (x['device_id'], x['hashId'][:-2], x['short_time'], x['noti_opened'], x['noti_shown'], x['event_name']))             .toDF(['deviceId', 'hashId', 'timeSpent', 'notiOpened', 'notiShown', 'event_name'])\n",
    "        \n",
    "        view_data = view_data.withColumn(\"notiOpened\",F.when(view_data.event_name == 'Notification Opened', 1).otherwise(0))\n",
    "        view_data = view_data.withColumn(\"notiShown\",F.when(view_data.event_name == 'Notification Shown', 1).otherwise(0))\n",
    "        view_data = view_data.select('deviceId', 'hashId', 'timeSpent', 'notiOpened', 'notiShown')  .groupby(view_data.deviceId, view_data.hashId)                              .agg(F.max(view_data.timeSpent).alias('timeSpent'),F.sum(view_data.notiOpened).alias('notiOpened'),F.sum(view_data.notiShown).alias('notiShown'))\n",
    "        view_data = view_data.withColumn(\"notiShown\",F.when(view_data.notiShown < view_data.notiOpened, \n",
    "                                                     view_data.notiOpened).otherwise(view_data.notiShown))\n",
    "        return view_data\n",
    "    except Exception as e:\n",
    "        logger.warning(\"Error processing data: \" + str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e2818",
   "metadata": {},
   "outputs": [],
   "source": [
    "NIS_RAW_DATA_BASE_PATH = \"gs://inshorts-minimal-event/data/inshorts-minimal-event-v1/\"\n",
    "\n",
    "path = get_raw_path_notification(datetime.datetime.now().strftime(\"%Y/%m/%d\"))\n",
    "today_data = process_raw_data_notification(path)\n",
    "today_data = today_data.filter(today_data.hashId.isin(hashIdsWithFilter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbddc25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 7\n",
    "st_date = datetime.datetime.now() - datetime.timedelta(days=n_days+1)\n",
    "\n",
    "date_fmt = \"%Y/%m/%d\"\n",
    "\n",
    "dates = [(st_date + datetime.timedelta(days=i)) for i in range(n_days+1)]\n",
    "dates.sort()\n",
    "\n",
    "dates_str = [date.strftime(date_fmt) for date in dates]\n",
    "\n",
    "# millisMin = dates[0].timestamp() * 1000\n",
    "# millisMax = (dates[-1] + datetime.timedelta(days=1)).timestamp() * 1000\n",
    "\n",
    "hashIdsWithFilter, newsMap = getNewsData(dates[0], datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "datestr = [d.strftime(date_fmt) for d in dates]\n",
    "paths = get_path(datestr, 'otherEvents')\n",
    "\n",
    "data = sqlContext.read.parquet(*paths)\n",
    "\n",
    "data = filter_app(data, app_name=None)\n",
    "data = filter_tenant(data, tenant='en')\n",
    "\n",
    "# data = data.filter((data.eventTimestamp > millisMin) & (data.eventTimestamp < millisMax))\n",
    "\n",
    "data = data.select(data.deviceId, (F.split(data.hashId, '-')[0]).alias('hashId'), data.overallTimeSpent, data.notificationOpened, data.notificationShown, data.notificationType,data.eventName)\n",
    "data = data.filter( data.eventName.isin(['Notification Shown','Notification Opened']) )\n",
    "\n",
    "data = data.filter(data.hashId.isin(hashIdsWithFilter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e80aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.select('deviceId', 'hashId' ,'overallTimeSpent', 'notificationOpened', 'notificationShown')\\\n",
    "            .groupby('deviceId', 'hashId')\\\n",
    "            .agg(F.max('overallTimeSpent').alias('timeSpent'), \n",
    "                 F.sum('notificationShown').alias('notiShown'),F.sum('notificationOpened').alias('notiOpened'))\n",
    "data = data.withColumn(\"notiShown\",F.when(data.notiShown < data.notiOpened, \n",
    "                                                     data.notiOpened).otherwise(data.notiShown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f217ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.union(today_data)\n",
    "data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05bd9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_temp1=data_filtered.select(['deviceid']).distinct()\n",
    "df_temp1=df_temp.withColumnRenamed(\"deviceid\",\"deviceId\")\n",
    "\n",
    "data=data.join(df_temp1,[\"deviceId\"],\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2670c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataWithClusters = data.select('hashId', 'notiOpened','notiShown', F.udf(lambda deviceId: updatedDeviceClusters[deviceId], IntegerType())('deviceId').alias('cluster'))\n",
    "newsClusterNotiMean = dataWithClusters.groupby('hashId', 'cluster').agg(F.sum('notiOpened'),F.sum('notiShown'),F.count('hashId'))\n",
    "# newsClusterNotiMeanDf = newsClusterNotiMean.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55399a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsClusterNotiMean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb7aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsNotiSpentCollection = trainDB['newsNotiSpent']\n",
    "newsNotiScoreCollection = trainDB['newsNotiScores']\n",
    "\n",
    "\n",
    "def getNewsNotiSpentFromMongo(hashIds):\n",
    "    newsNotiSpentMap = {}\n",
    "    cursor = newsNotiSpentCollection.find({\"_id\" : {\"$in\" : hashIds }})\n",
    "    \n",
    "    for c in cursor:\n",
    "        data = {}\n",
    "        for cluster in c['nOpened']:\n",
    "            data[int(cluster)] = c['nOpened'][cluster]\n",
    "        newsNotiSpentMap[c['_id']] = data\n",
    "        \n",
    "    return newsNotiSpentMap\n",
    "\n",
    "def insertNewsNotiSpentInMongo(newsNotiSpentMap):\n",
    "    for hashId in newsNotiSpentMap:\n",
    "        key = {\"_id\" : hashId}\n",
    "        data = {}\n",
    "        for cluster in newsNotiSpentMap[hashId]:\n",
    "            data[str(cluster)] = newsNotiSpentMap[hashId][cluster]\n",
    "        NotiSpentData = {\"$set\" : {\"nOpened\" : data}}\n",
    "        newsNotiSpentCollection.update_one(key, NotiSpentData, upsert=True)\n",
    "\n",
    "def getNewsNotiScoreInMongo(hashIds):\n",
    "    newsNotiScoreMap = {}\n",
    "    cursor = newsNotiScoreCollection.find({\"_id\" : {\"$in\" : hashIds }})\n",
    "    \n",
    "    for c in cursor:\n",
    "        data = {}\n",
    "        for cluster in c['nOpened']:\n",
    "            data[int(cluster)] = c['nOpened'][cluster]\n",
    "        newsNotiScoreMap[c['_id']] = data\n",
    "        \n",
    "    return newsNotiScoreMap\n",
    "\n",
    "def insertNewsNotiScoreInMongo(newsNotiSpentMap):\n",
    "    for hashId in newsNotiSpentMap:\n",
    "        key = {\"_id\" : hashId}\n",
    "        data = {}\n",
    "        for cluster in newsNotiSpentMap[hashId]:\n",
    "            if newsNotiSpentMap[hashId][cluster][1] > 1:\n",
    "                data[str(cluster)] = newsNotiSpentMap[hashId][cluster][0] / newsNotiSpentMap[hashId][cluster][1]  \n",
    "        \n",
    "        notiScoreData = {\"$set\" : {\"nOpened\" : data, \"updatedAt\" : datetime.datetime.now()}}\n",
    "        newsNotiScoreCollection.update_one(key, notiScoreData, upsert=True)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a3cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Log(\"Exectution finished successfully, in : %s minutes, %s seconds \" % (int((time.time() - start_time) / 60), int((time.time() - start_time) % 60)), flag=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
