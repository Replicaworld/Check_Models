{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 856 Âµs (started: 2023-06-19 11:29:26 +00:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 150:>  (0 + 1) / 1][Stage 152:>  (0 + 1) / 1][Stage 153:>  (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, RobertaTokenizer , RobertaForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "#agar train karna ho toh\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, RobertaTokenizer , RobertaForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "\n",
    "# Read data\n",
    "data = df_news.copy()\n",
    "# data = data.sample(frac = 0.1)\n",
    "# Define pretrained tokenizer and model\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=k, output_hidden_states=True)\n",
    "\n",
    "# ----- 1. Preprocess data -----#\n",
    "# Preprocess data\n",
    "X = list(data[\"content\"])\n",
    "y = list(data[\"label\"])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Create torch dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "val_dataset = Dataset(X_val_tokenized, y_val)\n",
    "\n",
    "# ----- 2. Fine-tune pretrained model -----#\n",
    "# Define Trainer parameters\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred, average='weighted')\n",
    "    precision = precision_score(y_true=labels, y_pred=pred, average='weighted')\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred, average = 'weighted')\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# Define Trainer\n",
    "class SaveModelCallback(EarlyStoppingCallback):\n",
    "    def __init__(self, output_dir):\n",
    "        super().__init__(early_stopping_patience=3)\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        super().on_epoch_end(args, state, control, **kwargs)\n",
    "        epoch = state.epoch\n",
    "        model = control.model  # Access the model directly from the Trainer object\n",
    "        model.save_pretrained(os.path.join(self.output_dir, f\"checkpoint-{epoch}\"))\n",
    "# ... Rest of your code ...\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    seed=0,\n",
    "    load_best_model_at_end = True\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "# Train pre-trained model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>              (4 + 112) / 200][Stage 7:>                (0 + 4) / 200]\r"
     ]
    }
   ],
   "source": [
    "model_path = \"output/checkpoint-12000\"\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_path, num_labels=10,output_hidden_states=True)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model, tokenizer, text):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        input_ids = tokenizer.encode(text, add_special_tokens=True, return_tensors='pt').to(device)\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        hidden_states = outputs.hidden_states\n",
    "        last_layer_hidden_states = hidden_states[-1]\n",
    "        embeddings = torch.mean(last_layer_hidden_states, dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "# Compute document embeddings for all articles in the dataset\n",
    "document_embeddings = {}\n",
    "\n",
    "df = df_news.copy()\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    newsid = row['hid']\n",
    "    text = row['content']\n",
    "\n",
    "    document_embeddings[newsid] = get_embedding(model, tokenizer, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "0\n",
      "time: 17.7 ms (started: 2023-06-19 11:29:44 +00:00)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, udf, when\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import DataFrameStatFunctions as stat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, udf, when\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "\n",
    "try:\n",
    "    %load_ext autotime\n",
    "except:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime\n",
    "print(0)\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "from pymongo import MongoClient\n",
    "from tqdm import tqdm\n",
    "from pyspark.sql import DataFrameStatFunctions as stat\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "\n",
    "NIS_DATA_BASE_PATH = \"gs://nis-segment-datasource-v3/processed/\"\n",
    "NIS_OLD_DATA_BASE_PATH = \"gs://nis-localytics-datasource/processed/\"\n",
    "\n",
    "#conf = SparkConf().setAll([('spark.driver.memory', '20g'), ('spark.broadcast.blockSize', '10m'), (\"spark.executor.instances\", '30')])\n",
    "#sc = SparkContext(conf=conf)\n",
    "#sqlContext = SQLContext(sc)\n",
    "\n",
    "conf = SparkConf().setAll([('spark.sql.broadcastTimeout',1000)\n",
    "    # ('spark.driver.memory', '20g'), \\\n",
    "#                            ('spark.broadcast.blockSize', '10m'),\\\n",
    "#                            ('spark.dynamicAllocation.enabled','False'),\\\n",
    "#                            ('spark.executor.instances','9999')\n",
    "# #                            ,(\"spark.sql.autoBroadcastJoinThreshold\",-1)\n",
    "                          ])\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "#sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "#sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f1406caeac0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.46 ms (started: 2023-06-19 11:29:52 +00:00)\n"
     ]
    }
   ],
   "source": [
    "conf.set(\"spark.driver.maxResultSize\", \"100g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.57 ms (started: 2023-06-19 11:29:55 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def get_path(dates, prefix, padding=None):\n",
    "    dates_ = dates + []\n",
    "    if padding:\n",
    "        st_date, ed_date = sorted(dates)[0], sorted(dates)[-1]\n",
    "        for i in range(1, 4):\n",
    "            d = (datetime.datetime.strptime(ed_date, date_fmt) + datetime.timedelta(days=i)).strftime(date_fmt)\n",
    "            if d < datetime.datetime.today().strftime(date_fmt):\n",
    "                dates_.append(d)\n",
    "    dates_ = list(set(dates_) - set([\"2019/02/17\", \"2019/02/18\", \"2019/05/28\", \"2019/06/03\", \"2019/07/02\", \"2019/07/03\", \"2019/07/04\", \"2019/11/13\", \"2019/11/14\", \"2020/02/22\", \"2020/03/31\", \"2020/04/16\", \"2020/04/18\", \"2020/05/11\", \"2021/05/13\"]))\n",
    "    paths = []\n",
    "    for date in dates_:\n",
    "        base_path = NIS_DATA_BASE_PATH\n",
    "        if date < \"2018/06/26\":\n",
    "            base_path = NIS_OLD_DATA_BASE_PATH\n",
    "        paths.append(base_path + date + \"/\" + prefix + \"/*.parquet\")\n",
    "    return paths\n",
    "\n",
    "date_fmt = \"%Y/%m/%d\"\n",
    "month_fmt = \"%Y/%m\"\n",
    "\n",
    "def millis2date(x):\n",
    "    try:\n",
    "        if x < 15000000000:\n",
    "            return datetime.datetime.fromtimestamp(x).strftime(date_fmt)\n",
    "        else:\n",
    "            return datetime.datetime.fromtimestamp(x / 1000.).strftime(date_fmt)\n",
    "    except:\n",
    "        return \"1970/01/01\"\n",
    "\n",
    "def millis2month(x):\n",
    "    try:\n",
    "        if x < 15000000000:\n",
    "            return datetime.datetime.fromtimestamp(x).strftime(month_fmt)\n",
    "        else:\n",
    "            return datetime.datetime.fromtimestamp(x / 1000.).strftime(month_fmt)\n",
    "    except:\n",
    "        return \"1970/01\"\n",
    "\n",
    "millis2date_udf = F.udf(millis2date, StringType())\n",
    "millis2month_udf = F.udf(millis2month, StringType())\n",
    "\n",
    "def divide_maps(d1, d2):\n",
    "    keys = set(d1.keys()).intersection(set(d2.keys()))\n",
    "    res = {}\n",
    "    for k in keys:\n",
    "        res[k] = d1[k] * 1. / (d2[k] + 1e-10)\n",
    "    return res\n",
    "\n",
    "def timediff(y, x, date_fmt=\"%Y/%m/%d\"): \n",
    "    end = datetime.datetime.strptime(y, date_fmt)\n",
    "    start = datetime.datetime.strptime(x, date_fmt)\n",
    "    delta = (end - start).days\n",
    "    return delta\n",
    "\n",
    "def monthdiff(y, x, month_fmt=\"%Y/%m\"): \n",
    "    millis = y - x\n",
    "    delta = millis / (1000 * 3600 * 24 * 30)\n",
    "    return delta\n",
    "\n",
    "timediff_udf = udf(timediff, IntegerType())\n",
    "monthdiff_udf = udf(monthdiff, IntegerType())\n",
    "\n",
    "def filter_platform(data, platform=None):\n",
    "    if platform == \"ANDROID\":\n",
    "        data = data.filter(data.platform == \"ANDROID\")\n",
    "    elif platform == \"IOS\":\n",
    "        data = data.filter(data.platform != \"ANDROID\")\n",
    "    return data\n",
    "\n",
    "def filter_category(data, categories=None):\n",
    "    if categories:\n",
    "        data = data.filter(data.categoryWhenEventHappened.isin(categories))\n",
    "    return data\n",
    "\n",
    "def filter_tenant(data, tenant=None):\n",
    "    if tenant in ['hi', 'HINDI']:\n",
    "        data = data.filter(data.tenant.isin(['hi', 'HINDI', 'Hindi', 'hindi']))\n",
    "    elif tenant in ['en', 'ENGLISH']:\n",
    "        data = data.filter(~data.tenant.isin(['hi', 'HINDI', 'Hindi', 'hindi']))\n",
    "    return data\n",
    "\n",
    "\n",
    "def filter_app(data, app_name=None):\n",
    "    if 'appName' in data.columns:\n",
    "        if app_name:\n",
    "            data = data.filter(data.appName == app_name)\n",
    "        else:\n",
    "            data = data.filter((data.appName != \"mini\") & (data.appName != \"crux\"))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.85 ms (started: 2023-06-19 11:29:55 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def get_raw_path(date, hours=None):\n",
    "    paths = []\n",
    "    base_path = NIS_RAW_DATA_BASE_PATH + date\n",
    "    if not hours:\n",
    "        return base_path + \"/*/*.gz\"\n",
    "    for hour in hours:\n",
    "        paths.append(base_path + \"/\" + str(hour).zfill(2) + \"/*.gz\")\n",
    "    return \",\".join(paths)\n",
    "\n",
    "def process_raw_data(paths):\n",
    "    def view_data_filters(x):\n",
    "        x = x['properties']\n",
    "        deviceid_filter = ('deviceId' in x) and (x['deviceId'] != '')\n",
    "        time_filter = ('timeSpent' in x) and (int(x['timeSpent']) <= 100) and (int(x['timeSpent']) >= 0)\n",
    "        return deviceid_filter and time_filter\n",
    "\n",
    "    try:\n",
    "        rdd = sc.textFile(paths)             .map(json.loads)             .filter(lambda x: \"batch\" in x).flatMap(lambda x: x[\"batch\"])             .filter(lambda x: (\"event\" in x) and (x[\"event\"].lower() == \"timespent-front\"))             .filter(view_data_filters)             .map(lambda x: x['properties'])\n",
    "        view_data = rdd.map(lambda x: (x['deviceId'], x['hashId'][:-2], x['timeSpent'],x['Notification Shown'],x['Notification Open'])).toDF(['deviceId', 'hashId', 'timeSpent','Notification Shown','Notification Open'])\n",
    "        \n",
    "        view_data = view_data.filter(view_data.timeSpent.isNotNull())\n",
    "        #view_data = view_data.filter((getHashBucketUDF(view_data.deviceId) >= 16) & (getHashBucketUDF(view_data.deviceId) <= 25))\n",
    "        view_data2 = view_data.groupby(view_data.deviceId, view_data.hashId).agg(F.max(view_data.timeSpent).alias('overallTimeSpent'))\n",
    "        #view_data=view_data.groupby(['deviceId','hashId','Notification Shown','Notification Open']).join(view_data2,[\"deviceId\",\"hashId\"],\"inner\")\n",
    "\n",
    "        \n",
    "        return view_data2\n",
    "    except Exception as e:\n",
    "        logger.warning(\"Error processing data: \" + str(e))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.17 ms (started: 2023-06-19 11:29:56 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from pymongo import UpdateOne\n",
    "\n",
    "def getMongoClient():\n",
    "    hosts = ['171.16.11.97','171.16.11.96','171.16.11.94']\n",
    "    host = ','.join([i + \":27017\" for i in hosts])\n",
    "    conn_url = 'mongodb://root:superman@' + host\n",
    "    client = MongoClient(conn_url)\n",
    "    return client\n",
    "\n",
    "def getMongoColl(db_name, coll_name, hosts=[]):\n",
    "    host = ','.join([i + \":27017\" for i in hosts])\n",
    "    conn_url = 'mongodb://root:superman@' + host + '/' + db_name\n",
    "    client = MongoClient(conn_url, minPoolSize=10, maxPoolSize=None)\n",
    "    return client[db_name][coll_name]\n",
    "\n",
    "def getNewsInHashIds(hashIds):\n",
    "    news_coll = getMongoColl(db_name='nis-news', \n",
    "                           coll_name='News', \n",
    "                           hosts=['172.16.11.196', '172.16.11.195', '172.16.11.194'])\n",
    "    \n",
    "    newsMap = {}\n",
    "    array = [t for t in news_coll.find({'_id' : {\"$in\" : hashIds}})]\n",
    "    \n",
    "    for i in range(len(array)):\n",
    "        newsMap[array[i]['_id']] = array[i]\n",
    "        \n",
    "    return newsMap\n",
    "\n",
    "def getNewsInDates(begin, end):\n",
    "    news_coll = getMongoColl(db_name='nis-news', \n",
    "                           coll_name='News', \n",
    "                           hosts=['172.16.11.196', '172.16.11.195', '172.16.11.194'])\n",
    "    \n",
    "    newsMap = {}\n",
    "    array = [t for t in news_coll.find({'createdAt' : {\"$gt\" : begin, \"$lt\" : end}})]\n",
    "    \n",
    "    for i in range(len(array)):\n",
    "        newsMap[array[i]['_id']] = array[i]\n",
    "        \n",
    "    return newsMap\n",
    "def getNewsData(d1, d2):\n",
    "    newsMap = getNewsInDates(d1, d2)\n",
    "    hashIdList = list(newsMap.keys())\n",
    "\n",
    "    hashIdsWithFilter = []\n",
    "    for h in hashIdList:\n",
    "        if 'newsLanguage' in newsMap[h] and newsMap[h]['newsLanguage'] == 'english' and newsMap[h]['publishGroupList'][0]['countryCode'] == 'IN':\n",
    "            hashIdsWithFilter.append(h.split('-')[0])\n",
    "    \n",
    "    return hashIdsWithFilter, newsMap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping data of 3 days for training\n",
    "n_days = 40\n",
    "#st_date = datetime.datetime(2023, 2, 18)\n",
    "#st_date = datetime.datetime(2023, 5, 18) - datetime.timedelta(days=n_days+1)\n",
    "\n",
    "st_date = datetime.datetime.now() - datetime.timedelta(days=n_days+1)\n",
    "\n",
    "date_fmt = \"%Y/%m/%d\"\n",
    "\n",
    "dates = [(st_date + datetime.timedelta(days=i)) for i in range(n_days+1)]\n",
    "dates.sort()\n",
    "#print(dates)\n",
    "dates_str = [date.strftime(date_fmt) for date in dates]\n",
    "\n",
    "# millisMin = dates[0].timestamp() * 1000\n",
    "# millisMax = (dates[-1] + datetime.timedelta(days=1)).timestamp() * 1000\n",
    "\n",
    "hashIdsWithFilter, newsMap = getNewsData(dates[0], datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking 14Feb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'home/Gourav/Classification_model'\n",
      "/\n",
      "time: 1.47 ms (started: 2023-06-19 11:30:03 +00:00)\n"
     ]
    }
   ],
   "source": [
    "cd home/Gourav/Classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2023/01/04', '2023/01/05', '2023/01/06', '2023/01/07', '2023/01/08', '2023/01/09', '2023/01/10', '2023/01/11', '2023/01/12', '2023/01/13', '2023/01/14', '2023/01/15', '2023/01/16', '2023/01/17', '2023/01/18', '2023/01/19', '2023/01/20', '2023/01/21', '2023/01/22', '2023/01/23', '2023/01/24', '2023/01/25', '2023/01/26', '2023/01/27', '2023/01/28', '2023/01/29', '2023/01/30', '2023/01/31', '2023/02/01', '2023/02/02', '2023/02/03', '2023/02/04', '2023/02/05', '2023/02/06', '2023/02/07', '2023/02/08', '2023/02/09', '2023/02/10', '2023/02/11', '2023/02/12', '2023/02/13']\n",
      "time: 3.07 s (started: 2023-06-19 11:30:08 +00:00)\n"
     ]
    }
   ],
   "source": [
    "n_days = 40\n",
    "st_date = sampled_dates[3] - datetime.timedelta(days=n_days+1)\n",
    "\n",
    "date_fmt = \"%Y/%m/%d\"\n",
    "\n",
    "dates = [(st_date + datetime.timedelta(days=i)) for i in range(n_days+1)]\n",
    "dates.sort()\n",
    "\n",
    "dates_str = [date.strftime(date_fmt) for date in dates]\n",
    "print(dates_str)\n",
    "# millisMin = dates[0].timestamp() * 1000\n",
    "# millisMax = (dates[-1] + datetime.timedelta(days=1)).timestamp() * 1000\n",
    "\n",
    "hashIdsWithFilter, newsMap = getNewsData(dates[0], sampled_dates[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 150:>  (0 + 1) / 1][Stage 152:>  (0 + 1) / 1][Stage 153:>  (0 + 1) / 1]  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 45 s (started: 2023-06-19 11:30:14 +00:00)\n"
     ]
    }
   ],
   "source": [
    "datestr = [d.strftime(date_fmt) for d in dates]\n",
    "paths = get_path(datestr, 'timeSpentFrontEvents')\n",
    "\n",
    "data = sqlContext.read.parquet(*paths)\n",
    "\n",
    "data = filter_app(data, app_name=None)\n",
    "data = filter_tenant(data, tenant='en')\n",
    "\n",
    "# data = data.filter((data.eventTimestamp > millisMin) & (data.eventTimestamp < millisMax))\n",
    "data = data.select(data.deviceId, data.overallTimeSpent, (F.split(data.hashId, '-')[0]).alias('hashId'),data.eventTimestamp)        .groupby('deviceId', 'hashId','eventTimestamp')         .agg(F.max('overallTimeSpent').alias('overallTimeSpent'))\n",
    "\n",
    "data = data.filter(data.hashId.isin(hashIdsWithFilter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[deviceId: string, hashId: string, eventTimestamp: bigint, overallTimeSpent: double]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.29 ms (started: 2023-06-19 11:30:59 +00:00)\n"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 687 Âµs (started: 2023-06-03 11:23:41 +00:00)\n"
     ]
    }
   ],
   "source": [
    "sampled_dates = [datetime.datetime(2023,2,1,0,0,0), datetime.datetime(2023,2,6,0,0,0),datetime.datetime(2023,2,9,0,0,0),datetime.datetime(2023,2,14,0,0,0),\n",
    "                datetime.datetime(2023,2,18,0,0,0),datetime.datetime(2023,2,20,0,0,0), datetime.datetime(2023,2,25,0,0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 30.3 ms (started: 2023-06-19 11:30:59 +00:00)\n"
     ]
    }
   ],
   "source": [
    "data=data.filter(data.overallTimeSpent<100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from pyspark.sql.functions import lit, udf, when,col\n",
    "def toTokenCollection(eventTimestamp):\n",
    "    dt_obj = dt.fromtimestamp(eventTimestamp)\n",
    "\n",
    "    \n",
    "    return dt_obj\n",
    "#timestamp = 1545730073\n",
    "from pyspark.sql.types import IntegerType\n",
    "newsToTokensUdf = F.udf(toTokenCollection,DateType())\n",
    "\n",
    "data = data.withColumn(\"eventTimestamp\", data[\"eventTimestamp\"].cast(IntegerType()))\n",
    "data = data.withColumn(\"eventTimestamp\", data[\"eventTimestamp\"]/1000)\n",
    "\n",
    "#data=data.withColumn('Event_dt',F.udf(toTokenCollection))\n",
    "data = data.select('deviceId','hashId','overallTimeSpent', newsToTokensUdf('eventTimestamp').alias('Event_dt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select('Event_dt').distinct().show(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generating embedding for 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 30\n",
    "st_date = sampled_dates[3] - datetime.timedelta(days=40+1)\n",
    "\n",
    "date_fmt = \"%Y/%m/%d\"\n",
    "\n",
    "dates = [(st_date + datetime.timedelta(days=i)) for i in range(n_days+1)]\n",
    "dates.sort()\n",
    "\n",
    "dates_str = [date.strftime(date_fmt) for date in dates]\n",
    "print(dates_str)\n",
    "# millisMin = dates[0].timestamp() * 1000\n",
    "# millisMax = (dates[-1] + datetime.timedelta(days=1)).timestamp() * 1000\n",
    "\n",
    "hashIdsWithFilter, newsMap = getNewsData(dates[0], sampled_dates[3])\n",
    "\n",
    "datestr = [d.strftime(date_fmt) for d in dates]\n",
    "paths = get_path(datestr, 'timeSpentFrontEvents')\n",
    "\n",
    "data = sqlContext.read.parquet(*paths)\n",
    "\n",
    "data = filter_app(data, app_name=None)\n",
    "data = filter_tenant(data, tenant='en')\n",
    "\n",
    "# data = data.filter((data.eventTimestamp > millisMin) & (data.eventTimestamp < millisMax))\n",
    "data = data.select(data.deviceId, data.overallTimeSpent, (F.split(data.hashId, '-')[0]).alias('hashId'))\\\n",
    "        .groupby('deviceId', 'hashId') \\\n",
    "        .agg(F.max('overallTimeSpent').alias('overallTimeSpent'))\n",
    "\n",
    "data = data.filter(data.hashId.isin(hashIdsWithFilter))\n",
    "data=data.filter(data.overallTimeSpent<100)\n",
    "\n",
    "df_users=data.groupBy(\"deviceId\").count()\n",
    "df_users=df_users.withColumnRenamed(\"count\",\"count_ad\")\n",
    "df_users=df_users.filter((df_users.count_ad< 9630) & (df_users.count_ad > 144)).select(['deviceId']).distinct()\n",
    "data=data.join(df_users,[\"deviceId\"],\"inner\")\n",
    "\n",
    "\n",
    "\n",
    "def popularity(data, newsMeanMap):\n",
    "    #Log(\"Computing news mean\")\n",
    "    newsMeandf = data.groupby(data.hashId).agg({'overallTimeSpent' : 'sum', 'hashId' : 'count'}).toPandas()\n",
    "\n",
    "    for v in newsMeandf.values:\n",
    "        newsMeanMap[v[0]] = newsMeanMap[v[0]][0] + v[1],  newsMeanMap[v[0]][1] + v[2]\n",
    "\n",
    "newsMeanMap = defaultdict(lambda: (7, 1))\n",
    "popularity(data, newsMeanMap)\n",
    "\n",
    "\n",
    "data_filtered = data.filter(F.udf(lambda hashId, overallTimeSpent: overallTimeSpent > (2.5 * newsMeanMap[hashId][0]/newsMeanMap[hashId][1]), BooleanType())('hashId', 'overallTimeSpent'))\n",
    "\n",
    "df_notif=sqlContext.read.csv('gs://pvtrough_asia_south1/clustering/MLP_embedding_reduced', sep=',',\n",
    "                         inferSchema=True, header=True)\n",
    "from pyspark.sql.functions import array, col\n",
    "feat_cols = [str(x) for x in range(0,255)]\n",
    "df_notif = df_notif.select('hid', array([col(x) for x in feat_cols ]).alias('embedding'))\n",
    "\n",
    "df_notif=df_notif.withColumnRenamed(\"hid\",\"hashId\")\n",
    "\n",
    " # calculate idf_weight\n",
    "\n",
    "N = data_filtered.select('hashId').distinct().count()\n",
    "users = data_filtered.groupBy('hashId').agg(F.countDistinct('deviceId').alias('users'))\n",
    "idf_weights = users.withColumn('idf_weight', F.log(N / F.col('users')))\n",
    "\n",
    "\n",
    "\n",
    "# Multiply embeddings with IDF weights\n",
    "#idf_weights=idf_weights.withColumnRenamed(\"hid\",\"hashId\")\n",
    "df_result=df_embedding.join(idf_weights,[\"hashId\"])\n",
    "\n",
    "#df_result = df_embedding.join(idf_weights, df_embedding.hid == idf_weights.hashId)\n",
    "df_result = df_result.withColumn('weighted_embedding', F.expr('transform(embedding, (x, i) -> x * idf_weight)'))\n",
    "\n",
    "df_result = df_result.select('hashId','weighted_embedding')\n",
    "#df_result=df_result.withColumnRenamed(\"hid\",\"hashId\")\n",
    "df_joined=data_filtered.join(df_result,[\"hashId\"],\"left\")\n",
    "\n",
    "#df_joined = data_filtered.join(df_result, data_filtered.hashId == df_result.hid , 'left')\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "grouped_df = df_joined.groupBy(\"deviceId\").agg(*[\n",
    "        expr(f\"avg(weighted_embedding[{i}]) as avg_{i}\") for i in range(255)\n",
    "    ])\n",
    "\n",
    "avg_cols = [col(f\"avg_{i}\") for i in range(255)]\n",
    "grouped_df = grouped_df.withColumn(\"topic_avg_embedding\", array(*avg_cols))\n",
    "\n",
    "path_vec = \"gs://pvtrough_asia_south1/clustering/30_embeddings_users\"+\"/\"\n",
    "grouped_df.select('deviceId', 'topic_avg_embedding').write.parquet(path=path_vec, mode='overwrite')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Users embedding for 40 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 40\n",
    "st_date = sampled_dates[3] - datetime.timedelta(days=n_days+1)\n",
    "\n",
    "date_fmt = \"%Y/%m/%d\"\n",
    "\n",
    "dates = [(st_date + datetime.timedelta(days=i)) for i in range(n_days+1)]\n",
    "dates.sort()\n",
    "\n",
    "dates_str = [date.strftime(date_fmt) for date in dates]\n",
    "print(dates_str)\n",
    "# millisMin = dates[0].timestamp() * 1000\n",
    "# millisMax = (dates[-1] + datetime.timedelta(days=1)).timestamp() * 1000\n",
    "\n",
    "hashIdsWithFilter, newsMap = getNewsData(dates[0], sampled_dates[3])\n",
    "\n",
    "\n",
    "hashIdsWithFilter, newsMap = getNewsData(dates[0], sampled_dates[3])\n",
    "\n",
    "datestr = [d.strftime(date_fmt) for d in dates]\n",
    "paths = get_path(datestr, 'timeSpentFrontEvents')\n",
    "\n",
    "data = sqlContext.read.parquet(*paths)\n",
    "\n",
    "data = filter_app(data, app_name=None)\n",
    "data = filter_tenant(data, tenant='en')\n",
    "\n",
    "# data = data.filter((data.eventTimestamp > millisMin) & (data.eventTimestamp < millisMax))\n",
    "data = data.select(data.deviceId, data.overallTimeSpent, (F.split(data.hashId, '-')[0]).alias('hashId'))\\\n",
    "        .groupby('deviceId', 'hashId') \\\n",
    "        .agg(F.max('overallTimeSpent').alias('overallTimeSpent'))\n",
    "\n",
    "data = data.filter(data.hashId.isin(hashIdsWithFilter))\n",
    "data=data.filter(data.overallTimeSpent<100)\n",
    "\n",
    "df_users=data.groupBy(\"deviceId\").count()\n",
    "df_users=df_users.withColumnRenamed(\"count\",\"count_ad\")\n",
    "df_users=df_users.filter((df_users.count_ad< 9630) & (df_users.count_ad > 144)).select(['deviceId']).distinct()\n",
    "data=data.join(df_users,[\"deviceId\"],\"inner\")\n",
    "\n",
    "\n",
    "\n",
    "def popularity(data, newsMeanMap):\n",
    "    #Log(\"Computing news mean\")\n",
    "    newsMeandf = data.groupby(data.hashId).agg({'overallTimeSpent' : 'sum', 'hashId' : 'count'}).toPandas()\n",
    "\n",
    "    for v in newsMeandf.values:\n",
    "        newsMeanMap[v[0]] = newsMeanMap[v[0]][0] + v[1],  newsMeanMap[v[0]][1] + v[2]\n",
    "\n",
    "newsMeanMap = defaultdict(lambda: (7, 1))\n",
    "popularity(data, newsMeanMap)\n",
    "\n",
    "\n",
    "data_filtered = data.filter(F.udf(lambda hashId, overallTimeSpent: overallTimeSpent > (2.5 * newsMeanMap[hashId][0]/newsMeanMap[hashId][1]), BooleanType())('hashId', 'overallTimeSpent'))\n",
    "\n",
    "df_notif=sqlContext.read.csv('gs://pvtrough_asia_south1/clustering/MLP_embedding_reduced', sep=',',\n",
    "                         inferSchema=True, header=True)\n",
    "from pyspark.sql.functions import array, col\n",
    "feat_cols = [str(x) for x in range(0,255)]\n",
    "df_notif = df_notif.select('hid', array([col(x) for x in feat_cols ]).alias('embedding'))\n",
    "\n",
    "df_notif=df_notif.withColumnRenamed(\"hid\",\"hashId\")\n",
    "\n",
    " # calculate idf_weight\n",
    "\n",
    "N = data_filtered.select('hashId').distinct().count()\n",
    "users = data_filtered.groupBy('hashId').agg(F.countDistinct('deviceId').alias('users'))\n",
    "idf_weights = users.withColumn('idf_weight', F.log(N / F.col('users')))\n",
    "\n",
    "\n",
    "\n",
    "# Multiply embeddings with IDF weights\n",
    "#idf_weights=idf_weights.withColumnRenamed(\"hid\",\"hashId\")\n",
    "df_result=df_embedding.join(idf_weights,[\"hashId\"])\n",
    "\n",
    "#df_result = df_embedding.join(idf_weights, df_embedding.hid == idf_weights.hashId)\n",
    "df_result = df_result.withColumn('weighted_embedding', F.expr('transform(embedding, (x, i) -> x * idf_weight)'))\n",
    "\n",
    "df_result = df_result.select('hashId','weighted_embedding')\n",
    "#df_result=df_result.withColumnRenamed(\"hid\",\"hashId\")\n",
    "df_joined=data_filtered.join(df_result,[\"hashId\"],\"left\")\n",
    "\n",
    "#df_joined = data_filtered.join(df_result, data_filtered.hashId == df_result.hid , 'left')\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "grouped_df = df_joined.groupBy(\"deviceId\").agg(*[\n",
    "        expr(f\"avg(weighted_embedding[{i}]) as avg_{i}\") for i in range(255)\n",
    "    ])\n",
    "\n",
    "avg_cols = [col(f\"avg_{i}\") for i in range(255)]\n",
    "grouped_df = grouped_df.withColumn(\"topic_avg_embedding\", array(*avg_cols))\n",
    "\n",
    "path_vec = \"gs://pvtrough_asia_south1/clustering/40_embeddings_users\"+\"/\"\n",
    "grouped_df.select('deviceId', 'topic_avg_embedding').write.parquet(path=path_vec, mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 91:(92 + 39) / 200][Stage 92:(127 + 39) / 200][Stage 94:(44 + 38) / 200]]]]\r"
     ]
    }
   ],
   "source": [
    "grouped_df.select(['topic_avg_embedding']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embeddings for notification using text (title of news only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, RobertaTokenizer , RobertaForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, RobertaTokenizer , RobertaForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "\n",
    "#loading model\n",
    "model_path = \"output/checkpoint-12000\"\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_path, num_labels=1000,output_hidden_states=True,from_tf=True)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "#defining function to generate embeddings\n",
    "def get_embedding(model, tokenizer, text):\n",
    "    model.eval()\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        input_ids = tokenizer.encode(text, add_special_tokens=True, return_tensors='pt').to(device)\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        hidden_states = outputs.hidden_states\n",
    "        last_layer_hidden_states = hidden_states[-1]\n",
    "        embeddings = torch.mean(last_layer_hidden_states, dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2023/02/03', '2023/02/04', '2023/02/05', '2023/02/06', '2023/02/07', '2023/02/08', '2023/02/09', '2023/02/10', '2023/02/11', '2023/02/12', '2023/02/13']\n",
      "time: 714 ms (started: 2023-06-03 10:56:19 +00:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 145:===========================>                        (107 + 93) / 200]\r"
     ]
    }
   ],
   "source": [
    "n_days = 10\n",
    "st_date = sampled_dates[3] - datetime.timedelta(days=10+1)\n",
    "\n",
    "date_fmt = \"%Y/%m/%d\"\n",
    "\n",
    "dates = [(st_date + datetime.timedelta(days=i)) for i in range(n_days+1)]\n",
    "dates.sort()\n",
    "\n",
    "dates_str = [date.strftime(date_fmt) for date in dates]\n",
    "print(dates_str)\n",
    "# millisMin = dates[0].timestamp() * 1000\n",
    "# millisMax = (dates[-1] + datetime.timedelta(days=1)).timestamp() * 1000\n",
    "\n",
    "hashIdsWithFilter, newsMap = getNewsData(dates[0], sampled_dates[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsMapProcessed={}\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "# Compute document embeddings for all articles in the dataset\n",
    "document_embeddings = {}\n",
    "for hId in tqdm(newsMap):\n",
    "    newsid = hId.split('-')[0]\n",
    "    #print(h)\n",
    "    #newsMapProcessed[newsid] = {}\n",
    "    #newsMapProcessed[newsid]['title'] = newsMap[hId]['title']\n",
    "    #newsMapProcessed[newsid]['content'] = newsMap[hId]['content']\n",
    "    \n",
    "    #newsMapProcessed[newsid]['features'] = newsMapProcessed[h]['title'] + newsMapProcessed[h]['content']\n",
    "    text=newsMap[hId]['title']\n",
    "    document_embeddings[newsid] = get_embedding(model, tokenizer, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 872 Âµs (started: 2023-06-19 12:33:12 +00:00)\n"
     ]
    }
   ],
   "source": [
    "sampled_dates = [datetime.datetime(2023,2,1,0,0,0), datetime.datetime(2023,2,6,0,0,0),datetime.datetime(2023,2,9,0,0,0),datetime.datetime(2023,2,14,0,0,0),\n",
    "                datetime.datetime(2023,2,18,0,0,0),datetime.datetime(2023,2,20,0,0,0), datetime.datetime(2023,2,25,0,0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2023/02/03', '2023/02/04', '2023/02/05', '2023/02/06', '2023/02/07', '2023/02/08', '2023/02/09', '2023/02/10', '2023/02/11', '2023/02/12', '2023/02/13']\n",
      "time: 791 ms (started: 2023-06-19 12:33:16 +00:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 150:>  (0 + 1) / 1][Stage 152:>  (0 + 1) / 1][Stage 153:>  (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "n_days = 10\n",
    "st_date = sampled_dates[3] - datetime.timedelta(days=10+1)\n",
    "\n",
    "date_fmt = \"%Y/%m/%d\"\n",
    "\n",
    "dates = [(st_date + datetime.timedelta(days=i)) for i in range(n_days+1)]\n",
    "dates.sort()\n",
    "\n",
    "dates_str = [date.strftime(date_fmt) for date in dates]\n",
    "print(dates_str)\n",
    "# millisMin = dates[0].timestamp() * 1000\n",
    "# millisMax = (dates[-1] + datetime.timedelta(days=1)).timestamp() * 1000\n",
    "\n",
    "hashIdsWithFilter, newsMap = getNewsData(dates[0], sampled_dates[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datestr = [d.strftime(date_fmt) for d in dates]\n",
    "paths = get_path(datestr, 'timeSpentFrontEvents')\n",
    "\n",
    "data = sqlContext.read.parquet(*paths)\n",
    "\n",
    "data = filter_app(data, app_name=None)\n",
    "data = filter_tenant(data, tenant='en')\n",
    "\n",
    "# data = data.filter((data.eventTimestamp > millisMin) & (data.eventTimestamp < millisMax))\n",
    "data = data.filter(data.categories.isNotNull())\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "#data=data.select(data.deviceId, (F.split(data.hashId, '-')[0]).alias('hashId'),data.notificationOpened,explode(data.categories)).withColumnRenamed(\"col\",\"category\")\n",
    "data = data.select(data.deviceId, data.overallTimeSpent, (F.split(data.hashId, '-')[0]).alias('hashId')).groupby('deviceId', 'hashId').agg(F.max('overallTimeSpent').alias('overallTimeSpent'))\n",
    "\n",
    "data = data.filter(data.hashId.isin(hashIdsWithFilter))\n",
    "#data=data.filter(data.overallTimeSpent<100)\n",
    "\n",
    "#data=data.na.fill(value=0,subset=[\"notificationOpened\"])\n",
    "#data=data.na.fill(value=0,subset=[\"notificationOpened\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[deviceId: string, hashId: string, overallTimeSpent: double]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.69 ms (started: 2023-06-19 12:33:37 +00:00)\n"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 150:>  (0 + 1) / 1][Stage 152:>  (0 + 1) / 1][Stage 153:>  (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "df_notif = sqlContext.read.csv('gs://pvtrough_asia_south1/clustering/df_embedding_LM_large.csv', sep=',',\n",
    "                         inferSchema=True, header=True)\n",
    "from pyspark.sql.functions import array, col\n",
    "feat_cols = [str(x) for x in range(0,768)]\n",
    "df_notif = df_notif.select('hid', array([col(x) for x in feat_cols ]).alias('embedding'))\n",
    "\n",
    "df_notif=df_notif.withColumnRenamed(\"hid\",\"hashId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hashId: string, embedding: array<double>]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.8 ms (started: 2023-06-19 12:34:19 +00:00)\n"
     ]
    }
   ],
   "source": [
    "df_notif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_embedd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 292 ms (started: 2023-06-19 12:34:38 +00:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 150:>  (0 + 1) / 1][Stage 152:>  (0 + 1) / 1][Stage 153:>  (0 + 1) / 1]  \r"
     ]
    }
   ],
   "source": [
    "path_vec = \"gs://pvtrough_asia_south1/clustering/30_embeddings_users\"+\"/\"\n",
    "df_devices=sqlContext.read.parquet(path_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[deviceId: string, topic_avg_embedding: array<double>]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.82 ms (started: 2023-06-19 12:34:42 +00:00)\n"
     ]
    }
   ],
   "source": [
    "df_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 48.6 ms (started: 2023-06-19 12:34:45 +00:00)\n"
     ]
    }
   ],
   "source": [
    "df_train=data.join(df_notif,[\"hashId\"],\"inner\").join(df_devices,['deviceId'],\"inner\").select(['deviceId','hashId','overallTimeSpent','embedding','topic_avg_embedding'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.12 ms (started: 2023-06-19 12:34:50 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.functions import array_to_vector\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DataParallel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, RobertaTokenizer , RobertaForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback\n",
    "from pyspark.sql.functions import array, col\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import array_contains, size, col\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import expr, udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 45.9 ms (started: 2023-06-19 12:34:52 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, concat\n",
    "\n",
    "train_data = df_train.withColumn('embedding_features', \n",
    "                                            concat(col('topic_avg_embedding'), col('embedding')))\\\n",
    "                                .select('embedding_features', 'overallTimeSpent')\n",
    "\n",
    "train_data=train_data.select(array_to_vector('embedding_features').alias('vec_embedding'),'overallTimeSpent')#.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training using Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'vec_embedding', labelCol='overallTimeSpent', maxIter=100, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_data)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction using Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df is data of next 7 days news embedding and 40 days users embdding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_predictions.select(\"prediction\",\"MV\",\"features\").show(5)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"MV\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = lr_model.evaluate(test_df)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr_model.transform(test_df)\n",
    "predictions.select(\"prediction\",\"MV\",\"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Decision tree for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressordt = DecisionTreeRegressor(featuresCol ='vec_embedding', labelCol = 'overallTimeSpent')\n",
    "dt_model = dt.fit(train_data)\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "dt_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"overallTimeSpent\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = dt_evaluator.evaluate(dt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GBT regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(featuresCol = 'vec_embedding', labelCol = 'overallTimeSpent', maxIter=10)\n",
    "gbt_model = gbt.fit(train_data)\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "gbt_predictions.select('prediction', 'overallTimeSpent', 'features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"MV\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = gbt_evaluator.evaluate(gbt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using XGB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.08 ms (started: 2023-06-19 12:35:40 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from xgboost.spark import SparkXGBRegressor\n",
    "spark_reg_estimator = SparkXGBRegressor(\n",
    "  features_col=\"vec_embedding\",\n",
    "  label_col=\"overallTimeSpent\",\n",
    "  num_workers=4,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_regressor_model = spark_reg_estimator.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test_spark_dataframe = spark_reg_estimator.predict(test_spark_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.spark import SparkXGBRegressor\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "\n",
    "label_name = \"overallTimeSpent\"\n",
    "\n",
    "feature_names = \"vec_embedding\"\n",
    "\n",
    "regressor = SparkXGBRegressor(\n",
    "  features_col=feature_names,\n",
    "  label_col=label_name,\n",
    "  num_workers=2,\n",
    "  use_gpu=True,\n",
    ")\n",
    "\n",
    "model = regressor.fit(train_data)\n",
    "\n",
    "predict_df = model.transform(test_df)\n",
    "predict_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "484685207"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 29s (started: 2023-06-12 10:51:46 +00:00)\n"
     ]
    }
   ],
   "source": [
    "df_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 23.1 ms (started: 2023-06-12 10:53:16 +00:00)\n"
     ]
    }
   ],
   "source": [
    "user_embedd=data.select(['deviceId']).distinct().join(df_devices,['deviceId'],'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 12s (started: 2023-06-12 10:53:16 +00:00)\n"
     ]
    }
   ],
   "source": [
    "user_embedd.write.parquet(path=\"gs://pvtrough_asia_south1/clustering/30_embeddings_users_v2\"+\"/\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1358084"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 57.8 s (started: 2023-06-12 10:54:28 +00:00)\n"
     ]
    }
   ],
   "source": [
    "user_embedd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 20\n",
    "  \n",
    "each_len = user_embedd.count() // n_splits\n",
    "  \n",
    "copy_df = user_embedd\n",
    "hhhhg=[]\n",
    "i = 0\n",
    "while i < n_splits:\n",
    "  \n",
    "    temp_df = copy_df.limit(each_len)\n",
    "  \n",
    "    copy_df = copy_df.subtract(temp_df)\n",
    "    #temp_df.cache()\n",
    "    #temp_df.show(truncate=False)\n",
    "    temp=temp_df.select('topic_avg_embedding').rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    #user_weight=torch.FloatTensor(hhhhg)\n",
    "    hhhhg.extend(temp)\n",
    "    print(i)\n",
    "    #path_vec = \"gs://pvtrough_asia_south1/clustering/30_embeddings_users_v2\"+str(i)+\"/\"\n",
    "    #temp_df.write.parquet(path=path_vec, mode='overwrite')\n",
    "    #print(path_vec)\n",
    "\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.67 ms (started: 2023-06-12 10:55:26 +00:00)\n"
     ]
    }
   ],
   "source": [
    "df_temp=user_embedd.sample(0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[deviceId: string, topic_avg_embedding: array<double>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.88 ms (started: 2023-06-12 10:55:26 +00:00)\n"
     ]
    }
   ],
   "source": [
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 150:>  (0 + 1) / 1][Stage 152:>  (0 + 1) / 1][Stage 153:>  (0 + 1) / 1]  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 5s (started: 2023-06-12 10:55:26 +00:00)\n"
     ]
    }
   ],
   "source": [
    "hhhhg=df_temp.select('topic_avg_embedding').rdd.flatMap(lambda x: x).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhhhg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 150:>  (0 + 1) / 1][Stage 152:>  (0 + 1) / 1][Stage 153:>  (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "hh[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 27.2 ms (started: 2023-06-12 10:56:31 +00:00)\n"
     ]
    }
   ],
   "source": [
    "notif_embedd=data.select(['hashId']).distinct().join(df_notif,['hashId'],'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notif_embedd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 150:>  (0 + 1) / 1][Stage 152:>  (0 + 1) / 1][Stage 153:>  (0 + 1) / 1]  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 58.6 s (started: 2023-06-12 10:57:28 +00:00)\n"
     ]
    }
   ],
   "source": [
    "hh=notif_embedd.select('embedding').rdd.flatMap(lambda x: x).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.31 s (started: 2023-06-12 10:58:26 +00:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 150:>  (0 + 1) / 1][Stage 152:>  (0 + 1) / 1][Stage 153:>  (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 20\n",
    "i = 0\n",
    "user_weight_dict={}\n",
    "while i < n_splits:\n",
    "    path_vec = \"gs://pvtrough_asia_south1/clustering/30_embeddings_users_v2\"+str(i)+\"/\"\n",
    "    user_embedd=sqlContext.read.parquet(path_vec)\n",
    "    hhhhg=user_embedd.select(\"topic_avg_embedding\").rdd.flatMap(lambda x: x).collect()\n",
    "    user_weight=torch.FloatTensor(hhhhg)\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embedd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.65 s (started: 2023-06-12 10:59:41 +00:00)\n"
     ]
    }
   ],
   "source": [
    "user_weight=torch.FloatTensor(hhhhg)\n",
    "user_embedding = nn.Embedding.from_pretrained(user_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 139 ms (started: 2023-06-12 10:59:59 +00:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 150:>  (0 + 1) / 1][Stage 152:>  (0 + 1) / 1][Stage 153:>  (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "notif_weight=torch.FloatTensor(hh)\n",
    "notification_embedding = nn.Embedding.from_pretrained(notif_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 641 Âµs (started: 2023-06-05 12:09:52 +00:00)\n"
     ]
    }
   ],
   "source": [
    "input = torch.LongTensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.73 ms (started: 2023-06-05 12:11:31 +00:00)\n"
     ]
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(128.9333)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.84 ms (started: 2023-06-05 12:41:43 +00:00)\n"
     ]
    }
   ],
   "source": [
    "notification_embedding(torch.LongTensor([5]))[0].pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(128.9333)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.44 ms (started: 2023-06-05 12:45:26 +00:00)\n"
     ]
    }
   ],
   "source": [
    "notif_weight[5].pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2851"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.02 ms (started: 2023-06-05 12:46:41 +00:00)\n"
     ]
    }
   ],
   "source": [
    "len((notif_weight*user_weight).sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(2851, 768)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.18 ms (started: 2023-06-05 12:42:01 +00:00)\n"
     ]
    }
   ],
   "source": [
    "notification_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5108e-02,  1.7124e-02,  6.4353e-02, -6.5654e-02, -2.4161e-01,\n",
       "        -2.6214e-02,  5.6563e-03,  8.6963e-02,  1.6884e-01,  3.3735e-02,\n",
       "        -1.4390e-01, -1.3412e-01, -5.2816e-03, -6.9174e-02,  1.3551e-01,\n",
       "        -3.0892e-01,  1.5388e-01,  7.7667e-02,  9.2706e-03,  4.9212e-02,\n",
       "         6.1749e-02,  2.1046e-01, -1.1446e-01, -2.7498e-02, -2.2766e-02,\n",
       "         3.8531e-02,  4.8362e-02,  6.5857e-03, -7.1668e-02,  7.7646e-03,\n",
       "        -1.1520e-01,  2.0958e-02, -8.4604e-03, -4.0875e-04, -1.0900e-01,\n",
       "        -4.7042e-02,  1.4840e-01,  4.2106e-03,  3.7527e-01,  7.4570e-02,\n",
       "         2.6634e-01, -1.2184e-02, -7.3225e-02,  5.4505e-02, -3.8288e-03,\n",
       "        -8.5135e-03, -8.3278e-02, -1.2030e-02,  3.9430e-02,  8.7702e-03,\n",
       "        -1.6854e-01,  6.7730e-02,  1.8065e-01,  6.3362e-02, -7.3808e-02,\n",
       "        -2.0488e-02,  2.8329e-02, -1.6032e-01,  6.9426e-02, -1.0553e-01,\n",
       "         2.5786e-02, -3.3725e-01, -1.2825e-01,  5.0514e-02, -3.6329e-02,\n",
       "        -4.0595e-02, -1.0010e-01, -3.7579e-02, -5.2207e-02,  4.3934e-02,\n",
       "        -2.4090e-02, -7.0003e-02,  2.5328e-02,  3.2305e-02,  4.1727e-02,\n",
       "        -5.8025e-02,  4.7287e-02, -6.1268e+00, -9.7549e-02, -1.5683e-01,\n",
       "         5.4983e-02, -1.6403e-01,  1.0949e+00,  6.0982e-02,  1.3270e-02,\n",
       "        -1.3499e-01,  7.4132e-02, -2.1437e-01,  6.9115e-02,  4.8085e-02,\n",
       "        -5.8836e-02,  4.2861e-02,  3.0738e-02,  1.7839e-02,  1.0579e-01,\n",
       "         1.4054e-01, -1.5142e-01,  9.3764e-03, -5.6944e-03, -1.0276e-01,\n",
       "         1.1132e-02, -5.8750e-03, -2.8897e-02,  1.2946e-01, -3.0380e-02,\n",
       "        -7.9690e-04, -9.2247e-02, -2.8117e-01, -9.6032e-02,  6.4411e-02,\n",
       "         7.7738e-02, -1.6957e-01,  1.6747e-01,  1.2636e-01, -3.5223e-02,\n",
       "         7.5445e-02,  2.2873e-02,  1.0424e-01,  6.9573e-02,  1.0662e-01,\n",
       "         1.3828e-01,  6.7099e-02,  5.3789e-02, -1.1146e-02,  9.0159e-02,\n",
       "         5.2433e-02, -2.6553e-02,  2.4225e-02, -9.5069e-02, -1.4827e-01,\n",
       "        -1.0663e-01, -2.8075e-01, -8.3618e-02, -5.0250e-02,  6.7042e-02,\n",
       "         8.0933e-02,  3.1384e-02,  9.0775e-02,  3.6525e-02,  1.1003e-01,\n",
       "         8.6733e-03, -9.9179e-03, -6.8065e-02,  3.6762e-02,  4.5909e-04,\n",
       "        -1.0664e-02,  2.1583e-02, -1.8981e-01,  1.2598e-01, -8.2134e-03,\n",
       "         3.1128e-02, -7.8893e-02, -2.5032e-02,  7.7671e-02,  2.2672e-01,\n",
       "         3.8548e-01,  2.1675e-02,  4.2000e-01,  6.5368e-02,  4.8175e-01,\n",
       "         1.1097e-01,  1.2959e-01, -1.5214e-02, -5.1805e-02, -1.2527e-01,\n",
       "         1.3584e-01, -1.7319e-01,  4.7619e-02,  3.1788e-02,  7.6932e-02,\n",
       "        -1.1356e-03, -7.1555e-03, -1.3512e-01, -3.2419e-02, -8.4338e-02,\n",
       "        -1.2678e-01,  1.7345e-01,  8.2897e-02,  2.6136e-04, -1.6793e-02,\n",
       "         1.5916e-01,  7.2596e-02,  6.3899e-02, -6.2010e-02,  1.0568e-01,\n",
       "         8.7217e-02,  5.9523e-02,  1.0074e-01,  6.8018e-02,  2.4594e-03,\n",
       "        -1.6058e-02,  4.1587e-02, -3.2873e-02,  1.4708e-01,  5.9689e-02,\n",
       "         2.6753e-02,  5.0491e-03, -1.4256e-01, -1.6245e-03, -1.6370e-01,\n",
       "         1.1045e-02, -9.9707e-03,  1.5088e-01, -6.7985e-02,  1.9670e-01,\n",
       "         4.4896e-02,  1.4529e-01,  1.3259e-03,  1.0757e-02, -1.9581e-01,\n",
       "         7.3936e-03,  1.6465e-01, -1.1236e-01,  1.4371e-01, -9.3396e-02,\n",
       "        -3.6842e-02, -1.1342e-01, -7.2947e-01,  7.0486e-02,  2.6486e-01,\n",
       "         9.2547e-02,  1.2954e-01,  3.4810e-03,  6.1256e-03,  1.4771e-01,\n",
       "        -6.6625e-04,  6.4599e-02,  2.7254e-02, -1.1432e-01, -6.0247e-02,\n",
       "         5.6919e-02, -6.1535e-02, -1.8779e-01, -9.8709e-02,  5.3359e-02,\n",
       "        -5.7572e-02, -7.7687e-02,  1.6302e-01,  1.4472e-01,  1.1688e-02,\n",
       "        -2.6694e-01,  1.8847e-01,  4.5289e-03,  1.1700e-01,  1.4110e-02,\n",
       "         1.2371e-01,  9.3986e-02,  6.0589e-02,  2.7169e-01,  7.5174e-02,\n",
       "        -3.6905e-02, -9.2755e-02, -1.9294e-02, -1.1722e-01,  1.9622e-01,\n",
       "        -1.1621e-01, -6.3395e-03,  9.0276e-02, -3.4554e-01, -5.1509e-01,\n",
       "        -2.2271e-02,  1.4212e-01,  5.5167e-02, -7.5584e-03, -4.7193e-02,\n",
       "        -3.5200e-02, -2.6392e-02,  1.3438e-02, -1.2968e-02,  4.3619e-02,\n",
       "        -5.4536e-03, -4.5575e-02,  5.5368e-02, -1.4868e-01, -2.1480e-02,\n",
       "         4.3083e-02, -8.9143e-02,  1.7288e-01,  2.5977e-01,  7.5877e-02,\n",
       "         1.6451e-02, -1.1934e-01,  2.0860e-01, -1.4985e-01,  1.8048e-01,\n",
       "         1.8232e-01, -8.8311e-02,  8.9221e-02,  2.3072e-02,  4.6100e-02,\n",
       "         3.3407e-02,  7.1542e-02, -2.7239e-02, -9.0320e-02, -6.7600e-02,\n",
       "        -8.2464e-02,  1.2022e-01,  2.7052e-01,  5.3832e-02,  1.5425e-01,\n",
       "         1.8108e-02, -2.7781e-02,  2.9270e-01, -1.8924e-02,  1.0002e-02,\n",
       "        -8.1032e-03,  1.2507e-01, -4.4057e-02, -6.0447e-02,  2.0258e-01,\n",
       "        -5.9661e-02, -1.5914e-01,  3.7124e-02,  1.4240e-01, -2.2429e-02,\n",
       "        -3.9623e-02,  5.3370e-02, -3.4743e-02,  4.2586e-02,  6.9194e-02,\n",
       "         5.7572e-02, -7.4340e-02, -1.4728e-03,  1.1343e-01, -1.4532e-01,\n",
       "        -5.0325e-02, -4.5443e-02,  2.8993e-02,  6.2280e-02,  1.6814e-01,\n",
       "         4.2776e-01,  7.7017e-01,  1.3746e-01, -1.5831e-01,  8.7777e-02,\n",
       "        -3.2303e-02, -8.1601e-03,  1.9932e-01,  1.2579e-01,  1.5301e-01,\n",
       "        -1.4383e-01,  7.5027e-02, -1.7846e-01, -6.7028e-02,  3.7197e-02,\n",
       "         7.1441e-02,  1.4171e-01, -5.4988e-02,  8.7888e-02,  6.8537e-02,\n",
       "         2.0960e-01, -8.1044e-03,  9.6331e-02,  2.6477e-02,  3.0739e-02,\n",
       "         8.5943e-03,  8.6080e-02,  9.1742e-02,  9.3368e-02,  2.4444e-02,\n",
       "         1.8026e-02,  1.1602e-01, -7.2110e-03,  6.0831e-02, -2.2566e-01,\n",
       "        -7.5033e-02, -1.2945e-01,  2.5086e-01, -1.1505e-01,  5.8084e-02,\n",
       "         2.3333e-01, -4.8815e-02, -2.2054e-02, -1.2491e-01, -6.1470e-02,\n",
       "        -5.1325e-02, -9.3454e-03,  1.8201e-01, -1.3783e-01, -3.3170e-02,\n",
       "        -5.6550e-02,  1.0551e-02, -7.0120e-02,  4.7842e-02,  1.0549e-02,\n",
       "        -2.1422e-03,  1.0745e-01, -3.9483e-02,  3.9800e-02, -4.4791e-02,\n",
       "         4.7904e-02,  3.1788e-02, -1.2087e-03,  1.2598e-01,  8.5180e-02,\n",
       "        -7.3195e-02,  2.0988e-01, -3.4378e-02,  1.8944e-01, -1.2688e-01,\n",
       "        -1.2765e-02, -8.7183e-02, -1.2299e-01,  9.2655e-02,  7.4649e-02,\n",
       "         1.8214e-01, -6.6040e-03, -6.3499e-02, -1.2653e-01,  3.2167e-01,\n",
       "         9.3499e-02,  1.3624e-01,  1.8066e-02,  1.4024e-01,  3.1007e-02,\n",
       "        -6.0681e-02,  2.8047e-01, -2.2129e-01, -8.9914e-03, -1.4553e-02,\n",
       "        -6.5047e-02, -1.3704e-01,  2.2573e-01, -9.6381e-02,  4.7104e-01,\n",
       "        -2.1932e-02,  5.0663e-02, -9.2815e-02,  9.4882e-02, -1.5351e-01,\n",
       "        -3.1082e-02,  1.3906e-01, -2.3251e-01, -4.6419e-02,  2.3058e-02,\n",
       "         2.4624e-02, -3.4073e-02, -4.0798e-03,  1.7731e-02, -1.1016e-01,\n",
       "         4.8191e-02,  9.1717e-02,  6.4826e-02,  4.6293e-02,  4.3683e-02,\n",
       "         1.0416e-01, -1.3630e-02, -1.9651e-03,  1.3513e-01, -1.0614e-01,\n",
       "        -3.8428e-02, -3.5919e-02, -1.9197e-01, -1.7725e+00, -6.1041e-02,\n",
       "         4.2052e-02,  2.3773e-01,  6.1149e-02, -1.1354e-01,  1.0794e-01,\n",
       "        -2.9582e-02, -9.6220e-02, -6.7097e-02,  5.8353e-02, -2.7495e-02,\n",
       "        -8.8732e-02,  3.5246e-03,  2.2371e-01, -2.9783e-02,  4.0714e-02,\n",
       "         4.7706e-02,  3.3489e-02,  8.5249e-03, -6.9776e-02,  2.3813e-02,\n",
       "        -2.3223e-02, -7.4628e-05,  2.2561e-01, -1.2257e-01, -2.0356e-02,\n",
       "        -2.3901e-02,  1.4417e-01,  1.8875e-01,  5.1174e-02, -3.3426e-02,\n",
       "         8.8971e-02, -1.6394e-02,  6.5087e-02,  1.5603e-01, -6.9847e-02,\n",
       "        -1.0019e-01, -1.6622e-01,  7.3106e-02,  1.3867e-01,  1.4337e-01,\n",
       "         3.8765e-02, -3.9130e-01, -3.9034e-02, -2.8289e-02,  1.6179e-02,\n",
       "        -2.3442e-02, -2.1909e-03,  5.1034e-02,  1.6475e-02,  4.7953e-02,\n",
       "         9.4354e-02, -1.9284e-02, -1.1975e-01,  1.1036e-02,  5.3184e-02,\n",
       "         6.8374e-02,  1.8350e-01,  1.5452e-01, -3.1680e-02,  4.6514e-02,\n",
       "         4.1247e-02, -8.1050e-02,  3.4516e-02, -2.4729e-04, -1.4355e-01,\n",
       "        -1.2991e-01, -3.9084e-02, -5.7268e-02,  2.2898e-01, -7.4991e-02,\n",
       "        -3.5921e-02,  3.3657e-02, -1.1450e-02, -2.9736e-02,  4.3106e-02,\n",
       "         2.0197e-01, -1.2650e-01,  1.4832e-01, -2.6186e-02, -2.4936e-01,\n",
       "         1.9284e-02,  5.4886e-02,  1.6681e-01,  1.5259e-01,  8.8753e-03,\n",
       "        -2.3608e-01,  6.1127e-02,  7.0530e-02,  4.5473e-02,  5.1513e-02,\n",
       "        -1.6505e-02, -1.4634e-01,  6.5958e-02,  2.9717e-02, -9.3700e-02,\n",
       "         1.5216e-02,  2.7288e+00, -8.3928e-02, -1.7597e-01,  1.1505e-02,\n",
       "        -9.8956e-02, -1.4722e-02,  1.7428e-02, -1.0473e-01, -6.6622e-02,\n",
       "        -2.1401e-02, -1.1938e-01, -3.6593e-02,  1.1500e-02, -4.2089e-02,\n",
       "         6.4963e-03,  3.8492e-02,  4.4287e-02,  2.8809e-02,  3.6256e-02,\n",
       "        -7.6137e-02,  3.6671e-02, -6.7125e-02, -7.9790e-02,  1.3971e-01,\n",
       "         5.5084e-02,  2.4383e-02,  1.6086e-01, -2.0213e-01, -1.7847e-03,\n",
       "        -1.2060e-01, -2.3463e-02,  7.9321e-02,  2.0026e-01,  1.5424e-01,\n",
       "         1.7031e-01, -1.8651e-01, -2.6238e-01,  8.0880e+00,  6.3267e-02,\n",
       "        -3.4209e-02,  4.9532e-02,  1.5268e-02,  6.3302e-02,  4.9761e-03,\n",
       "        -1.4697e-01,  2.0070e-03, -8.2588e-02,  2.9466e-02,  9.8896e-02,\n",
       "         1.4533e-01, -1.9167e-01,  8.8667e-02,  1.2211e-01, -6.7321e-02,\n",
       "         1.0942e-02,  8.0610e-03,  9.1293e-02, -3.5136e-02,  4.2794e-02,\n",
       "        -3.8572e-02,  4.4031e-01, -7.8779e-02,  1.1605e-01,  3.2071e-01,\n",
       "         1.4815e-01, -8.2994e-02,  3.6372e-02,  2.7341e-02, -6.4881e-02,\n",
       "         1.4292e-02, -1.0301e-02,  6.6660e-02,  1.1790e-01,  1.2759e-01,\n",
       "         5.6461e-02,  1.2311e-01,  5.7845e-02,  1.6376e-01,  9.5522e-02,\n",
       "         3.5295e-02, -7.5182e-02,  4.5187e-02,  2.2382e-03, -2.2395e-03,\n",
       "         9.6164e-02,  1.1272e-01, -9.9650e-02, -3.6293e-02,  5.2544e-03,\n",
       "         1.0745e-01,  2.0751e-03, -5.0308e-02,  1.0913e-01, -1.4401e-01,\n",
       "         1.9745e-02,  1.9373e-01,  1.6441e-01,  2.6806e-02, -1.1785e-01,\n",
       "         4.6585e-02,  2.9880e-01, -2.2905e-01,  6.0341e-03,  4.9596e-02,\n",
       "         1.7733e-02,  2.4400e-01, -5.3138e-02,  5.4400e-02, -4.4515e-02,\n",
       "        -1.8317e-02,  3.5177e-02,  1.3125e-01, -5.1495e-02,  2.2195e-01,\n",
       "        -3.6325e-03, -8.0596e-02, -1.1974e-01, -3.3911e-02,  5.0480e-02,\n",
       "         6.9064e-02,  3.8399e-02,  2.3232e-01,  2.0871e-01,  6.8373e-02,\n",
       "        -1.7981e-02,  1.2492e-01, -5.2127e-02,  4.3103e-02,  2.1802e-01,\n",
       "         3.2164e-01, -1.6185e-02,  1.4554e-01, -1.2976e-01, -1.1633e-01,\n",
       "        -3.2594e-02,  1.1794e-01, -7.3940e-03, -6.8504e-02,  1.3780e-01,\n",
       "        -6.4428e-02, -2.3470e-02,  8.9149e-02, -2.3703e-02, -2.8337e-01,\n",
       "        -2.1864e-01,  1.5330e-02,  1.3894e-02, -9.5880e-02,  6.5698e-02,\n",
       "         2.7212e-01,  1.6614e-01,  1.6779e-01, -6.5624e-02, -1.1234e-02,\n",
       "        -4.8023e-02,  6.4171e-02, -1.6722e-01, -1.3401e-01, -1.5100e-02,\n",
       "        -1.5013e-01, -2.5433e-02, -4.9377e-02,  4.2133e-03,  7.1359e-02,\n",
       "         4.8003e-02, -1.0942e-01,  7.2255e-02,  3.7880e-02,  1.5880e-01,\n",
       "         1.1522e-01,  1.5561e-01,  2.3216e-01, -4.4869e-02,  1.5838e-01,\n",
       "         1.6974e-01,  2.9563e-01,  1.2960e-02, -2.3926e-02, -1.7721e-02,\n",
       "         6.7481e-02, -1.4587e+00, -1.4522e-01, -1.4348e-01, -4.9237e-02,\n",
       "         9.5658e-02,  1.2369e-01, -2.2425e-01,  4.6634e-03, -1.2673e-01,\n",
       "        -6.1688e-03,  1.3039e-02,  1.0573e-01,  1.0639e-02,  1.1512e-01,\n",
       "         1.8114e-01,  9.3169e-02, -9.9055e-02, -1.8764e-01,  5.2844e-01,\n",
       "         2.6472e-01, -3.6932e-01,  2.1040e-01,  1.3941e-01,  1.0711e-01,\n",
       "         1.5611e-02,  2.1570e-02,  1.1003e-01, -6.8550e-02, -9.1344e-02,\n",
       "        -9.3346e-02,  3.3376e-02, -4.1688e-02, -4.9791e-02,  2.2411e-01,\n",
       "        -7.1786e-02,  5.7897e-02, -5.2030e-02])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.17 ms (started: 2023-06-05 12:40:36 +00:00)\n"
     ]
    }
   ],
   "source": [
    "notification_embedding(torch.LongTensor([5]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2851"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.13 ms (started: 2023-06-05 07:09:49 +00:00)\n"
     ]
    }
   ],
   "source": [
    "len(notif_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68677"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.41 ms (started: 2023-06-05 10:15:02 +00:00)\n"
     ]
    }
   ],
   "source": [
    "len(hhhhg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhhhg[68676]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.4 ms (started: 2023-06-05 10:18:05 +00:00)\n"
     ]
    }
   ],
   "source": [
    "type(hhhhg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.14 ms (started: 2023-06-05 10:19:07 +00:00)\n"
     ]
    }
   ],
   "source": [
    "any(elem is None for elem in hhhhg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.64 ms (started: 2023-06-05 11:06:18 +00:00)\n"
     ]
    }
   ],
   "source": [
    "user_embedd_9098=user_embedd.coalesce(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o5496.parquet.\n: java.util.NoSuchElementException: None.get\n\tat scala.None$.get(Option.scala:529)\n\tat scala.None$.get(Option.scala:527)\n\tat org.apache.spark.sql.execution.datasources.BasicWriteJobStatsTracker$.metrics(BasicWriteStatsTracker.scala:175)\n\tat org.apache.spark.sql.execution.command.DataWritingCommand.metrics(DataWritingCommand.scala:51)\n\tat org.apache.spark.sql.execution.command.DataWritingCommand.metrics$(DataWritingCommand.scala:51)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.metrics$lzycompute(InsertIntoHadoopFsRelationCommand.scala:49)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.metrics(InsertIntoHadoopFsRelationCommand.scala:49)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.metrics$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.metrics(commands.scala:104)\n\tat org.apache.spark.sql.execution.SparkPlanInfo$.fromSparkPlan(SparkPlanInfo.scala:63)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:101)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\n\tat sun.reflect.GeneratedMethodAccessor309.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43muser_embedd_9098\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitionBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeviceId\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgs://pvtrough_asia_south1/clustering/30_embeddings_users_v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m678\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py:1250\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1250\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o5496.parquet.\n: java.util.NoSuchElementException: None.get\n\tat scala.None$.get(Option.scala:529)\n\tat scala.None$.get(Option.scala:527)\n\tat org.apache.spark.sql.execution.datasources.BasicWriteJobStatsTracker$.metrics(BasicWriteStatsTracker.scala:175)\n\tat org.apache.spark.sql.execution.command.DataWritingCommand.metrics(DataWritingCommand.scala:51)\n\tat org.apache.spark.sql.execution.command.DataWritingCommand.metrics$(DataWritingCommand.scala:51)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.metrics$lzycompute(InsertIntoHadoopFsRelationCommand.scala:49)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.metrics(InsertIntoHadoopFsRelationCommand.scala:49)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.metrics$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.metrics(commands.scala:104)\n\tat org.apache.spark.sql.execution.SparkPlanInfo$.fromSparkPlan(SparkPlanInfo.scala:63)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:101)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\n\tat sun.reflect.GeneratedMethodAccessor309.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 245 ms (started: 2023-06-05 11:06:33 +00:00)\n"
     ]
    }
   ],
   "source": [
    "user_embedd_9098.write.partitionBy(\"deviceId\").parquet(\"gs://pvtrough_asia_south1/clustering/30_embeddings_users_v2\"+str(678)+\"/\",mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
