{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9d4afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd home/Gourav/Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af50d50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting of Regressor training')\n",
    "import logging\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "print(os.getcwd())\n",
    "if not os.path.exists('/home/Gourav/Models/approach_logs/Regressor/'):\n",
    "    os.makedirs('/home/Gourav/Models/approach_logs/Regressor/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84f7c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52243f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, udf, when\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import DataFrameStatFunctions as stat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, udf, when\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "from pymongo import MongoClient\n",
    "from tqdm import tqdm\n",
    "from pyspark.sql import DataFrameStatFunctions as stat\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "NIS_DATA_BASE_PATH = \"gs://nis-segment-datasource-v3/processed/\"\n",
    "NIS_OLD_DATA_BASE_PATH = \"gs://nis-localytics-datasource/processed/\"\n",
    "\n",
    "conf = SparkConf().setAll([('spark.driver.memory', '100g'), ('spark.broadcast.blockSize', '50m'), (\"spark.executor.instances\", '30')])\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751edd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(dates, prefix, padding=None):\n",
    "    dates_ = dates + []\n",
    "    if padding:\n",
    "        st_date, ed_date = sorted(dates)[0], sorted(dates)[-1]\n",
    "        for i in range(1, 4):\n",
    "            d = (datetime.datetime.strptime(ed_date, date_fmt) + datetime.timedelta(days=i)).strftime(date_fmt)\n",
    "            if d < datetime.datetime.today().strftime(date_fmt):\n",
    "                dates_.append(d)\n",
    "    dates_ = list(set(dates_) - set([\"2019/02/17\", \"2019/02/18\", \"2019/05/28\", \"2019/06/03\", \"2019/07/02\", \"2019/07/03\", \"2019/07/04\", \"2019/11/13\", \"2019/11/14\", \"2020/02/22\", \"2020/03/31\", \"2020/04/16\", \"2020/04/18\", \"2020/05/11\", \"2021/05/13\"]))\n",
    "    paths = []\n",
    "    for date in dates_:\n",
    "        base_path = NIS_DATA_BASE_PATH\n",
    "        if date < \"2018/06/26\":\n",
    "            base_path = NIS_OLD_DATA_BASE_PATH\n",
    "        paths.append(base_path + date + \"/\" + prefix + \"/*.parquet\")\n",
    "    return paths\n",
    "\n",
    "date_fmt = \"%Y/%m/%d\"\n",
    "month_fmt = \"%Y/%m\"\n",
    "\n",
    "def millis2date(x):\n",
    "    try:\n",
    "        if x < 15000000000:\n",
    "            return datetime.datetime.fromtimestamp(x).strftime(date_fmt)\n",
    "        else:\n",
    "            return datetime.datetime.fromtimestamp(x / 1000.).strftime(date_fmt)\n",
    "    except:\n",
    "        return \"1970/01/01\"\n",
    "\n",
    "def millis2month(x):\n",
    "    try:\n",
    "        if x < 15000000000:\n",
    "            return datetime.datetime.fromtimestamp(x).strftime(month_fmt)\n",
    "        else:\n",
    "            return datetime.datetime.fromtimestamp(x / 1000.).strftime(month_fmt)\n",
    "    except:\n",
    "        return \"1970/01\"\n",
    "\n",
    "millis2date_udf = F.udf(millis2date, StringType())\n",
    "millis2month_udf = F.udf(millis2month, StringType())\n",
    "\n",
    "def divide_maps(d1, d2):\n",
    "    keys = set(d1.keys()).intersection(set(d2.keys()))\n",
    "    res = {}\n",
    "    for k in keys:\n",
    "        res[k] = d1[k] * 1. / (d2[k] + 1e-10)\n",
    "    return res\n",
    "\n",
    "def timediff(y, x, date_fmt=\"%Y/%m/%d\"): \n",
    "    end = datetime.datetime.strptime(y, date_fmt)\n",
    "    start = datetime.datetime.strptime(x, date_fmt)\n",
    "    delta = (end - start).days\n",
    "    return delta\n",
    "\n",
    "def monthdiff(y, x, month_fmt=\"%Y/%m\"): \n",
    "    millis = y - x\n",
    "    delta = millis / (1000 * 3600 * 24 * 30)\n",
    "    return delta\n",
    "\n",
    "timediff_udf = udf(timediff, IntegerType())\n",
    "monthdiff_udf = udf(monthdiff, IntegerType())\n",
    "\n",
    "def filter_platform(data, platform=None):\n",
    "    if platform == \"ANDROID\":\n",
    "        data = data.filter(data.platform == \"ANDROID\")\n",
    "    elif platform == \"IOS\":\n",
    "        data = data.filter(data.platform != \"ANDROID\")\n",
    "    return data\n",
    "\n",
    "def filter_category(data, categories=None):\n",
    "    if categories:\n",
    "        data = data.filter(data.categoryWhenEventHappened.isin(categories))\n",
    "    return data\n",
    "\n",
    "def filter_tenant(data, tenant=None):\n",
    "    if tenant in ['hi', 'HINDI']:\n",
    "        data = data.filter(data.tenant.isin(['hi', 'HINDI', 'Hindi', 'hindi']))\n",
    "    elif tenant in ['en', 'ENGLISH']:\n",
    "        data = data.filter(~data.tenant.isin(['hi', 'HINDI', 'Hindi', 'hindi']))\n",
    "    return data\n",
    "\n",
    "\n",
    "def filter_app(data, app_name=None):\n",
    "    if 'appName' in data.columns:\n",
    "        if app_name:\n",
    "            data = data.filter(data.appName == app_name)\n",
    "        else:\n",
    "            data = data.filter((data.appName != \"mini\") & (data.appName != \"crux\"))\n",
    "    return data\n",
    "\n",
    "\n",
    "# In[7]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ffad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_path(date, hours=None):\n",
    "    paths = []\n",
    "    base_path = NIS_RAW_DATA_BASE_PATH + date\n",
    "    if not hours:\n",
    "        return base_path + \"/*/*.gz\"\n",
    "    for hour in hours:\n",
    "        paths.append(base_path + \"/\" + str(hour).zfill(2) + \"/*.gz\")\n",
    "    return \",\".join(paths)\n",
    "\n",
    "def process_raw_data(paths):\n",
    "    def view_data_filters(x):\n",
    "        x = x['properties']\n",
    "        deviceid_filter = ('deviceId' in x) and (x['deviceId'] != '')\n",
    "        time_filter = ('timeSpent' in x) and (int(x['timeSpent']) <= 100) and (int(x['timeSpent']) >= 0)\n",
    "        return deviceid_filter and time_filter\n",
    "\n",
    "    try:\n",
    "        rdd = sc.textFile(paths) \\\n",
    "            .map(json.loads) \\\n",
    "            .filter(lambda x: \"batch\" in x).flatMap(lambda x: x[\"batch\"]) \\\n",
    "            .filter(lambda x: (\"event\" in x) and (x[\"event\"].lower() == \"timespent-front\")) \\\n",
    "            .filter(view_data_filters) \\\n",
    "            .map(lambda x: x['properties'])\n",
    "        view_data = rdd.map(lambda x: (x['deviceId'], x['hashId'][:-2], x['timeSpent'])) \\\n",
    "            .toDF(['deviceId', 'hashId', 'timeSpent'])\n",
    "        \n",
    "        view_data = view_data.filter(view_data.timeSpent.isNotNull())\n",
    "        #view_data = view_data.filter((getHashBucketUDF(view_data.deviceId) >= 16) & (getHashBucketUDF(view_data.deviceId) <= 25))\n",
    "        view_data = view_data.groupby(view_data.deviceId, view_data.hashId) \\\n",
    "                             .agg(F.max(view_data.timeSpent).alias('overallTimeSpent'))\n",
    "        \n",
    "        return view_data\n",
    "    except Exception as e:\n",
    "        logger.warning(\"Error processing data: \" + str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6020f580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import *\n",
    "def getNewsData(d1, d2):\n",
    "    newsMap = getNewsInDates(d1, d2)\n",
    "    hashIdList = list(newsMap.keys())\n",
    "\n",
    "    hashIdsWithFilter = []\n",
    "    for h in hashIdList:\n",
    "        if 'newsLanguage' in newsMap[h] and newsMap[h]['newsLanguage'] == 'english' and newsMap[h]['publishGroupList'][0]['countryCode'] == 'IN':\n",
    "            hashIdsWithFilter.append(h.split('-')[0])\n",
    "    \n",
    "    return hashIdsWithFilter, newsMap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c840742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(str(datetime.datetime.today().date()))\n",
    "hdlr = logging.FileHandler(\n",
    "    '/home/Gourav/Models/approach_logs/Regressor/' + str(datetime.datetime.today().date()) + '.log')\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22977824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Log(s, flag=True):\n",
    "    if flag:\n",
    "        logger.info(s)\n",
    "        print(s)\n",
    "\n",
    "Log('----', flag=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d43a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dates = [datetime.datetime(2023,2,1,0,0,0), datetime.datetime(2023,2,6,0,0,0),datetime.datetime(2023,2,9,0,0,0),datetime.datetime(2023,2,14,0,0,0),\n",
    "                datetime.datetime(2023,2,18,0,0,0),datetime.datetime(2023,2,20,0,0,0), datetime.datetime(2023,2,25,0,0,0)]\n",
    "sampled_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e316b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_days = 10\n",
    "test_days = 7\n",
    "embed_days = 30\n",
    "n_days = train_days + test_days + embed_days\n",
    "k = 0\n",
    "st_date = (sampled_dates[k]+ datetime.timedelta(days=test_days)) - datetime.timedelta(days=n_days)\n",
    "\n",
    "\n",
    "date_fmt = \"%Y/%m/%d\"\n",
    "\n",
    "dates = [(st_date + datetime.timedelta(days=i)) for i in range(n_days+1)]\n",
    "dates.sort()\n",
    "\n",
    "dates_str = [date.strftime(date_fmt) for date in dates]\n",
    "dates_str_embed = dates_str[0:embed_days]\n",
    "dates_str_train = dates_str[embed_days:embed_days+train_days+1]\n",
    "dates_str_test = dates_str[-test_days:]\n",
    "print(\"embed data dates : \", dates_str_embed, len(dates_str_embed))\n",
    "print(\"train data dates : \",dates_str_train, len(dates_str_train) )\n",
    "print(\"test data dates : \",dates_str_test, len(dates_str_test))\n",
    "# millisMin = dates[0].timestamp() * 1000\n",
    "# millisMax = (dates[-1] + datetime.timedelta(days=1)).timestamp() * 1000\n",
    "print(dates[0], dates[-1])\n",
    "hashIdsWithFilter, newsMap = getNewsData(dates[0], dates[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c58479-8c41-4c1f-866c-316c517e09ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d525b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need this for live data else skip if backtesting\n",
    "NIS_RAW_DATA_BASE_PATH = \"gs://inshorts-segment-raw/data/segment-raw-v5a/\"\n",
    "\n",
    "path = get_raw_path(datetime.datetime.now().strftime(\"%Y/%m/%d\"))\n",
    "today_data = process_raw_data(path)\n",
    "today_data = today_data.filter(today_data.hashId.isin(hashIdsWithFilter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3586e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate datasets\n",
    "def generate_data(datestr):\n",
    "#     datestr = [d.strftime(date_fmt) for d in dates]\n",
    "    datestr = [d.strftime(date_fmt) for d in dates]\n",
    "    paths = get_path(datestr, 'timeSpentFrontEvents')\n",
    "\n",
    "    data = sqlContext.read.parquet(*paths)\n",
    "\n",
    "    data = filter_app(data, app_name=None)\n",
    "    data = filter_tenant(data, tenant='en')\n",
    "\n",
    "    # data = data.filter((data.eventTimestamp > millisMin) & (data.eventTimestamp < millisMax))\n",
    "    data = data.filter(data.categories.isNotNull())\n",
    "    from pyspark.sql.functions import explode\n",
    "\n",
    "    #data=data.select(data.deviceId, (F.split(data.hashId, '-')[0]).alias('hashId'),data.notificationOpened,explode(data.categories)).withColumnRenamed(\"col\",\"category\")\n",
    "    data = data.select(data.deviceId, data.overallTimeSpent, (F.split(data.hashId, '-')[0]).alias('hashId')).groupby('deviceId', 'hashId').agg(F.max('overallTimeSpent').alias('overallTimeSpent'))\n",
    "\n",
    "    data = data.filter(data.hashId.isin(hashIdsWithFilter))\n",
    "    return data\n",
    "\n",
    "data_embed = generate_data(dates_str_embed)\n",
    "data_train = generate_data(dates_str_train)\n",
    "data_test = generate_data(dates_str_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28e848a-b575-4cfd-bfc8-03bf726c7f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372f0193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to be used when live data is needed\n",
    "data = data.union(today_data)\n",
    "data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a570c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity(data, newsMeanMap):\n",
    "    Log(\"Computing news mean\")\n",
    "    newsMeandf = data.groupby(data.hashId).agg({'overallTimeSpent' : 'sum', 'hashId' : 'count'}).toPandas()\n",
    "\n",
    "    for v in newsMeandf.values:\n",
    "        newsMeanMap[v[0]] = newsMeanMap[v[0]][0] + v[1],  newsMeanMap[v[0]][1] + v[2]\n",
    "\n",
    "newsMeanMap = defaultdict(lambda: (7, 1))\n",
    "popularity(data_embed, newsMeanMap)\n",
    "\n",
    "data_embed = data_embed.filter(F.udf(lambda hashId, overallTimeSpent: overallTimeSpent > (2.5 * newsMeanMap[hashId][0]/newsMeanMap[hashId][1]), BooleanType())('hashId', 'overallTimeSpent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbcc22c-f5cd-448f-aba0-d0f2e06bb89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity(data, newsMeanMap):\n",
    "    Log(\"Computing news mean\")\n",
    "    newsMeandf = data.groupby(data.hashId).agg({'overallTimeSpent' : 'sum', 'hashId' : 'count'}).toPandas()\n",
    "\n",
    "    for v in newsMeandf.values:\n",
    "        newsMeanMap[v[0]] = newsMeanMap[v[0]][0] + v[1],  newsMeanMap[v[0]][1] + v[2]\n",
    "\n",
    "newsMeanMap = defaultdict(lambda: (7, 1))\n",
    "popularity(data_train, newsMeanMap)\n",
    "\n",
    "data_train = data_train.filter(F.udf(lambda hashId, overallTimeSpent: overallTimeSpent > (2.5 * newsMeanMap[hashId][0]/newsMeanMap[hashId][1]), BooleanType())('hashId', 'overallTimeSpent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d3ca35-44d7-4cd6-8c25-208c10eaccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity(data, newsMeanMap):\n",
    "    Log(\"Computing news mean\")\n",
    "    newsMeandf = data.groupby(data.hashId).agg({'overallTimeSpent' : 'sum', 'hashId' : 'count'}).toPandas()\n",
    "\n",
    "    for v in newsMeandf.values:\n",
    "        newsMeanMap[v[0]] = newsMeanMap[v[0]][0] + v[1],  newsMeanMap[v[0]][1] + v[2]\n",
    "\n",
    "newsMeanMap = defaultdict(lambda: (7, 1))\n",
    "popularity(data_test, newsMeanMap)\n",
    "\n",
    "data_test = data_test.filter(F.udf(lambda hashId, overallTimeSpent: overallTimeSpent > (2.5 * newsMeanMap[hashId][0]/newsMeanMap[hashId][1]), BooleanType())('hashId', 'overallTimeSpent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b8e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2986af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_comb = data_embed.union(data_train)\n",
    "#data_comb.groupBy('eventName').count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee8e979-1559-4dfd-b58a-479d1a2b14db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342c0a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_comb.filter(data_comb.notificationOpened > 0).groupBy('eventName').count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaea4bb3-0e38-424a-9420-14bf8337ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whats happening in next few rowS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0943dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.clustering import LDA\n",
    "import time\n",
    "\n",
    "training_start = time.time()\n",
    "\n",
    "\n",
    "# data_filtered = data.filter(F.udf(lambda hashId, noti: overallTimeSpent > (2.5 * newsMeanMap[hashId][0]/newsMeanMap[hashId][1]), BooleanType())('hashId', 'overallTimeSpent'))\n",
    "# abcset=sqlContext.read.parquet(\"gs://pvtrough_asia_south1/tf_idf/devices_fulfilling_criteria\")\n",
    "# df_temp=abcset.filter(abcset.set_typ == 'roberta').select(['deviceid']).distinct()\n",
    "# df_temp=df_temp.withColumnRenamed(\"deviceid\",\"deviceId\")\n",
    "\n",
    "# data_filtered=data_filtered.join(df_temp,[\"deviceId\"],\"inner\")\n",
    "\n",
    "data_embed = data_embed.filter(data_embed.notiOpened>0)    \n",
    "\n",
    "\n",
    "# Log(\"Training took %s minutes\"%(int((time.time() - training_start) / 60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c1fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = data.select('deviceId').distinct().collect()\n",
    "d2 = data_filtered.select('deviceId').distinct().collect()\n",
    "d3 = df_temp.select('deviceId').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1284dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "df_left = pd.DataFrame(columns = ['deviceId', 'cluster_left_out_date'])\n",
    "left_out = set(d3) - set(d2)\n",
    "pbar = tqdm(total=len(left_out))\n",
    "cluster_left_out = (datetime.datetime.now() + datetime.timedelta(days=0)) .strftime(\"%Y_%m_%d\") \n",
    "i = 0\n",
    "for d in left_out:\n",
    "    df_left.loc[i,'deviceId'] = d['deviceId']\n",
    "    df_left.loc[i,'cluster_left_out_date'] = cluster_left_out\n",
    "    i= i+1\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49936e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = df_left.copy()\n",
    "print(dfl.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde4dbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_leftout = 'gs://pvtrough_asia_south1/clustering/leftout_devices_roberta'\n",
    "df_left = sqlContext.createDataFrame(dfl[['deviceId', 'cluster_left_out_date']])\n",
    "df_left.coalesce(1).write.partitionBy('cluster_left_out_date').csv(path_leftout , sep=',', header=True,mode = 'append')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a00a51d-13c8-4b82-93aa-8e7162264e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#till here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dec3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DataParallel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, RobertaTokenizer , RobertaForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "\n",
    "model_name = \"roberta-base\"\n",
    "num_classes = 17\n",
    "# model_path = \"output/checkpoint-12000\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_classes,output_hidden_states=True)\n",
    "\n",
    "# Move the model to the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Check the number of available GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    # Specify which GPUs to use\n",
    "    device_ids = [0, 1]  # Adjust the GPU IDs based on your system configuration\n",
    "    model = DataParallel(model, device_ids=device_ids)\n",
    "    print(isinstance(model, DataParallel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083cb92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model, tokenizer, text):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        input_ids = tokenizer.encode(text, add_special_tokens=True, return_tensors='pt').to(device)\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        hidden_states = outputs.hidden_states\n",
    "        last_layer_hidden_states = hidden_states[-1]\n",
    "        embeddings = torch.mean(last_layer_hidden_states, dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c4541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def Dim_reduction(sentences, tokenizer, model):\n",
    "    '''\n",
    "        This method will accept array of sentences, roberta tokenizer & model\n",
    "        next it will call methods for dimention reduction\n",
    "    '''\n",
    "\n",
    "    vecs = []\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for sentence in sentences:\n",
    "            inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True,  max_length=64)\n",
    "            inputs['input_ids'] = inputs['input_ids'].to(DEVICE)\n",
    "            inputs['attention_mask'] = inputs['attention_mask'].to(DEVICE)\n",
    "\n",
    "            hidden_states = model(**inputs, return_dict=True, output_hidden_states=True).hidden_states\n",
    "\n",
    "            #Averaging the first & last hidden states\n",
    "            output_hidden_state = (hidden_states[-1] + hidden_states[1]).mean(dim=1)\n",
    "\n",
    "            vec = output_hidden_state.cpu().numpy()[0]\n",
    "\n",
    "            vecs.append(vec)\n",
    "    \n",
    "#     print(\"35:\",vecs)\n",
    "    #Finding Kernal\n",
    "    kernel, bias = compute_kernel_bias([vecs])\n",
    "    kernel = kernel[:, :200]\n",
    "    #If you want to reduce it to 128 dim\n",
    "    #kernel = kernel[:, :128]\n",
    "    embeddings = []\n",
    "    embeddings = np.vstack(vecs)\n",
    "#     print(\"43:\", kernel, bias)\n",
    "#     print(\"44:\",embeddings)\n",
    "    #Sentence embeddings can be converted into an identity matrix\n",
    "    #by utilizing the transformation matrix\n",
    "    embeddings = transform_and_normalize(embeddings, \n",
    "                kernel=kernel,\n",
    "                bias=bias\n",
    "            )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af77bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def transform_and_normalize(vecs, kernel, bias):\n",
    "    \"\"\"\n",
    "        Applying transformation then standardize\n",
    "    \"\"\"\n",
    "    if not (kernel is None or bias is None):\n",
    "        vecs = (vecs + bias).dot(kernel)\n",
    "    return normalize(vecs)\n",
    "    \n",
    "def normalize(vecs):\n",
    "    \"\"\"\n",
    "        Standardization\n",
    "    \"\"\"\n",
    "    return vecs / (vecs**2).sum(axis=1, keepdims=True)**0.5\n",
    "    \n",
    "def compute_kernel_bias(vecs):\n",
    "    \"\"\"\n",
    "    Calculate Kernal & Bias for the final transformation - y = (x + bias).dot(kernel)\n",
    "    \"\"\"\n",
    "    vecs = np.concatenate(vecs, axis=0)\n",
    "    mu = vecs.mean(axis=0, keepdims=True)\n",
    "    cov = np.cov(vecs.T)\n",
    "    u, s, vh = np.linalg.svd(cov)\n",
    "    W = np.dot(u, np.diag(s**0.5))\n",
    "    W = np.linalg.inv(W.T)\n",
    "    return W, -mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80758e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('C:/Models/roberta-base-nli-stsb-mean-tokens')\n",
    "# model = RobertaModel.from_pretrained('C:/Models/roberta-base-nli-stsb-mean-tokens')\n",
    "\n",
    "sentences = ['someone is slicing a onion', 'someone is carrying a fish']\n",
    "\n",
    "embeddings = Dim_reduction(sentences, tokenizer, model)\n",
    "# print(distance.cosine(embeddings[0],embeddings[1]))\n",
    "print(f'Similarity score of two sentences {cosine_similarity([embeddings[0],embeddings[1]])[0][1]}')\n",
    "print(f\"New Dimension is {np.shape(embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379c6bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4137ba6-c9fc-4647-915a-663ccac6ea33",
   "metadata": {},
   "source": [
    "# Checking for News embeddings just using title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f726d28-f27f-44ab-a9df-48a474042224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing the size of embeddings using dimensionally reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97176885",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "from pyspark.sql.functions import array, col\n",
    "path_embedding = \"gs://pvtrough_asia_south1/clustering/News_tilte_embedding\"+sampled_dates[k].strftime(\"%Y_%m_%d\")+\"/\"\n",
    "df_embedding = sqlContext.read.csv(path_embedding, sep=',',\n",
    "                         inferSchema=True, header=True)\n",
    "# df_embedding.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50c4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate news embeddings based on notification text\n",
    "embedding_start = time.time()\n",
    "document_embeddings_title={}\n",
    "\n",
    "newsMapProcessed = {}\n",
    "document_embeddings = {}\n",
    "#try:\n",
    "#    already_embedded = df_embedding.select('hid').distinct().collect()\n",
    "#    already_embedded = [x['hid'] for x in already_embedded]\n",
    "#except:\n",
    "#    print(\"News Embeddings not found, generating new one\")\n",
    "already_embedded = []\n",
    "for hId in tqdm(newsMap):\n",
    "    \n",
    "    h = hId.split('-')[0]\n",
    "#     print(h)\n",
    "    if h in already_embedded or (h not in hashIdsWithFilter) :\n",
    "        continue\n",
    "    else:    \n",
    "        newsMapProcessed[h] = {}\n",
    "        newsMapProcessed[h]['title'] = newsMap[hId]['title']\n",
    "        newsMapProcessed[h]['content'] = newsMap[hId]['content']\n",
    "        newsMapProcessed[h]['features'] = newsMapProcessed[h]['title'] #+ \".\" + newsMapProcessed[h]['content']\n",
    "        document_embeddings[h] = get_embedding(model, tokenizer, newsMapProcessed[h]['features'])\n",
    "        \n",
    "\n",
    "        document_embeddings_title[h] = document_embeddings[h]\n",
    "        \n",
    "\n",
    "Log(\"Embedding took %s minutes\"%(int((time.time() - embedding_start) / 60)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ff49d7-4e84-4465-828f-715bd118459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(document_embeddings_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566bde2a-445b-42e7-ac55-98feba861c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_embedding = \"gs://pvtrough_asia_south1/clustering/News_tilte_embedding\"+sampled_dates[k].strftime(\"%Y_%m_%d\")+\"/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055a8565",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(document_embeddings_title) > 0:\n",
    "    df_em_pandas = pd.DataFrame(document_embeddings_title).T\n",
    "    df_em_pandas['hid'] = df_em_pandas.index\n",
    "    df_em_pandas['embed_date'] = sampled_dates[0]\n",
    "    df_em_spark = (sqlContext.createDataFrame(df_em_pandas))\n",
    "    df_em_spark.write.partitionBy('embed_date').csv(path_embedding , sep=',', header=True,mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f40bcb2-f129-4890-89b6-01cf900f1d5f",
   "metadata": {},
   "source": [
    "# News Embeddings using content and title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ff372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, col\n",
    "\n",
    "path_embedding = \"gs://pvtrough_asia_south1/clustering/Content_news_embedding.csv\"+sampled_dates[k].strftime(\"%Y_%m_%d\")+\"/\"\n",
    "\n",
    "df_news_embedding = sqlContext.read.csv(path_embedding, sep=',',\n",
    "                         inferSchema=True, header=True)\n",
    "df_news_embedding = df_news_embedding.distinct()\n",
    "feat_cols = [str(x) for x in range(0,len(df_news_embedding.columns) - 2)]\n",
    "df_embedding_exploded = df_news_embedding.distinct()\n",
    "df_news_embedding = df_news_embedding.select('hid', array([col(x) for x in feat_cols ]).alias('embedding'))\n",
    "df_news_embedding.take(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b187ace9-8657-47ee-868a-3a07d3d0d689",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news_embedding.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf29d30e-3ab4-4450-b491-b2d4eada4c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news_embedding.select(['hid']).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd08d9a-1d21-4315-b10f-c0722ace42eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71368da6-f547-45af-b287-3bc048dd0eb4",
   "metadata": {},
   "source": [
    "# Checking for deviceids embeddings using content and title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e04a69-2e3f-43b9-ba42-3548967ee8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_embedding = \"gs://pvtrough_asia_south1/clustering/Content_news_embedding.csv\"+sampled_dates[k].strftime(\"%Y_%m_%d\")+\"/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78603a12-fa6d-415c-b1e0-c91d103ec408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, col\n",
    "path_embedding = 'gs://pvtrough_asia_south1/clustering/Content_news_embedding.csv'\n",
    "df_embedding = sqlContext.read.csv(path_embedding, sep=',',\n",
    "                         inferSchema=True, header=True)\n",
    "\n",
    "\n",
    "# df_embedding.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e5d04b-b333-46ff-b803-9d018232052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate news embeddings based on notification text\n",
    "newsMapProcessed = {}\n",
    "document_embeddings_content = {}\n",
    "document_embeddings={}\n",
    "embedding_start = time.time()\n",
    "\n",
    "#try:\n",
    "#    already_embedded = df_embedding.select('hid').distinct().collect()\n",
    "#    already_embedded = [x['hid'] for x in already_embedded]\n",
    "#except:\n",
    "#    print(\"News Embeddings not found, generating new one\")\n",
    "already_embedded = []\n",
    "for hId in tqdm(newsMap):\n",
    "    \n",
    "    h = hId.split('-')[0]\n",
    "#     print(h)\n",
    "    if h in already_embedded or (h not in hashIdsWithFilter) :\n",
    "        continue\n",
    "    else:    \n",
    "        newsMapProcessed[h] = {}\n",
    "        newsMapProcessed[h]['title'] = newsMap[hId]['title']\n",
    "        newsMapProcessed[h]['content'] = newsMap[hId]['content']\n",
    "        newsMapProcessed[h]['features'] = newsMapProcessed[h]['title'] + newsMapProcessed[h]['content']\n",
    "        document_embeddings[h] = get_embedding(model, tokenizer, newsMapProcessed[h]['features'])\n",
    "\n",
    "        document_embeddings_content[h] = document_embeddings[h]\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "Log(\"Embedding took %s minutes\"%(int((time.time() - embedding_start) / 60)))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8a9c20-21d5-46fb-8f4f-05b7e08028fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(document_embeddings_content['l_ebpzzatk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68c4501-f46c-4f6f-9575-f27bc9118e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(document_embeddings_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2810d90-4d97-42a7-8e10-6de14ce9eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate news embeddings based on notification text\n",
    "newsMapProcessed = {}\n",
    "document_embeddings = {}\n",
    "try:\n",
    "    already_embedded = df_embedding.select('hid').distinct().collect()\n",
    "    already_embedded = [x['hid'] for x in already_embedded]\n",
    "except:\n",
    "    print(\"News Embeddings not found, generating new one\")\n",
    "    already_embedded = []\n",
    "for hId in tqdm(newsMap):\n",
    "    \n",
    "    h = hId.split('-')[0]\n",
    "#     print(h)\n",
    "    if h in already_embedded or (h not in hashIdsWithFilter) :\n",
    "        continue\n",
    "    else:    \n",
    "        newsMapProcessed[h] = {}\n",
    "        newsMapProcessed[h]['title'] = newsMap[hId]['title']\n",
    "        newsMapProcessed[h]['content'] = newsMap[hId]['content']\n",
    "        newsMapProcessed[h]['features'] = newsMapProcessed[h]['title'] + \".\" + newsMapProcessed[h]['content']\n",
    "        document_embeddings[h] = get_embedding(model, tokenizer, newsMapProcessed[h]['features'])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ed04ff-fbb5-4aeb-a801-f26417394d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896aa3b6-8dbb-40b3-8a54-e1080bb07639",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_embedding = \"gs://pvtrough_asia_south1/clustering/Content_news_embedding.csv\"+sampled_dates[k].strftime(\"%Y_%m_%d\")+\"/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b14692f-29d9-41a6-a247-6df833819641",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(document_embeddings_content) > 0:\n",
    "    df_em_pandas = pd.DataFrame(document_embeddings_content).T\n",
    "    df_em_pandas['hid'] = df_em_pandas.index\n",
    "    df_em_pandas['embed_date'] = sampled_dates[0]\n",
    "    df_em_spark = (sqlContext.createDataFrame(df_em_pandas))\n",
    "    \n",
    "    df_em_spark.write.partitionBy('embed_date').csv(path_embedding , sep=',', header=True,mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b47e947-9fde-47d2-979f-c63d0a3e3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embedding = sqlContext.read.csv(path_embedding, sep=',',\n",
    "                         inferSchema=True, header=True)\n",
    "\n",
    "from pyspark.sql.functions import array, col\n",
    "feat_cols = [str(x) for x in range(0,len(df_embedding.columns) - 2)]\n",
    "df_embedding = df_embedding.select('hid', array([col(x) for x in feat_cols ]).alias('embedding'))\n",
    "df_embedding.take(1)\n",
    "\n",
    "#df_embedding=df_embedding.withColumnRenamed(\"hid\",\"hashId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd51e2-7f24-4226-b9e4-e6a9d43af83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embedding.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999256b3-7924-451d-8174-bf693e5ccc2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884469bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "k = 0\n",
    "data_filtered = data_embed\n",
    "# # calculate idf_weight\n",
    "N = data_filtered.select('hashId').distinct().count()\n",
    "users = data_filtered.groupBy('hashId').agg(F.countDistinct('deviceId').alias('users'))\n",
    "idf_weights = users.withColumn('idf_weight', F.log(N / F.col('users')))\n",
    "\n",
    "# Multiply embeddings with IDF weights\n",
    "df_result = df_embedding.join(idf_weights, df_embedding.hid == idf_weights.hashId)\n",
    "df_result = df_result.withColumn('weighted_embedding', F.expr('transform(embedding, (x, i) -> x * idf_weight)'))\n",
    "\n",
    "\n",
    "df_result = df_result.select('hid','weighted_embedding')\n",
    "df_joined = data_filtered.join(df_result, data_filtered.hashId == df_result.hid , 'left')\n",
    "\n",
    "\n",
    "# assuming your dataframe is called df\n",
    "grouped_df = df_joined.groupBy(\"deviceId\").agg(*[\n",
    "    expr(f\"avg(weighted_embedding[{i}]) as avg_{i}\") for i in range(768)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# assuming your dataframe is called grouped_df\n",
    "avg_cols = [col(f\"avg_{i}\") for i in range(768)]\n",
    "grouped_df = grouped_df.withColumn(\"topic_avg_embedding\", array(*avg_cols))\n",
    "\n",
    "path_vec = \"gs://pvtrough_asia_south1/clustering/MLP_user_embeddings\"+sampled_dates[k].strftime(\"%Y_%m_%d\")+\"/\"\n",
    "path_vec_exploded = \"gs://pvtrough_asia_south1/clustering/MLP_device_vectors_exploded_\"+sampled_dates[k].strftime(\"%Y_%m_%d\")+\"/\"\n",
    "print(path_vec, path_vec_exploded)\n",
    "\n",
    "grouped_df.select('deviceId', 'topic_avg_embedding').write.parquet(path=path_vec, mode='overwrite')\n",
    "#grouped_df.select('deviceId', *avg_cols).write.parquet(path=path_vec_exploded, mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c5ea07-a1dd-4757-9eb6-1a270ba1415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0613149b-2a5e-439b-b2df-b777f7fdc573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8246a8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains, size, col\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "k = 0\n",
    "#num_clusters = 1000\n",
    "path_vec = \"gs://pvtrough_asia_south1/clustering/MLP_user_embeddings\"+sampled_dates[k].strftime(\"%Y_%m_%d\")+\"/\"\n",
    "# path_vec = \"gs://pvtrough_asia_south1/clustering/device_vectors_roberta_live/\"\n",
    "print(path_vec)\n",
    "# print(path_vec)\n",
    "\n",
    "temp = sqlContext.read.parquet(path_vec)\n",
    "df_devices_embeddings = temp.filter(~array_contains(temp.topic_avg_embedding, float('nan')))\n",
    "\n",
    "\n",
    "df_devices_embeddings = df_devices_embeddings.dropna(how='all')\n",
    "df_devices_embeddings = df_devices_embeddings.where(size(col(\"topic_avg_embedding\")) == 768)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759d5dc9-a37b-4a12-962e-d4d6ba951237",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_devices_embeddings.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c028888-5796-486f-abea-0520534995a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news_embedding = df_news_embedding.where(size(col(\"embedding\")) == 768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bf5ae1-6392-45bf-bd69-d96ed46e9688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a020cae-72b2-4ba3-9025-c15dae595b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef94fad-3ac6-4ffe-875c-fe1e87c965b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b887635-241e-42bd-a405-775196a58b43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11910734",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_train.join(df_devices_embeddings,[\"deviceId\"],\"inner\")\n",
    "train_data = train_data.join(df_news_embedding,train_data.hashId == df_news_embedding.hid,\"inner\")\n",
    "train_data = train_data.select('deviceId', 'hashId', 'topic_avg_embedding', 'embedding','overallTimeSpent')\n",
    "#train_data = train_data.withColumn(\"notiOpened\",F.when(train_data.notiOpened > 1, \n",
    "#                                                     1).otherwise(train_data.notiOpened))\n",
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f8fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat\n",
    "\n",
    "train_data_features = train_data.withColumn('embedding_features', \n",
    "                                            concat(col('topic_avg_embedding'), col('embedding')))\\\n",
    "                                .select('embedding_features', 'overallTimeSpent')\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7dda1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01046746-c78f-480b-b572-94d29eb1fa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import array_to_vector\n",
    "\n",
    "train_data_features=train_data_features.select(array_to_vector('embedding_features').alias('vec_embedding'),'overallTimeSpent')#.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3fbc60-2876-4a11-9515-fbbe42680023",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc71db-3ea8-4bb5-8f49-eb7c88744714",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features.count()#70973387"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df144252-9a3b-4a78-9b99-00124933fefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0442981-e5c4-4ed8-aa13-c398f36cb690",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vec=path_vec = \"gs://pvtrough_asia_south1/clustering/Regressor_train\"+sampled_dates[k].strftime(\"%Y_%m_%d\")+\"/\"\n",
    "\n",
    "train_data_features.select('vec_embedding', 'overallTimeSpent').write.parquet(path=path_vec, mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7a7c7d-171c-415b-bf0b-2c505b7f67f2",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391eec6b-fde2-4368-a7b3-e02159e28656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "training_start = time.time()\n",
    "data = train_data_features.select(\"vec_embedding\", \"overallTimeSpent\")#.sample(fraction = 0.01)\n",
    "(training_data, testing_data) = data.randomSplit([0.7, 0.3])#.sample(fraction)\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'vec_embedding', labelCol='overallTimeSpent')\n",
    "lr_model = lr.fit(training_data)\n",
    "\n",
    "#Log(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "Log(\"Intercept: \" + str(lr_model.intercept))\n",
    "\n",
    "trainingSummary = lr_model.summary\n",
    "#Log(\"RMSE_train: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "#Log(\"r2: %f\" % trainingSummary.r2)\n",
    "\n",
    "lr_predictions = lr_model.transform(training_data)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"overallTimeSpent\",metricName=\"r2\")\n",
    "Log(\"R Squared (R2) on train data = %g\" % lr_evaluator.evaluate(lr_predictions))\n",
    "train_result = lr_model.evaluate(training_data)\n",
    "Log(\"Root Mean Squared Error (RMSE) on train data = %g\" % train_result.rootMeanSquaredError)\n",
    "\n",
    "\n",
    "\n",
    "lr_predictions = lr_model.transform(testing_data)\n",
    "#lr_predictions.select(\"prediction\",\"overallTimeSpent\",\"vec_embedding\").show(5)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"overallTimeSpent\",metricName=\"r2\")\n",
    "Log(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))\n",
    "\n",
    "test_result = lr_model.evaluate(testing_data)\n",
    "Log(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)\n",
    "\n",
    "Log(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "#Log(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "Log(\"Model training took %s minutes\"%(int((time.time() - training_start) / 60)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1121de53-f760-48f3-9c1b-65bf5bd89cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/25 16:45:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1053.4 KiB\n",
      "[Stage 274:=================>                                (1371 + 14) / 3961]\r"
     ]
    }
   ],
   "source": [
    "lr_predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4f64d6-b9ec-4877-a88f-baedc9739d65",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84efdc69-e49b-4afb-8966-fc5f6fe03904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to use DL here also, will try to use Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417e9702-a62d-4efa-8b1a-e7358e0ed8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1d5de4-2bc3-473c-9184-7bc5160ea981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7eec20-447a-44b2-a6a4-fda87095d177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb6b29b-775c-4d76-b6fe-64424088fda4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfd3933-bfb2-4962-889a-95cc71dab8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da17ac3-1952-44d7-8ddd-b4445d45b521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e393fdf9-58e5-46a3-9539-83dd696a7253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
