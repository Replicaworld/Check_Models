{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43da694b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e26aa08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b0dc2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af253e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embedding = sqlContext.read.csv('gs://pvtrough_asia_south1/clustering/df_embedding_LM_large.csv', sep=',',\n",
    "                         inferSchema=True, header=True)\n",
    "df_embedding.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39857eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, col\n",
    "feat_cols = [str(x) for x in range(0,768)]\n",
    "df_embedding = df_embedding.select('hid', array([col(x) for x in feat_cols ]).alias('embedding'))\n",
    "df_embedding.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ed73da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a9f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col, expr\n",
    "k = 0\n",
    "while k<=6:\n",
    "    print(sampled_dates[k])\n",
    "    n_days = 40\n",
    "    st_date = sampled_dates[k] - datetime.timedelta(days=n_days+1)\n",
    "\n",
    "    date_fmt = \"%Y/%m/%d\"\n",
    "\n",
    "    dates = [(st_date + datetime.timedelta(days=i)) for i in range(n_days+1)]\n",
    "    dates.sort()\n",
    "\n",
    "    dates_str = [date.strftime(date_fmt) for date in dates]\n",
    "    print(dates_str)\n",
    "    # millisMin = dates[0].timestamp() * 1000\n",
    "    # millisMax = (dates[-1] + datetime.timedelta(days=1)).timestamp() * 1000\n",
    "\n",
    "    hashIdsWithFilter, newsMap = getNewsData(dates[0], sampled_dates[k])\n",
    "\n",
    "    NIS_RAW_DATA_BASE_PATH = \"gs://inshorts-segment-raw/data/segment-raw-v5/\"\n",
    "\n",
    "    path = get_raw_path(sampled_dates[k].strftime(\"%Y/%m/%d\"))\n",
    "    print(path)\n",
    "    today_data = process_raw_data(path)\n",
    "    today_data = today_data.filter(today_data.hashId.isin(hashIdsWithFilter))\n",
    "\n",
    "    datestr = [d.strftime(date_fmt) for d in dates]\n",
    "    paths = get_path(datestr, 'timeSpentFrontEvents')\n",
    "\n",
    "    p_17 = 'gs://nis-segment-datasource-v3/processed/2023/02/17/timeSpentFrontEvents/*.parquet'\n",
    "    if p_17 in paths: paths.remove(p_17)\n",
    "\n",
    "    data = sqlContext.read.parquet(*paths)\n",
    "\n",
    "    data = filter_app(data, app_name=None)\n",
    "    data = filter_tenant(data, tenant='en')\n",
    "\n",
    "    # data = data.filter((data.eventTimestamp > millisMin) & (data.eventTimestamp < millisMax))\n",
    "    data = data.select(data.deviceId, data.overallTimeSpent, (F.split(data.hashId, '-')[0]).alias('hashId'))\\\n",
    "            .groupby('deviceId', 'hashId') \\\n",
    "            .agg(F.max('overallTimeSpent').alias('overallTimeSpent'))\n",
    "\n",
    "    data = data.filter(data.hashId.isin(hashIdsWithFilter))\n",
    "\n",
    "    data = data.union(today_data)\n",
    "    data.cache()\n",
    "    \n",
    "#     newsMeanMap = defaultdict(lambda: (7, 1))\n",
    "#     popularity(data, newsMeanMap)\n",
    "    newsMeanMap = newsMeanMap_dict[sampled_dates[k].strftime(\"%Y_%m_%d\")]\n",
    "\n",
    "    data_filtered = data.filter(F.udf(lambda hashId, overallTimeSpent: overallTimeSpent > (2.5 * newsMeanMap[hashId][0]/newsMeanMap[hashId][1]), BooleanType())('hashId', 'overallTimeSpent'))\n",
    "    \n",
    "    # calculate idf_weight\n",
    "    N = data_filtered.select('hashId').distinct().count()\n",
    "    users = data_filtered.groupBy('hashId').agg(F.countDistinct('deviceId').alias('users'))\n",
    "    idf_weights = users.withColumn('idf_weight', F.log(N / F.col('users')))\n",
    "    \n",
    "    # Multiply embeddings with IDF weights\n",
    "    df_result = df_embedding.join(idf_weights, df_embedding.hid == idf_weights.hashId)\n",
    "    df_result = df_result.withColumn('weighted_embedding', F.expr('transform(embedding, (x, i) -> x * idf_weight)'))\n",
    "    \n",
    "    \n",
    "    df_result = df_result.select('hid','weighted_embedding')\n",
    "    df_joined = data_filtered.join(df_result, data_filtered.hashId == df_result.hid , 'left')\n",
    "    \n",
    "\n",
    "    # assuming your dataframe is called df\n",
    "    grouped_df = df_joined.groupBy(\"deviceId\").agg(*[\n",
    "        expr(f\"avg(weighted_embedding[{i}]) as avg_{i}\") for i in range(768)\n",
    "    ])\n",
    "\n",
    "    \n",
    "    \n",
    "    # assuming your dataframe is called grouped_df\n",
    "    avg_cols = [col(f\"avg_{i}\") for i in range(768)]\n",
    "    grouped_df = grouped_df.withColumn(\"topic_avg_embedding\", array(*avg_cols))\n",
    "    \n",
    "    path_vec = \"gs://pvtrough_asia_south1/clustering/device_vectors_roberta_idf1_AVG_large_\"+sampled_dates[k].strftime(\"%Y_%m_%d\")+\"/\"\n",
    "    print(path_vec)\n",
    "    \n",
    "    grouped_df.select('deviceId', 'topic_avg_embedding').write.parquet(path=path_vec, mode='overwrite')\n",
    "    \n",
    "    k = k+1\n",
    "#     def popularity(data, newsMeanMap):\n",
    "#         Log(\"Computing news mean\")\n",
    "#         newsMeandf = data.groupby(data.hashId).agg({'overallTimeSpent' : 'sum', 'hashId' : 'count'}).toPandas()\n",
    "\n",
    "#         for v in newsMeandf.values:\n",
    "#             newsMeanMap[v[0]] = newsMeanMap[v[0]][0] + v[1],  newsMeanMap[v[0]][1] + v[2]\n",
    "\n",
    "#     newsMeanMap = defaultdict(lambda: (7, 1))\n",
    "#     popularity(data, newsMeanMap)\n",
    "\n",
    "#     #run Storage\n",
    "#     newsMeanMap_dict[sampled_dates[k].strftime(\"%Y_%m_%d\")] = newsMeanMap.copy()\n",
    "    \n",
    "#     k=k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62d11fa-8d49-42c6-9dfe-3aa45e3c6091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c620726-81bc-494b-8dec-12ddc30b0fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      ":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.google.cloud.spark#spark-bigquery-with-dependencies_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8e115a80-22d6-4b09-a9b0-098bb82c2082;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.27.0 in central\n",
      ":: resolution report :: resolve 166ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.27.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8e115a80-22d6-4b09-a9b0-098bb82c2082\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/4ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/03 17:15:06 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "23/06/03 17:15:06 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "23/06/03 17:15:06 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/06/03 17:15:06 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "23/06/03 17:15:08 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.cloud.spark_spark-bigquery-with-dependencies_2.12-0.27.0.jar added multiple times to distributed cache.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "application_1680918691099_1680\n",
      "0\n",
      "0\n",
      "time: 637 Âµs (started: 2023-06-03 17:15:13 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import  pyspark.sql.functions as F\n",
    "import  pyspark.sql.types as T\n",
    "import pyspark.sql.window as W\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "print(0)\n",
    "import sys\n",
    "from google.cloud import storage\n",
    "import os\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql.functions import sqrt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "conf = SparkConf().setAll([('spark.jars.packages','com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.27.0')\n",
    "                          ,('spark.executor.instances',\"2\")\n",
    "                          ,('spark.dynamicAllocation.enabled',\"false\")\n",
    "                          ])\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "#Step 1: one time context generation\n",
    "spark=SparkSession(sc)\n",
    "\n",
    "bucket='pvtrough_asia_south1/tmp/tmpgcs_for_bq'\n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "spark.conf.set(\"viewsEnabled\",\"true\")\n",
    "spark.conf.set(\"materializationDataset\",'tmp')\n",
    "\n",
    "print (sc.applicationId)\n",
    "\n",
    "prop_key=['power_user_flg','created_dt','days_active']\n",
    "\n",
    "# spark.read.parquet()\n",
    "stnd_jql_fmt = \"%Y-%m-%d\"\n",
    "gcs_inp_fmt = \"%Y/%m/%d\"\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import  pyspark.sql.functions as F\n",
    "import  pyspark.sql.types as T\n",
    "import pyspark.sql.window as W\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "print(0)\n",
    "import sys\n",
    "from google.cloud import storage\n",
    "import os\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql.functions import sqrt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    %load_ext autotime\n",
    "except:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime\n",
    "print(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b938ca75-9233-4f6d-a40d-24a569868e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://pvtrough_asia_south1/tmp/deviceid_category_trend_notif2023_02_03\n",
      "time: 2min 25s (started: 2023-06-03 17:22:38 +00:00)\n"
     ]
    }
   ],
   "source": [
    "#Step 2 -repetitve for different dates\n",
    "master_dt=\"2023-02-03\"\n",
    "#category metrics over past 7 days from above dt  are stored in below bucket\n",
    "storage_buck=\"gs://pvtrough_asia_south1/tmp/deviceid_category_trend_notif\"+master_dt.replace('-','_')\n",
    "agg_query='''SELECT * FROM inshorts-1374.inshorts_scheduled.deviceid_category_trend \n",
    "WHERE event_date <= date_add(date('{0}'),INTERVAL 10 DAY) and event_date >= date('{0}') and upper(coalesce(appname,'UNKNOWN')) in ('UNKNOWN','INSHORTS')\n",
    "and short_Views>=1\n",
    "'''.format(master_dt)\n",
    "#Note: there are cases where notification was sent(notif_shown) for a category but there was no short_views on it, as above table will contain all such records \n",
    "# we can choose categories with atleast one short view by  applying short_views>=1 filter\n",
    "\n",
    "spark.read.format('bigquery').option('query',agg_query).load().coalesce(6).write.parquet(storage_buck,mode='overwrite')\n",
    "\n",
    "print (storage_buck)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9adbec6-faf5-4cf1-a314-26a421045533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bacf04-12db-4918-adc1-11bdf01bb031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
